{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b6ee6d89",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/luhung3080/miniconda3/envs/chou/lib/python3.7/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pytorch ver .  1.11.0+cu113\n",
      "Is CUDA available? True\n",
      "pynio ver .  1.5.5\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "import torch\n",
    "print(\"pytorch ver . \",torch.__version__)\n",
    "print(\"Is CUDA available?\",torch.cuda.is_available())\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset,DataLoader\n",
    "import torch.utils.data as Data\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "from pytorch_lightning.callbacks.early_stopping import EarlyStopping\n",
    "import xarray as xr\n",
    "import os\n",
    "os.environ['R_HOME'] = '/home/luhung3080/miniconda3/envs/chou/lib/R'\n",
    "from rpy2.robjects import r, numpy2ri\n",
    "numpy2ri.activate()\n",
    "from rpy2.robjects.packages import importr\n",
    "sinkr = importr('sinkr')\n",
    "import Nio\n",
    "print (\"pynio ver . \",Nio.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f62dcd70",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/luhung3080/miniconda3/envs/chou/lib/python3.7/site-packages/IPython/core/interactiveshell.py:3524: DtypeWarning: Columns (10) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  exec(code_obj, self.user_global_ns, self.user_ns)\n"
     ]
    }
   ],
   "source": [
    "data=pd.read_csv('v100_hr_20200101-0930_cut_utf8.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db42019f",
   "metadata": {},
   "source": [
    "# Y = b0 + b1X1 (96hr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "c778a6d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(244, 6816)\n",
      "(244, 6816)\n"
     ]
    }
   ],
   "source": [
    "###\n",
    "#cal_PMf  \n",
    "###\n",
    "u=np.zeros([244,6816])\n",
    "for i in range (0,244):\n",
    "    a=np.array(data['cal_PMf'][6816*i:6816*i+6816])\n",
    "    for j in range(0,6816):\n",
    "        if a[j]=='\\\\N' :\n",
    "            a[j]=np.nan\n",
    "    u[i]=a.T\n",
    "###\n",
    "#obs_PMf\n",
    "###\n",
    "v=np.zeros([244,6816])\n",
    "for i in range (0,244):\n",
    "    a=np.array(data['obs_PMf'][6816*i:6816*i+6816])\n",
    "    for j in range(0,6816):\n",
    "        if a[j]=='\\\\N' :\n",
    "            a[j]=np.nan\n",
    "    v[i]=a.T\n",
    "\n",
    "print(np.shape(u))\n",
    "print(np.shape(v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "4b788cb5",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] \"1 EOF ; RMS = 9.93506801\"\n",
      "[1] \"1 EOF ; RMS = 9.92613522\"\n",
      "[1] \"1 EOF ; RMS = 9.92636403\"\n",
      "[1] \"2 EOF ; RMS = 9.02756575\"\n",
      "[1] \"2 EOF ; RMS = 9.02624323\"\n",
      "[1] \"2 EOF ; RMS = 9.02611752\"\n",
      "[1] \"2 EOF ; RMS = 9.02607211\"\n",
      "[1] \"2 EOF ; RMS = 9.02605448\"\n",
      "[1] \"2 EOF ; RMS = 9.02604729\"\n",
      "[1] \"3 EOF ; RMS = 8.70249363\"\n",
      "[1] \"3 EOF ; RMS = 8.70435635\"\n",
      "[1] \"4 EOF ; RMS = 8.46576495\"\n",
      "[1] \"4 EOF ; RMS = 8.47310206\"\n",
      "[1] \"5 EOF ; RMS = 8.30686772\"\n",
      "[1] \"5 EOF ; RMS = 8.32268705\"\n",
      "[1] \"6 EOF ; RMS = 8.23892984\"\n",
      "[1] \"6 EOF ; RMS = 8.25361783\"\n",
      "[1] \"7 EOF ; RMS = 8.15532496\"\n",
      "[1] \"7 EOF ; RMS = 8.16791977\"\n",
      "[1] \"8 EOF ; RMS = 8.09537777\"\n",
      "[1] \"8 EOF ; RMS = 8.11839344\"\n",
      "[1] \"9 EOF ; RMS = 8.11322832\"\n",
      "[1] \"9 EOF ; RMS = 8.15047305\"\n",
      "[1] \"10 EOF ; RMS = 8.13616664\"\n",
      "[1] \"10 EOF ; RMS = 8.16005763\"\n",
      "[1] \"11 EOF ; RMS = 8.08949966\"\n",
      "[1] \"11 EOF ; RMS = 8.09875024\"\n",
      "[1] \"12 EOF ; RMS = 8.04003736\"\n",
      "[1] \"12 EOF ; RMS = 8.05846835\"\n",
      "[1] \"13 EOF ; RMS = 8.02165135\"\n",
      "[1] \"13 EOF ; RMS = 8.04179708\"\n",
      "[1] \"14 EOF ; RMS = 7.9883091\"\n",
      "[1] \"14 EOF ; RMS = 8.00638147\"\n",
      "[1] \"15 EOF ; RMS = 8.01040893\"\n",
      "[1] \"1 EOF ; RMS = 8.15209815\"\n",
      "[1] \"1 EOF ; RMS = 8.11210946\"\n",
      "[1] \"1 EOF ; RMS = 8.11095237\"\n",
      "[1] \"1 EOF ; RMS = 8.11082797\"\n",
      "[1] \"1 EOF ; RMS = 8.11080733\"\n",
      "[1] \"1 EOF ; RMS = 8.11080349\"\n",
      "[1] \"2 EOF ; RMS = 7.44583273\"\n",
      "[1] \"2 EOF ; RMS = 7.44427049\"\n",
      "[1] \"2 EOF ; RMS = 7.44429801\"\n",
      "[1] \"3 EOF ; RMS = 6.93325208\"\n",
      "[1] \"3 EOF ; RMS = 6.93179956\"\n",
      "[1] \"3 EOF ; RMS = 6.93186048\"\n",
      "[1] \"4 EOF ; RMS = 6.64733653\"\n",
      "[1] \"4 EOF ; RMS = 6.64908933\"\n",
      "[1] \"5 EOF ; RMS = 6.50336869\"\n",
      "[1] \"5 EOF ; RMS = 6.50398997\"\n",
      "[1] \"6 EOF ; RMS = 6.34981156\"\n",
      "[1] \"6 EOF ; RMS = 6.35104665\"\n",
      "[1] \"7 EOF ; RMS = 6.20366602\"\n",
      "[1] \"7 EOF ; RMS = 6.20485169\"\n",
      "[1] \"8 EOF ; RMS = 6.10041366\"\n",
      "[1] \"8 EOF ; RMS = 6.1016868\"\n",
      "[1] \"9 EOF ; RMS = 6.02003548\"\n",
      "[1] \"9 EOF ; RMS = 6.02517895\"\n",
      "[1] \"10 EOF ; RMS = 5.95243935\"\n",
      "[1] \"10 EOF ; RMS = 5.95672541\"\n",
      "[1] \"11 EOF ; RMS = 5.86789577\"\n",
      "[1] \"11 EOF ; RMS = 5.87243499\"\n",
      "[1] \"12 EOF ; RMS = 5.79620722\"\n",
      "[1] \"12 EOF ; RMS = 5.80036125\"\n",
      "[1] \"13 EOF ; RMS = 5.75605402\"\n",
      "[1] \"13 EOF ; RMS = 5.76350423\"\n",
      "[1] \"14 EOF ; RMS = 5.70702981\"\n",
      "[1] \"14 EOF ; RMS = 5.7107908\"\n",
      "[1] \"15 EOF ; RMS = 5.66191717\"\n",
      "[1] \"15 EOF ; RMS = 5.66328993\"\n",
      "[1] \"16 EOF ; RMS = 5.63332039\"\n",
      "[1] \"16 EOF ; RMS = 5.63998172\"\n",
      "[1] \"17 EOF ; RMS = 5.60230605\"\n",
      "[1] \"17 EOF ; RMS = 5.6059636\"\n",
      "[1] \"18 EOF ; RMS = 5.56996738\"\n",
      "[1] \"18 EOF ; RMS = 5.57173175\"\n",
      "[1] \"19 EOF ; RMS = 5.54882481\"\n",
      "[1] \"19 EOF ; RMS = 5.55740992\"\n",
      "[1] \"20 EOF ; RMS = 5.54632387\"\n",
      "[1] \"20 EOF ; RMS = 5.55618836\"\n",
      "[1] \"21 EOF ; RMS = 5.53245097\"\n",
      "[1] \"21 EOF ; RMS = 5.53834435\"\n",
      "[1] \"22 EOF ; RMS = 5.52335397\"\n",
      "[1] \"22 EOF ; RMS = 5.53209636\"\n",
      "[1] \"23 EOF ; RMS = 5.49885334\"\n",
      "[1] \"23 EOF ; RMS = 5.50424379\"\n",
      "[1] \"24 EOF ; RMS = 5.48908678\"\n",
      "[1] \"24 EOF ; RMS = 5.49667399\"\n",
      "[1] \"25 EOF ; RMS = 5.48002754\"\n",
      "[1] \"25 EOF ; RMS = 5.48574732\"\n",
      "[1] \"26 EOF ; RMS = 5.46265109\"\n",
      "[1] \"26 EOF ; RMS = 5.46885276\"\n",
      "[1] \"27 EOF ; RMS = 5.46174533\"\n",
      "[1] \"27 EOF ; RMS = 5.47185873\"\n",
      "[1] \"28 EOF ; RMS = 5.45042703\"\n",
      "[1] \"28 EOF ; RMS = 5.45612796\"\n",
      "[1] \"29 EOF ; RMS = 5.43985039\"\n",
      "[1] \"29 EOF ; RMS = 5.44697447\"\n",
      "[1] \"30 EOF ; RMS = 5.43004358\"\n",
      "[1] \"30 EOF ; RMS = 5.43625158\"\n",
      "[1] \"31 EOF ; RMS = 5.42641893\"\n",
      "[1] \"31 EOF ; RMS = 5.43583535\"\n",
      "[1] \"32 EOF ; RMS = 5.43217826\"\n",
      "[1] \"32 EOF ; RMS = 5.44705633\"\n",
      "[1] \"33 EOF ; RMS = 5.4591548\"\n"
     ]
    }
   ],
   "source": [
    "XRestruct=sinkr.dineof(u)\n",
    "YRestruct=sinkr.dineof(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "ba0e8c2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "(244, 6816)\n",
      "(244, 6816)\n"
     ]
    }
   ],
   "source": [
    "XRestruct_Fun=np.array(XRestruct[0])\n",
    "YRestruct_Fun=np.array(YRestruct[0])\n",
    "print(type(XRestruct_Fun))\n",
    "print(type(YRestruct_Fun))\n",
    "print(np.shape(XRestruct_Fun))\n",
    "print(np.shape(YRestruct_Fun))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "04eb08a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Xhat_train\n",
      "(244, 6816)\n",
      "Yhat_train\n",
      "(244, 6816)\n",
      "Xhat_test\n",
      "(30, 6816)\n",
      "Yhat_test\n",
      "(30, 6816)\n"
     ]
    }
   ],
   "source": [
    "Xhat=XRestruct_Fun\n",
    "Yhat=YRestruct_Fun\n",
    "Xhat_train = np.zeros([244,6816])\n",
    "Yhat_train = np.zeros([244,6816])\n",
    "for i in range (0,244):\n",
    "    for j in range (0,6816):\n",
    "        Xhat_train[i][j] = Xhat[i][j]\n",
    "        Yhat_train[i][j] = Yhat[i][j]\n",
    "\n",
    "Xhat_test = np.zeros([30,6816])\n",
    "Yhat_test = np.zeros([30,6816])\n",
    "#data_cal\n",
    "for i in range (244,274):\n",
    "    a=np.array(data['cal_PMf'][6816*i:6816*i+6816])\n",
    "    for j in range(0,6816):\n",
    "        if a[j]=='\\\\N' :\n",
    "            a[j]=np.nan\n",
    "    Xhat_test[i-244]=a\n",
    "#data_obs\n",
    "for i in range (244,274):\n",
    "    a=np.array(data['obs_PMf'][6816*i:6816*i+6816])\n",
    "    for j in range(0,6816):\n",
    "        if a[j]=='\\\\N' :\n",
    "            a[j]=np.nan\n",
    "    Yhat_test[i-244]=a\n",
    "print('Xhat_train')\n",
    "#print(Xhat_train)\n",
    "print(np.shape(Xhat_train))\n",
    "print('Yhat_train')\n",
    "#print(Yhat_train)\n",
    "print(np.shape(Yhat_train))\n",
    "print('Xhat_test')\n",
    "#print(Xhat_test)\n",
    "print(np.shape(Xhat_test))\n",
    "print('Yhat_test')\n",
    "#print(Yhat_test)\n",
    "print(np.shape(Yhat_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67c32285",
   "metadata": {},
   "source": [
    "## GD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "85a258e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = Xhat_train\n",
    "y = Yhat_train\n",
    "xt = Xhat_test\n",
    "yt = Yhat_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "44e3f621",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model (x,b0,b1):\n",
    "    # y = b0 +  torch.matmul(x,b1)\n",
    "    y = b0 + b1*x\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "3fdc1086",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Training loss 8.1924, Testing loss 7.4484\n",
      "Epoch 1, Training loss 8.1388, Testing loss 7.4147\n",
      "Epoch 2, Training loss 8.0869, Testing loss 7.3829\n",
      "Epoch 3, Training loss 8.0368, Testing loss 7.3529\n",
      "Epoch 4, Training loss 7.9885, Testing loss 7.3247\n",
      "Epoch 5, Training loss 7.9419, Testing loss 7.2982\n",
      "Epoch 6, Training loss 7.8972, Testing loss 7.2734\n",
      "Epoch 7, Training loss 7.8543, Testing loss 7.2503\n",
      "Epoch 8, Training loss 7.8132, Testing loss 7.2289\n",
      "Epoch 9, Training loss 7.7738, Testing loss 7.2091\n",
      "Epoch 10, Training loss 7.7362, Testing loss 7.1909\n",
      "Epoch 20, Training loss 7.4477, Testing loss 7.0812\n",
      "Epoch 30, Training loss 7.2874, Testing loss 7.0601\n",
      "Epoch 40, Training loss 7.2029, Testing loss 7.0688\n",
      "Epoch 50, Training loss 7.1549, Testing loss 7.0741\n",
      "Epoch 60, Training loss 7.1227, Testing loss 7.0652\n",
      "Epoch 70, Training loss 7.0965, Testing loss 7.0456\n",
      "Epoch 80, Training loss 7.0732, Testing loss 7.0219\n",
      "Epoch 90, Training loss 7.0516, Testing loss 6.9973\n",
      "Epoch 100, Training loss 7.0313, Testing loss 6.9739\n",
      "Epoch 110, Training loss 7.0119, Testing loss 6.9515\n",
      "Epoch 120, Training loss 6.9933, Testing loss 6.9302\n",
      "Epoch 130, Training loss 6.9755, Testing loss 6.9098\n",
      "Epoch 140, Training loss 6.9583, Testing loss 6.8900\n",
      "Epoch 150, Training loss 6.9418, Testing loss 6.8706\n",
      "Epoch 160, Training loss 6.9258, Testing loss 6.8517\n",
      "Epoch 170, Training loss 6.9105, Testing loss 6.8330\n",
      "Epoch 180, Training loss 6.8957, Testing loss 6.8149\n",
      "Epoch 190, Training loss 6.8815, Testing loss 6.7974\n",
      "Epoch 200, Training loss 6.8678, Testing loss 6.7803\n",
      "Epoch 210, Training loss 6.8546, Testing loss 6.7637\n",
      "Epoch 220, Training loss 6.8418, Testing loss 6.7475\n",
      "Epoch 230, Training loss 6.8296, Testing loss 6.7317\n",
      "Epoch 240, Training loss 6.8177, Testing loss 6.7163\n",
      "Epoch 250, Training loss 6.8063, Testing loss 6.7012\n",
      "Epoch 260, Training loss 6.7954, Testing loss 6.6867\n",
      "Epoch 270, Training loss 6.7848, Testing loss 6.6725\n",
      "Epoch 280, Training loss 6.7746, Testing loss 6.6587\n",
      "Epoch 290, Training loss 6.7648, Testing loss 6.6453\n",
      "Epoch 300, Training loss 6.7553, Testing loss 6.6323\n",
      "Epoch 310, Training loss 6.7462, Testing loss 6.6194\n",
      "Epoch 320, Training loss 6.7374, Testing loss 6.6071\n",
      "Epoch 330, Training loss 6.7289, Testing loss 6.5950\n",
      "Epoch 340, Training loss 6.7208, Testing loss 6.5835\n",
      "Epoch 350, Training loss 6.7130, Testing loss 6.5723\n",
      "Epoch 360, Training loss 6.7055, Testing loss 6.5612\n",
      "Epoch 370, Training loss 6.6982, Testing loss 6.5504\n",
      "Epoch 380, Training loss 6.6912, Testing loss 6.5398\n",
      "Epoch 390, Training loss 6.6845, Testing loss 6.5295\n",
      "Epoch 400, Training loss 6.6781, Testing loss 6.5198\n",
      "Epoch 410, Training loss 6.6719, Testing loss 6.5103\n",
      "Epoch 420, Training loss 6.6659, Testing loss 6.5011\n",
      "Epoch 430, Training loss 6.6602, Testing loss 6.4923\n",
      "Epoch 440, Training loss 6.6547, Testing loss 6.4837\n",
      "Epoch 450, Training loss 6.6494, Testing loss 6.4754\n",
      "Epoch 460, Training loss 6.6443, Testing loss 6.4672\n",
      "Epoch 470, Training loss 6.6394, Testing loss 6.4594\n",
      "Epoch 480, Training loss 6.6347, Testing loss 6.4517\n",
      "Epoch 490, Training loss 6.6301, Testing loss 6.4443\n",
      "Epoch 500, Training loss 6.6258, Testing loss 6.4370\n",
      "Epoch 510, Training loss 6.6216, Testing loss 6.4300\n",
      "Epoch 520, Training loss 6.6175, Testing loss 6.4232\n",
      "Epoch 530, Training loss 6.6137, Testing loss 6.4166\n",
      "Epoch 540, Training loss 6.6099, Testing loss 6.4103\n",
      "Epoch 550, Training loss 6.6064, Testing loss 6.4042\n",
      "Epoch 560, Training loss 6.6029, Testing loss 6.3982\n",
      "Epoch 570, Training loss 6.5996, Testing loss 6.3924\n",
      "Epoch 580, Training loss 6.5964, Testing loss 6.3867\n",
      "Epoch 590, Training loss 6.5934, Testing loss 6.3813\n",
      "Epoch 600, Training loss 6.5905, Testing loss 6.3761\n",
      "Epoch 610, Training loss 6.5876, Testing loss 6.3712\n",
      "Epoch 620, Training loss 6.5849, Testing loss 6.3663\n",
      "Epoch 630, Training loss 6.5823, Testing loss 6.3615\n",
      "Epoch 640, Training loss 6.5799, Testing loss 6.3570\n",
      "Epoch 650, Training loss 6.5775, Testing loss 6.3525\n",
      "Epoch 660, Training loss 6.5752, Testing loss 6.3482\n",
      "Epoch 670, Training loss 6.5730, Testing loss 6.3440\n",
      "Epoch 680, Training loss 6.5708, Testing loss 6.3399\n",
      "Epoch 690, Training loss 6.5688, Testing loss 6.3360\n",
      "Epoch 700, Training loss 6.5669, Testing loss 6.3321\n",
      "Epoch 710, Training loss 6.5650, Testing loss 6.3285\n",
      "Epoch 720, Training loss 6.5632, Testing loss 6.3248\n",
      "Epoch 730, Training loss 6.5615, Testing loss 6.3213\n",
      "Epoch 740, Training loss 6.5598, Testing loss 6.3179\n",
      "Epoch 750, Training loss 6.5582, Testing loss 6.3147\n",
      "Epoch 760, Training loss 6.5567, Testing loss 6.3117\n",
      "Epoch 770, Training loss 6.5553, Testing loss 6.3087\n",
      "Epoch 780, Training loss 6.5539, Testing loss 6.3057\n",
      "Epoch 790, Training loss 6.5526, Testing loss 6.3029\n",
      "Epoch 800, Training loss 6.5513, Testing loss 6.3001\n",
      "Epoch 810, Training loss 6.5501, Testing loss 6.2976\n",
      "Epoch 820, Training loss 6.5489, Testing loss 6.2951\n",
      "Epoch 830, Training loss 6.5478, Testing loss 6.2927\n",
      "Epoch 840, Training loss 6.5467, Testing loss 6.2902\n",
      "Epoch 850, Training loss 6.5457, Testing loss 6.2880\n",
      "Epoch 860, Training loss 6.5447, Testing loss 6.2859\n",
      "Epoch 870, Training loss 6.5437, Testing loss 6.2838\n",
      "Epoch 880, Training loss 6.5428, Testing loss 6.2818\n",
      "Epoch 890, Training loss 6.5420, Testing loss 6.2798\n",
      "Epoch 900, Training loss 6.5411, Testing loss 6.2778\n",
      "Epoch 910, Training loss 6.5403, Testing loss 6.2760\n",
      "Epoch 920, Training loss 6.5396, Testing loss 6.2742\n",
      "Epoch 930, Training loss 6.5388, Testing loss 6.2725\n",
      "Epoch 940, Training loss 6.5381, Testing loss 6.2708\n",
      "Epoch 950, Training loss 6.5375, Testing loss 6.2692\n",
      "Epoch 960, Training loss 6.5368, Testing loss 6.2677\n",
      "Epoch 970, Training loss 6.5362, Testing loss 6.2661\n",
      "Epoch 980, Training loss 6.5356, Testing loss 6.2647\n",
      "Epoch 990, Training loss 6.5351, Testing loss 6.2632\n",
      "Epoch 1000, Training loss 6.5345, Testing loss 6.2619\n",
      "Epoch 1010, Training loss 6.5340, Testing loss 6.2606\n",
      "Epoch 1020, Training loss 6.5335, Testing loss 6.2594\n",
      "Epoch 1030, Training loss 6.5331, Testing loss 6.2581\n",
      "Epoch 1040, Training loss 6.5326, Testing loss 6.2570\n",
      "Epoch 1050, Training loss 6.5322, Testing loss 6.2557\n",
      "Epoch 1060, Training loss 6.5318, Testing loss 6.2547\n",
      "Epoch 1070, Training loss 6.5314, Testing loss 6.2537\n",
      "Epoch 1080, Training loss 6.5310, Testing loss 6.2527\n",
      "Epoch 1090, Training loss 6.5307, Testing loss 6.2518\n",
      "Epoch 1100, Training loss 6.5303, Testing loss 6.2508\n",
      "Epoch 1110, Training loss 6.5300, Testing loss 6.2499\n",
      "Epoch 1120, Training loss 6.5297, Testing loss 6.2490\n",
      "Epoch 1130, Training loss 6.5294, Testing loss 6.2481\n",
      "Epoch 1140, Training loss 6.5291, Testing loss 6.2474\n",
      "Epoch 1150, Training loss 6.5288, Testing loss 6.2465\n",
      "Epoch 1160, Training loss 6.5286, Testing loss 6.2458\n",
      "Epoch 1170, Training loss 6.5283, Testing loss 6.2451\n",
      "Epoch 1180, Training loss 6.5281, Testing loss 6.2444\n",
      "Epoch 1190, Training loss 6.5278, Testing loss 6.2437\n",
      "Epoch 1200, Training loss 6.5276, Testing loss 6.2430\n",
      "Epoch 1210, Training loss 6.5274, Testing loss 6.2423\n",
      "Epoch 1220, Training loss 6.5272, Testing loss 6.2418\n",
      "Epoch 1230, Training loss 6.5270, Testing loss 6.2411\n",
      "Epoch 1240, Training loss 6.5268, Testing loss 6.2405\n",
      "Epoch 1250, Training loss 6.5267, Testing loss 6.2400\n",
      "Epoch 1260, Training loss 6.5265, Testing loss 6.2394\n",
      "Epoch 1270, Training loss 6.5263, Testing loss 6.2389\n",
      "Epoch 1280, Training loss 6.5262, Testing loss 6.2383\n",
      "Epoch 1290, Training loss 6.5260, Testing loss 6.2379\n",
      "Epoch 1300, Training loss 6.5259, Testing loss 6.2374\n",
      "Epoch 1310, Training loss 6.5257, Testing loss 6.2370\n",
      "Epoch 1320, Training loss 6.5256, Testing loss 6.2365\n",
      "Epoch 1330, Training loss 6.5255, Testing loss 6.2361\n",
      "Epoch 1340, Training loss 6.5254, Testing loss 6.2356\n",
      "Epoch 1350, Training loss 6.5252, Testing loss 6.2353\n",
      "Epoch 1360, Training loss 6.5251, Testing loss 6.2349\n",
      "Epoch 1370, Training loss 6.5250, Testing loss 6.2345\n",
      "Epoch 1380, Training loss 6.5249, Testing loss 6.2341\n",
      "Epoch 1390, Training loss 6.5248, Testing loss 6.2337\n",
      "Epoch 1400, Training loss 6.5247, Testing loss 6.2336\n",
      "Epoch 1410, Training loss 6.5246, Testing loss 6.2332\n",
      "Epoch 1420, Training loss 6.5245, Testing loss 6.2328\n",
      "Epoch 1430, Training loss 6.5245, Testing loss 6.2325\n",
      "Epoch 1440, Training loss 6.5244, Testing loss 6.2323\n",
      "Epoch 1450, Training loss 6.5243, Testing loss 6.2319\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1460, Training loss 6.5242, Testing loss 6.2317\n",
      "Epoch 1470, Training loss 6.5241, Testing loss 6.2314\n",
      "Epoch 1480, Training loss 6.5241, Testing loss 6.2312\n",
      "Epoch 1490, Training loss 6.5240, Testing loss 6.2310\n",
      "Epoch 1500, Training loss 6.5239, Testing loss 6.2307\n",
      "Epoch 1510, Training loss 6.5239, Testing loss 6.2305\n",
      "Epoch 1520, Training loss 6.5238, Testing loss 6.2303\n",
      "Epoch 1530, Training loss 6.5238, Testing loss 6.2301\n",
      "Epoch 1540, Training loss 6.5237, Testing loss 6.2299\n",
      "Epoch 1550, Training loss 6.5237, Testing loss 6.2297\n",
      "Epoch 1560, Training loss 6.5236, Testing loss 6.2295\n",
      "Epoch 1570, Training loss 6.5236, Testing loss 6.2292\n",
      "Epoch 1580, Training loss 6.5235, Testing loss 6.2291\n",
      "Epoch 1590, Training loss 6.5235, Testing loss 6.2289\n",
      "Epoch 1600, Training loss 6.5234, Testing loss 6.2286\n",
      "Epoch 1610, Training loss 6.5234, Testing loss 6.2285\n",
      "Epoch 1620, Training loss 6.5234, Testing loss 6.2284\n",
      "Epoch 1630, Training loss 6.5233, Testing loss 6.2283\n",
      "Epoch 1640, Training loss 6.5233, Testing loss 6.2281\n",
      "Epoch 1650, Training loss 6.5232, Testing loss 6.2279\n",
      "Epoch 1660, Training loss 6.5232, Testing loss 6.2277\n",
      "Epoch 1670, Training loss 6.5232, Testing loss 6.2276\n",
      "Epoch 1680, Training loss 6.5231, Testing loss 6.2274\n",
      "Epoch 1690, Training loss 6.5231, Testing loss 6.2273\n",
      "Epoch 1700, Training loss 6.5231, Testing loss 6.2273\n",
      "Epoch 1710, Training loss 6.5230, Testing loss 6.2272\n",
      "Epoch 1720, Training loss 6.5230, Testing loss 6.2270\n",
      "Epoch 1730, Training loss 6.5230, Testing loss 6.2270\n",
      "Epoch 1740, Training loss 6.5230, Testing loss 6.2268\n",
      "Epoch 1750, Training loss 6.5229, Testing loss 6.2267\n",
      "Epoch 1760, Training loss 6.5229, Testing loss 6.2266\n",
      "Epoch 1770, Training loss 6.5229, Testing loss 6.2265\n",
      "Epoch 1780, Training loss 6.5229, Testing loss 6.2264\n",
      "Epoch 1790, Training loss 6.5229, Testing loss 6.2263\n",
      "Epoch 1800, Training loss 6.5228, Testing loss 6.2261\n",
      "Epoch 1810, Training loss 6.5228, Testing loss 6.2261\n",
      "Epoch 1820, Training loss 6.5228, Testing loss 6.2261\n",
      "Epoch 1830, Training loss 6.5228, Testing loss 6.2260\n",
      "Epoch 1840, Training loss 6.5228, Testing loss 6.2259\n",
      "Epoch 1850, Training loss 6.5227, Testing loss 6.2258\n",
      "Epoch 1860, Training loss 6.5227, Testing loss 6.2257\n",
      "Epoch 1870, Training loss 6.5227, Testing loss 6.2256\n",
      "Epoch 1880, Training loss 6.5227, Testing loss 6.2256\n",
      "Epoch 1890, Training loss 6.5227, Testing loss 6.2255\n",
      "Epoch 1900, Training loss 6.5227, Testing loss 6.2255\n",
      "Epoch 1910, Training loss 6.5226, Testing loss 6.2254\n",
      "Epoch 1920, Training loss 6.5226, Testing loss 6.2252\n",
      "Epoch 1930, Training loss 6.5226, Testing loss 6.2252\n",
      "Epoch 1940, Training loss 6.5226, Testing loss 6.2251\n",
      "Epoch 1950, Training loss 6.5226, Testing loss 6.2252\n",
      "Epoch 1960, Training loss 6.5226, Testing loss 6.2251\n",
      "Epoch 1970, Training loss 6.5226, Testing loss 6.2250\n",
      "Epoch 1980, Training loss 6.5226, Testing loss 6.2249\n",
      "Epoch 1990, Training loss 6.5226, Testing loss 6.2249\n",
      "Epoch 1991, Training loss 6.5225, Testing loss 6.2249\n",
      "Epoch 1992, Training loss 6.5225, Testing loss 6.2249\n",
      "Epoch 1993, Training loss 6.5225, Testing loss 6.2249\n",
      "Epoch 1994, Training loss 6.5225, Testing loss 6.2248\n",
      "Epoch 1995, Training loss 6.5225, Testing loss 6.2249\n",
      "Epoch 1996, Training loss 6.5225, Testing loss 6.2249\n",
      "Epoch 1997, Training loss 6.5225, Testing loss 6.2249\n",
      "Epoch 1998, Training loss 6.5225, Testing loss 6.2248\n",
      "Epoch 1999, Training loss 6.5225, Testing loss 6.2248\n",
      "Epoch 2000, Training loss 6.5225, Testing loss 6.2248\n"
     ]
    }
   ],
   "source": [
    "features = torch.from_numpy(x)\n",
    "targets = torch.from_numpy(y)\n",
    "x_test = torch.from_numpy(xt)\n",
    "y_test = torch.from_numpy(yt)\n",
    "\n",
    "# beta0 = torch.randn(6816 , requires_grad = True)\n",
    "# # beta1 = torch.randn([6816 , 6816], requires_grad = True)\n",
    "# beta1 = torch.randn(6816, requires_grad = True)\n",
    "\n",
    "beta0 = torch.ones(6816 , requires_grad = True)\n",
    "beta1 = torch.ones(6816, requires_grad = True)\n",
    "\n",
    "rate = 1e-2\n",
    "optimizer = optim.Adam([beta0 , beta1], lr=rate)\n",
    "\n",
    "epo = 2001\n",
    "loss = nn.L1Loss()\n",
    "train_error = np.zeros(epo)\n",
    "test_error = np.zeros(epo)\n",
    "\n",
    "\n",
    "for epoch in range (epo):\n",
    "    yhats_train = model(features.float() , beta0 , beta1)\n",
    "    train_loss = loss(targets.float() , yhats_train)\n",
    "    train_error[epoch] = train_loss\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    train_loss.backward() \n",
    "    optimizer.step()    \n",
    "\n",
    "    yhats_test = model(x_test.float(), beta0, beta1) \n",
    "#     for i in range (30):\n",
    "#         for j in range (6816):\n",
    "#             if y_test[i][j] == 0:\n",
    "#                 yhats_test[i][j] = 0\n",
    "    r = abs(yhats_test - y_test)\n",
    "    test_loss = torch.nanmean(r)\n",
    "    # test_loss = loss(y_test , yhats_test)\n",
    "    test_error[epoch] = test_loss\n",
    "\n",
    "    if epoch <= 10 or epoch % 10 == 0:\n",
    "        print(f\"Epoch {epoch}, Training loss {train_loss.item():.4f},\"\n",
    "                    f\" Testing loss {test_loss.item():.4f}\")\n",
    "        # print('\\tBeta_0 : ' , beta0)\n",
    "        # print('\\tBeta_1 : ' , beta1)\n",
    "    else :\n",
    "        if epoch >= epo-10 :\n",
    "            print(f\"Epoch {epoch}, Training loss {train_loss.item():.4f},\"\n",
    "                        f\" Testing loss {test_loss.item():.4f}\")\n",
    "            # print('\\tBeta_0 : ' , beta0)\n",
    "            # print('\\tBeta_1 : ' , beta1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "649d5628",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig1 = plt.figure(1)\n",
    "x=np.linspace(1,epo,epo)\n",
    "plt.plot(x,train_error, label = 'Training')\n",
    "plt.plot(x,test_error, label ='Testing')\n",
    "plt.legend(loc = 2)\n",
    "plt.title('MAE')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "79624493",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.224822758638766\n"
     ]
    }
   ],
   "source": [
    "print(np.min(test_error))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa5e3355",
   "metadata": {},
   "source": [
    "# Y = b0 + b1X12 (120hr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b2bff872",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(274, 6816)\n",
      "(274, 6816)\n",
      "January to August (training data set) before_error (MADE) : 8.247546\n",
      "September (testing data set) before_error (MADE) : 7.586688\n"
     ]
    }
   ],
   "source": [
    "old_x=np.zeros([274,6816])\n",
    "old_y=np.zeros([274,6816])\n",
    "#data_cal\n",
    "for i in range (0,274):\n",
    "    a=np.array(data['cal_PMf'][6816*i:6816*i+6816])\n",
    "    for j in range(0,6816):\n",
    "        if a[j]=='\\\\N' :\n",
    "            a[j]=np.nan\n",
    "    old_x[i]=a\n",
    "#data_obs\n",
    "for i in range (0,274):\n",
    "    a=np.array(data['obs_PMf'][6816*i:6816*i+6816])\n",
    "    for j in range(0,6816):\n",
    "        if a[j]=='\\\\N' :\n",
    "            a[j]=np.nan\n",
    "    old_y[i]=a\n",
    "print(np.shape(old_x))\n",
    "print(np.shape(old_y))\n",
    "\n",
    "old_x1=np.zeros([244,6816])\n",
    "old_y1=np.zeros([244,6816])\n",
    "for i in range (0,244):\n",
    "    for j in range (0,6816):\n",
    "        old_x1[i][j]=old_x[i][j]\n",
    "        old_y1[i][j]=old_y[i][j]\n",
    "before1=abs(old_x1-old_y1)\n",
    "before_error1=np.nanmean(before1)\n",
    "print(\"January to August (training data set) before_error (MADE) : %f\" %before_error1)\n",
    "\n",
    "old_x2=np.zeros([30,6816])\n",
    "old_y2=np.zeros([30,6816])\n",
    "for i in range (0,30):\n",
    "    for j in range (0,6816):\n",
    "        old_x2[i][j]=old_x[i+244][j]\n",
    "        old_y2[i][j]=old_y[i+244][j]\n",
    "before2=abs(old_x2-old_y2)\n",
    "before_error2=np.nanmean(before2)\n",
    "print(\"September (testing data set) before_error (MADE) : %f\" %before_error2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eab16d9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(243, 1704)\n",
      "(243, 6816)\n"
     ]
    }
   ],
   "source": [
    "x1=np.zeros([243,1704])\n",
    "x2=np.zeros([243,6816])\n",
    "\n",
    "#x1 (0th~1703th column as x)\n",
    "for i in range (0,243):\n",
    "    for j in range (0,71):\n",
    "        a=np.array(data['obs_PMf'][6816*i+96*j:6816*i+96*j+24])\n",
    "        for k in range (0,24):\n",
    "            if a[k]=='\\\\N' :\n",
    "                a[k]=np.nan\n",
    "        for k in range (0,24):\n",
    "            x1[i][j*24+k]=a[k]\n",
    "\n",
    "#x2 (1704th~8519th column as x)\n",
    "for i in range (1,244):\n",
    "    b=np.array(data['cal_PMf'][6816*i:6816*i+6816])\n",
    "    for j in range(0,6816):\n",
    "        if b[j]=='\\\\N' :\n",
    "            b[j]=np.nan\n",
    "    for j in range(0,6816):\n",
    "        x2[i-1][j]=b[j]\n",
    "        \n",
    "print(np.shape(x1))\n",
    "print(np.shape(x2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6e8dc722",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] \"1 EOF ; RMS = 7.13544337\"\n",
      "[1] \"1 EOF ; RMS = 7.09625196\"\n",
      "[1] \"1 EOF ; RMS = 7.09722521\"\n",
      "[1] \"2 EOF ; RMS = 6.27948371\"\n",
      "[1] \"2 EOF ; RMS = 6.27709497\"\n",
      "[1] \"2 EOF ; RMS = 6.27696686\"\n",
      "[1] \"2 EOF ; RMS = 6.2769576\"\n",
      "[1] \"3 EOF ; RMS = 5.9109914\"\n",
      "[1] \"3 EOF ; RMS = 5.92209935\"\n",
      "[1] \"4 EOF ; RMS = 5.57778421\"\n",
      "[1] \"4 EOF ; RMS = 5.57372761\"\n",
      "[1] \"4 EOF ; RMS = 5.57350379\"\n",
      "[1] \"4 EOF ; RMS = 5.57347823\"\n",
      "[1] \"4 EOF ; RMS = 5.5734743\"\n",
      "[1] \"5 EOF ; RMS = 5.31674553\"\n",
      "[1] \"5 EOF ; RMS = 5.31855832\"\n",
      "[1] \"6 EOF ; RMS = 5.16658233\"\n",
      "[1] \"6 EOF ; RMS = 5.16906346\"\n",
      "[1] \"7 EOF ; RMS = 5.08883393\"\n",
      "[1] \"7 EOF ; RMS = 5.09603281\"\n",
      "[1] \"8 EOF ; RMS = 5.06102197\"\n",
      "[1] \"8 EOF ; RMS = 5.06821764\"\n",
      "[1] \"9 EOF ; RMS = 4.97645539\"\n",
      "[1] \"9 EOF ; RMS = 4.97205797\"\n",
      "[1] \"9 EOF ; RMS = 4.96990287\"\n",
      "[1] \"9 EOF ; RMS = 4.96868843\"\n",
      "[1] \"9 EOF ; RMS = 4.96801768\"\n",
      "[1] \"9 EOF ; RMS = 4.96764745\"\n",
      "[1] \"9 EOF ; RMS = 4.96744109\"\n",
      "[1] \"9 EOF ; RMS = 4.96732463\"\n",
      "[1] \"9 EOF ; RMS = 4.96725816\"\n",
      "[1] \"9 EOF ; RMS = 4.96721989\"\n",
      "[1] \"9 EOF ; RMS = 4.9671977\"\n",
      "[1] \"9 EOF ; RMS = 4.96718477\"\n",
      "[1] \"9 EOF ; RMS = 4.96717722\"\n",
      "[1] \"10 EOF ; RMS = 4.92609385\"\n",
      "[1] \"10 EOF ; RMS = 4.93230072\"\n",
      "[1] \"11 EOF ; RMS = 4.88605947\"\n",
      "[1] \"11 EOF ; RMS = 4.89352628\"\n",
      "[1] \"12 EOF ; RMS = 4.87428026\"\n",
      "[1] \"12 EOF ; RMS = 4.87966566\"\n",
      "[1] \"13 EOF ; RMS = 4.85909358\"\n",
      "[1] \"13 EOF ; RMS = 4.87075294\"\n",
      "[1] \"14 EOF ; RMS = 4.81619946\"\n",
      "[1] \"14 EOF ; RMS = 4.82201967\"\n",
      "[1] \"15 EOF ; RMS = 4.82049505\"\n",
      "[1] \"15 EOF ; RMS = 4.83933182\"\n",
      "[1] \"16 EOF ; RMS = 4.84812851\"\n",
      "[1] \"1 EOF ; RMS = 10.17613402\"\n",
      "[1] \"1 EOF ; RMS = 10.17437063\"\n",
      "[1] \"1 EOF ; RMS = 10.17450118\"\n",
      "[1] \"2 EOF ; RMS = 9.4004344\"\n",
      "[1] \"2 EOF ; RMS = 9.40126833\"\n",
      "[1] \"3 EOF ; RMS = 9.13285257\"\n",
      "[1] \"3 EOF ; RMS = 9.13822936\"\n",
      "[1] \"4 EOF ; RMS = 8.94208524\"\n",
      "[1] \"4 EOF ; RMS = 8.95428011\"\n",
      "[1] \"5 EOF ; RMS = 8.77973369\"\n",
      "[1] \"5 EOF ; RMS = 8.78576552\"\n",
      "[1] \"6 EOF ; RMS = 8.65471202\"\n",
      "[1] \"6 EOF ; RMS = 8.6650824\"\n",
      "[1] \"7 EOF ; RMS = 8.57120288\"\n",
      "[1] \"7 EOF ; RMS = 8.5895836\"\n",
      "[1] \"8 EOF ; RMS = 8.50441271\"\n",
      "[1] \"8 EOF ; RMS = 8.50709033\"\n",
      "[1] \"9 EOF ; RMS = 8.40680248\"\n",
      "[1] \"9 EOF ; RMS = 8.41139728\"\n",
      "[1] \"10 EOF ; RMS = 8.34071222\"\n",
      "[1] \"10 EOF ; RMS = 8.34961517\"\n",
      "[1] \"11 EOF ; RMS = 8.30576306\"\n",
      "[1] \"11 EOF ; RMS = 8.32834049\"\n",
      "[1] \"12 EOF ; RMS = 8.31919823\"\n",
      "[1] \"12 EOF ; RMS = 8.34677254\"\n",
      "[1] \"13 EOF ; RMS = 8.36219325\"\n"
     ]
    }
   ],
   "source": [
    "x1Restruct=sinkr.dineof(x1)\n",
    "x2Restruct=sinkr.dineof(x2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "09bc4b48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(243, 1704)\n",
      "(243, 6816)\n"
     ]
    }
   ],
   "source": [
    "x1Restruct_Fun=np.array(x1Restruct[0])\n",
    "x2Restruct_Fun=np.array(x2Restruct[0])\n",
    "print(np.shape(x1Restruct_Fun))\n",
    "print(np.shape(x2Restruct_Fun))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bbb6d0e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(239, 6816)\n"
     ]
    }
   ],
   "source": [
    "YRestruct_Fun=np.zeros([239,6816])\n",
    "for i in range (0,239):\n",
    "    for j in range (0,1704):\n",
    "        YRestruct_Fun[i][j]=x1Restruct_Fun[i+1][j]\n",
    "    for j in range (1704,3408):\n",
    "        YRestruct_Fun[i][j]=x1Restruct_Fun[i+2][j-1704]\n",
    "    for j in range (3408,5112):\n",
    "        YRestruct_Fun[i][j]=x1Restruct_Fun[i+3][j-3408]\n",
    "    for j in range (5112,6816):\n",
    "        YRestruct_Fun[i][j]=x1Restruct_Fun[i+4][j-5112]\n",
    "print(np.shape(YRestruct_Fun))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a6f5b298",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(239, 8520)\n"
     ]
    }
   ],
   "source": [
    "XRestruct_Fun=np.zeros([239,8520])\n",
    "for i in range (0,239):\n",
    "    for j in range (0,1704):\n",
    "        XRestruct_Fun[i][j]=x1Restruct_Fun[i][j]\n",
    "    for j in range (1704,8520):\n",
    "        XRestruct_Fun[i][j]=x2Restruct_Fun[i][j-1704]\n",
    "print(np.shape(XRestruct_Fun))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "60f7508e",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] \"1 EOF ; RMS = 9.5505826\"\n",
      "[1] \"1 EOF ; RMS = 9.54045039\"\n",
      "[1] \"1 EOF ; RMS = 9.5405316\"\n",
      "[1] \"2 EOF ; RMS = 8.79432057\"\n",
      "[1] \"2 EOF ; RMS = 8.79431739\"\n",
      "[1] \"3 EOF ; RMS = 8.56430541\"\n",
      "[1] \"3 EOF ; RMS = 8.56714458\"\n",
      "[1] \"4 EOF ; RMS = 8.35045958\"\n",
      "[1] \"4 EOF ; RMS = 8.35452706\"\n",
      "[1] \"5 EOF ; RMS = 8.17712072\"\n",
      "[1] \"5 EOF ; RMS = 8.17761129\"\n",
      "[1] \"6 EOF ; RMS = 8.03637738\"\n",
      "[1] \"6 EOF ; RMS = 8.04082152\"\n",
      "[1] \"7 EOF ; RMS = 7.93151963\"\n",
      "[1] \"7 EOF ; RMS = 7.93463602\"\n",
      "[1] \"8 EOF ; RMS = 7.85853584\"\n",
      "[1] \"8 EOF ; RMS = 7.87084951\"\n",
      "[1] \"9 EOF ; RMS = 7.75872786\"\n",
      "[1] \"9 EOF ; RMS = 7.7596391\"\n",
      "[1] \"10 EOF ; RMS = 7.7097736\"\n",
      "[1] \"10 EOF ; RMS = 7.72357843\"\n",
      "[1] \"11 EOF ; RMS = 7.68024963\"\n",
      "[1] \"11 EOF ; RMS = 7.69140625\"\n",
      "[1] \"12 EOF ; RMS = 7.66417839\"\n",
      "[1] \"12 EOF ; RMS = 7.67838404\"\n",
      "[1] \"13 EOF ; RMS = 7.61631043\"\n",
      "[1] \"13 EOF ; RMS = 7.62514077\"\n",
      "[1] \"14 EOF ; RMS = 7.61311857\"\n",
      "[1] \"14 EOF ; RMS = 7.63079602\"\n",
      "[1] \"15 EOF ; RMS = 7.60906558\"\n",
      "[1] \"15 EOF ; RMS = 7.62602666\"\n",
      "[1] \"16 EOF ; RMS = 7.61326186\"\n",
      "[1] \"16 EOF ; RMS = 7.62845258\"\n",
      "[1] \"17 EOF ; RMS = 7.6061742\"\n",
      "[1] \"17 EOF ; RMS = 7.62068154\"\n",
      "[1] \"18 EOF ; RMS = 7.62053886\"\n",
      "[1] \"18 EOF ; RMS = 7.64843697\"\n",
      "[1] \"19 EOF ; RMS = 7.645838\"\n",
      "[1] \"19 EOF ; RMS = 7.6733613\"\n",
      "[1] \"20 EOF ; RMS = 7.65539508\"\n",
      "[1] \"20 EOF ; RMS = 7.67671023\"\n",
      "[1] \"21 EOF ; RMS = 7.67297606\"\n",
      "[1] \"21 EOF ; RMS = 7.69474573\"\n",
      "[1] \"22 EOF ; RMS = 7.66895453\"\n",
      "[1] \"22 EOF ; RMS = 7.68776737\"\n",
      "[1] \"23 EOF ; RMS = 7.67142148\"\n",
      "[1] \"23 EOF ; RMS = 7.68953635\"\n",
      "[1] \"24 EOF ; RMS = 7.69143629\"\n",
      "[1] \"1 EOF ; RMS = 7.92818132\"\n",
      "[1] \"1 EOF ; RMS = 7.89020331\"\n",
      "[1] \"1 EOF ; RMS = 7.88847365\"\n",
      "[1] \"1 EOF ; RMS = 7.88822921\"\n",
      "[1] \"1 EOF ; RMS = 7.88818824\"\n",
      "[1] \"1 EOF ; RMS = 7.88818102\"\n",
      "[1] \"2 EOF ; RMS = 7.35375839\"\n",
      "[1] \"2 EOF ; RMS = 7.35398513\"\n",
      "[1] \"3 EOF ; RMS = 6.85870268\"\n",
      "[1] \"3 EOF ; RMS = 6.85842569\"\n",
      "[1] \"3 EOF ; RMS = 6.85864946\"\n",
      "[1] \"4 EOF ; RMS = 6.57050103\"\n",
      "[1] \"4 EOF ; RMS = 6.5703262\"\n",
      "[1] \"4 EOF ; RMS = 6.57040229\"\n",
      "[1] \"5 EOF ; RMS = 6.40620498\"\n",
      "[1] \"5 EOF ; RMS = 6.40672313\"\n",
      "[1] \"6 EOF ; RMS = 6.26402266\"\n",
      "[1] \"6 EOF ; RMS = 6.26545284\"\n",
      "[1] \"7 EOF ; RMS = 6.16529189\"\n",
      "[1] \"7 EOF ; RMS = 6.16931164\"\n",
      "[1] \"8 EOF ; RMS = 6.05461018\"\n",
      "[1] \"8 EOF ; RMS = 6.0562433\"\n",
      "[1] \"9 EOF ; RMS = 5.9481674\"\n",
      "[1] \"9 EOF ; RMS = 5.95035235\"\n",
      "[1] \"10 EOF ; RMS = 5.88363993\"\n",
      "[1] \"10 EOF ; RMS = 5.88801043\"\n",
      "[1] \"11 EOF ; RMS = 5.79236098\"\n",
      "[1] \"11 EOF ; RMS = 5.79324575\"\n",
      "[1] \"12 EOF ; RMS = 5.73656203\"\n",
      "[1] \"12 EOF ; RMS = 5.74188121\"\n",
      "[1] \"13 EOF ; RMS = 5.68772653\"\n",
      "[1] \"13 EOF ; RMS = 5.69144495\"\n",
      "[1] \"14 EOF ; RMS = 5.6351413\"\n",
      "[1] \"14 EOF ; RMS = 5.63913497\"\n",
      "[1] \"15 EOF ; RMS = 5.59710512\"\n",
      "[1] \"15 EOF ; RMS = 5.60351881\"\n",
      "[1] \"16 EOF ; RMS = 5.57662303\"\n",
      "[1] \"16 EOF ; RMS = 5.5824756\"\n",
      "[1] \"17 EOF ; RMS = 5.55385681\"\n",
      "[1] \"17 EOF ; RMS = 5.55819424\"\n",
      "[1] \"18 EOF ; RMS = 5.52021084\"\n",
      "[1] \"18 EOF ; RMS = 5.52144731\"\n",
      "[1] \"19 EOF ; RMS = 5.49736142\"\n",
      "[1] \"19 EOF ; RMS = 5.50267756\"\n",
      "[1] \"20 EOF ; RMS = 5.47375559\"\n",
      "[1] \"20 EOF ; RMS = 5.47732567\"\n",
      "[1] \"21 EOF ; RMS = 5.45420337\"\n",
      "[1] \"21 EOF ; RMS = 5.46070355\"\n",
      "[1] \"22 EOF ; RMS = 5.43532087\"\n",
      "[1] \"22 EOF ; RMS = 5.44212885\"\n",
      "[1] \"23 EOF ; RMS = 5.42921293\"\n",
      "[1] \"23 EOF ; RMS = 5.43838928\"\n",
      "[1] \"24 EOF ; RMS = 5.429708\"\n",
      "[1] \"24 EOF ; RMS = 5.43983399\"\n",
      "[1] \"25 EOF ; RMS = 5.42467706\"\n",
      "[1] \"25 EOF ; RMS = 5.42976744\"\n",
      "[1] \"26 EOF ; RMS = 5.41799348\"\n",
      "[1] \"26 EOF ; RMS = 5.42410709\"\n",
      "[1] \"27 EOF ; RMS = 5.40477933\"\n",
      "[1] \"27 EOF ; RMS = 5.410392\"\n",
      "[1] \"28 EOF ; RMS = 5.3874845\"\n",
      "[1] \"28 EOF ; RMS = 5.39223682\"\n",
      "[1] \"29 EOF ; RMS = 5.38504102\"\n",
      "[1] \"29 EOF ; RMS = 5.39386182\"\n",
      "[1] \"30 EOF ; RMS = 5.3747749\"\n",
      "[1] \"30 EOF ; RMS = 5.37985009\"\n",
      "[1] \"31 EOF ; RMS = 5.36991489\"\n",
      "[1] \"31 EOF ; RMS = 5.37921491\"\n",
      "[1] \"32 EOF ; RMS = 5.38199388\"\n"
     ]
    }
   ],
   "source": [
    "Xhat=XRestruct_Fun\n",
    "Yhat=YRestruct_Fun\n",
    "Xhat_train = np.zeros([239,8520])\n",
    "Yhat_train = np.zeros([239,6816])\n",
    "for i in range (0,239):\n",
    "    for j in range (0,8520):\n",
    "        Xhat_train[i][j] = Xhat[i][j]\n",
    "    for j in range (0,6816):    \n",
    "        Yhat_train[i][j] = Yhat[i][j]\n",
    "\n",
    "Xhat_test = np.zeros([30,8520])\n",
    "Yhat_test = np.zeros([30,6816])\n",
    "x=np.zeros([273,8520])\n",
    "y=np.zeros([273,6816])\n",
    "#x1 (0th~1703th column as x)\n",
    "for i in range (0,273):\n",
    "    for j in range (0,71):\n",
    "        a=np.array(data['obs_PMf'][6816*i+96*j:6816*i+96*j+24])\n",
    "        for k in range (0,24):\n",
    "            if a[k]=='\\\\N' :\n",
    "                a[k]=np.nan\n",
    "        for k in range (0,24):\n",
    "            x[i][j*24+k]=a[k]\n",
    "\n",
    "#x2 (1704th~8519th column as x)\n",
    "for i in range (1,274):\n",
    "    b=np.array(data['cal_PMf'][6816*i:6816*i+6816])\n",
    "    for j in range(0,6816):\n",
    "        if b[j]=='\\\\N' :\n",
    "            b[j]=np.nan\n",
    "    for j in range(0,6816):\n",
    "        x[i-1][j+1704]=b[j]\n",
    "\n",
    "#data_obs\n",
    "for i in range (1,274):\n",
    "    a=np.array(data['obs_PMf'][6816*i:6816*i+6816])\n",
    "    for j in range(0,6816):\n",
    "        if a[j]=='\\\\N' :\n",
    "            a[j]=np.nan\n",
    "    y[i-1]=a\n",
    "xRestruct=sinkr.dineof(x)\n",
    "yRestruct=sinkr.dineof(y)\n",
    "xRestruct_Fun=np.array(xRestruct[0])\n",
    "yRestruct_Fun=np.array(yRestruct[0])\n",
    "    \n",
    "for i in range (243,273):\n",
    "    for j in range (0,8520):\n",
    "        Xhat_test[i-243][j]=xRestruct_Fun[i][j]\n",
    "    for j in range(0,6816):\n",
    "        Yhat_test[i-243][j]=yRestruct_Fun[i][j]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3f3ce44e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "September (testing data set) before_error (MADE) : 7.586688\n"
     ]
    }
   ],
   "source": [
    "old_x=np.zeros([30,6816])\n",
    "old_y=np.zeros([30,6816])\n",
    "for i in range (0,30):\n",
    "    for j in range (0,6816):\n",
    "        old_x[i][j]=x[i+243][j+1704]\n",
    "        old_y[i][j]=y[i+243][j]\n",
    "before=abs(old_x-old_y)\n",
    "before_error=np.nanmean(before)\n",
    "print(\"September (testing data set) before_error (MADE) : %f\" %before_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "0b57da44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Xhat_train\n",
      "(239, 8520)\n",
      "Yhat_train\n",
      "(239, 6816)\n",
      "Xhat_test\n",
      "(30, 8520)\n",
      "Yhat_test\n",
      "(30, 6816)\n"
     ]
    }
   ],
   "source": [
    "print('Xhat_train')\n",
    "#print(Xhat_train)\n",
    "print(np.shape(Xhat_train))\n",
    "print('Yhat_train')\n",
    "#print(Yhat_train)\n",
    "print(np.shape(Yhat_train))\n",
    "print('Xhat_test')\n",
    "#print(Xhat_test)\n",
    "print(np.shape(Xhat_test))\n",
    "print('Yhat_test')\n",
    "#print(Yhat_test)\n",
    "print(np.shape(Yhat_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5430dd10",
   "metadata": {},
   "source": [
    "## GD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "6b056926",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = Xhat_train\n",
    "y = Yhat_train\n",
    "xt = Xhat_test\n",
    "yt = Yhat_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "e34ab6bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model (x,b0,b1):\n",
    "    # y = b0 +  torch.matmul(x,b1)\n",
    "    # y = torch.add(b0 +  torch.matmul(x,b1))\n",
    "    y = b0 +  torch.mm(x , b1)\n",
    "#     y_8520 = b0 + b1*x\n",
    "#     y = y_8520[ : , 1704: ]\n",
    "\n",
    "    # y = torch.zeros(len(y_8520),6816)\n",
    "    # for i in range (0,len(y_8520)):\n",
    "    #     for j in range (0,6816):\n",
    "    #         y[i][j] = y_8520[i][j+1704]\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "68b546f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Training loss 128670.6719, Testing loss 122753.0257\n",
      "Epoch 1, Training loss 128541.9922, Testing loss 122630.1358\n",
      "Epoch 2, Training loss 128413.3125, Testing loss 122507.2494\n",
      "Epoch 3, Training loss 128284.6250, Testing loss 122384.3478\n",
      "Epoch 4, Training loss 128155.9219, Testing loss 122261.4567\n",
      "Epoch 5, Training loss 128027.2344, Testing loss 122138.5731\n",
      "Epoch 6, Training loss 127898.5625, Testing loss 122015.6970\n",
      "Epoch 7, Training loss 127769.9062, Testing loss 121892.7921\n",
      "Epoch 8, Training loss 127641.2031, Testing loss 121769.9038\n",
      "Epoch 9, Training loss 127512.5234, Testing loss 121647.0173\n",
      "Epoch 10, Training loss 127383.8125, Testing loss 121524.1272\n",
      "Epoch 20, Training loss 126097.0000, Testing loss 120295.2564\n",
      "Epoch 30, Training loss 124810.1328, Testing loss 119066.3270\n",
      "Epoch 40, Training loss 123523.3125, Testing loss 117837.4366\n",
      "Epoch 50, Training loss 122236.4609, Testing loss 116608.5334\n",
      "Epoch 60, Training loss 120949.6562, Testing loss 115379.6416\n",
      "Epoch 70, Training loss 119662.7891, Testing loss 114150.7525\n",
      "Epoch 80, Training loss 118375.9531, Testing loss 112921.8521\n",
      "Epoch 90, Training loss 117089.1094, Testing loss 111692.9500\n",
      "Epoch 100, Training loss 115802.2656, Testing loss 110464.0574\n",
      "Epoch 110, Training loss 114515.4297, Testing loss 109235.1561\n",
      "Epoch 120, Training loss 113228.6016, Testing loss 108006.2623\n",
      "Epoch 130, Training loss 111941.7656, Testing loss 106777.3650\n",
      "Epoch 140, Training loss 110654.9219, Testing loss 105548.4686\n",
      "Epoch 150, Training loss 109368.0625, Testing loss 104319.5651\n",
      "Epoch 160, Training loss 108081.2422, Testing loss 103090.6752\n",
      "Epoch 170, Training loss 106794.3984, Testing loss 101861.7761\n",
      "Epoch 180, Training loss 105507.5625, Testing loss 100632.8853\n",
      "Epoch 190, Training loss 104220.7188, Testing loss 99403.9896\n",
      "Epoch 200, Training loss 102933.8750, Testing loss 98175.0869\n",
      "Epoch 210, Training loss 101647.0547, Testing loss 96946.1972\n",
      "Epoch 220, Training loss 100360.2188, Testing loss 95717.3029\n",
      "Epoch 230, Training loss 99073.3594, Testing loss 94488.3930\n",
      "Epoch 240, Training loss 97786.5156, Testing loss 93259.5112\n",
      "Epoch 250, Training loss 96499.6719, Testing loss 92030.6136\n",
      "Epoch 260, Training loss 95212.8438, Testing loss 90801.7056\n",
      "Epoch 270, Training loss 93926.0156, Testing loss 89572.8223\n",
      "Epoch 280, Training loss 92639.1641, Testing loss 88343.9212\n",
      "Epoch 290, Training loss 91352.3281, Testing loss 87115.0261\n",
      "Epoch 300, Training loss 90065.4844, Testing loss 85886.1253\n",
      "Epoch 310, Training loss 88778.6406, Testing loss 84657.2266\n",
      "Epoch 320, Training loss 87491.8047, Testing loss 83428.3284\n",
      "Epoch 330, Training loss 86204.9531, Testing loss 82199.4396\n",
      "Epoch 340, Training loss 84918.1250, Testing loss 80970.5377\n",
      "Epoch 350, Training loss 83631.2891, Testing loss 79741.6420\n",
      "Epoch 360, Training loss 82344.4609, Testing loss 78512.7499\n",
      "Epoch 370, Training loss 81057.6016, Testing loss 77283.8513\n",
      "Epoch 380, Training loss 79770.7734, Testing loss 76054.9550\n",
      "Epoch 390, Training loss 78483.9453, Testing loss 74826.0611\n",
      "Epoch 400, Training loss 77197.0938, Testing loss 73597.1655\n",
      "Epoch 410, Training loss 75910.2578, Testing loss 72368.2625\n",
      "Epoch 420, Training loss 74623.4062, Testing loss 71139.3666\n",
      "Epoch 430, Training loss 73336.5859, Testing loss 69910.4731\n",
      "Epoch 440, Training loss 72049.7344, Testing loss 68681.5662\n",
      "Epoch 450, Training loss 70762.8906, Testing loss 67452.6754\n",
      "Epoch 460, Training loss 69476.0547, Testing loss 66223.7815\n",
      "Epoch 470, Training loss 68189.2266, Testing loss 64994.8857\n",
      "Epoch 480, Training loss 66902.3828, Testing loss 63765.9871\n",
      "Epoch 490, Training loss 65615.5391, Testing loss 62537.0897\n",
      "Epoch 500, Training loss 64328.7070, Testing loss 61308.1944\n",
      "Epoch 510, Training loss 63041.8633, Testing loss 60079.2969\n",
      "Epoch 520, Training loss 61755.0234, Testing loss 58850.3974\n",
      "Epoch 530, Training loss 60468.1797, Testing loss 57621.5028\n",
      "Epoch 540, Training loss 59181.3477, Testing loss 56392.6109\n",
      "Epoch 550, Training loss 57894.5000, Testing loss 55163.7127\n",
      "Epoch 560, Training loss 56607.6680, Testing loss 53934.8147\n",
      "Epoch 570, Training loss 55320.8281, Testing loss 52705.9209\n",
      "Epoch 580, Training loss 54033.9922, Testing loss 51477.0175\n",
      "Epoch 590, Training loss 52747.1484, Testing loss 50248.1276\n",
      "Epoch 600, Training loss 51460.3086, Testing loss 49019.2248\n",
      "Epoch 610, Training loss 50173.4648, Testing loss 47790.3412\n",
      "Epoch 620, Training loss 48886.6328, Testing loss 46561.4378\n",
      "Epoch 630, Training loss 47599.7930, Testing loss 45332.5373\n",
      "Epoch 640, Training loss 46312.9531, Testing loss 44103.6535\n",
      "Epoch 650, Training loss 45026.1094, Testing loss 42874.7475\n",
      "Epoch 660, Training loss 43739.2695, Testing loss 41645.8508\n",
      "Epoch 670, Training loss 42452.4375, Testing loss 40416.9545\n",
      "Epoch 680, Training loss 41165.5898, Testing loss 39188.0577\n",
      "Epoch 690, Training loss 39878.7539, Testing loss 37959.1589\n",
      "Epoch 700, Training loss 38591.9141, Testing loss 36730.2641\n",
      "Epoch 710, Training loss 37305.0742, Testing loss 35501.3713\n",
      "Epoch 720, Training loss 36018.2383, Testing loss 34272.4701\n",
      "Epoch 730, Training loss 34731.3984, Testing loss 33043.5763\n",
      "Epoch 740, Training loss 33444.5625, Testing loss 31814.6778\n",
      "Epoch 750, Training loss 32157.7168, Testing loss 30585.7816\n",
      "Epoch 760, Training loss 30870.8730, Testing loss 29356.8766\n",
      "Epoch 770, Training loss 29584.0293, Testing loss 28127.9760\n",
      "Epoch 780, Training loss 28297.1816, Testing loss 26899.0744\n",
      "Epoch 790, Training loss 27010.3379, Testing loss 25670.1719\n",
      "Epoch 800, Training loss 25723.4883, Testing loss 24441.2692\n",
      "Epoch 810, Training loss 24436.6484, Testing loss 23212.3657\n",
      "Epoch 820, Training loss 23149.7988, Testing loss 21983.4649\n",
      "Epoch 830, Training loss 21862.9570, Testing loss 20754.5627\n",
      "Epoch 840, Training loss 20576.1094, Testing loss 19525.6596\n",
      "Epoch 850, Training loss 19289.2637, Testing loss 18296.7573\n",
      "Epoch 860, Training loss 18002.4199, Testing loss 17067.8545\n",
      "Epoch 870, Training loss 16715.5742, Testing loss 15838.9531\n",
      "Epoch 880, Training loss 15428.7275, Testing loss 14610.0502\n",
      "Epoch 890, Training loss 14141.8828, Testing loss 13381.1480\n",
      "Epoch 900, Training loss 12855.0371, Testing loss 12152.2450\n",
      "Epoch 910, Training loss 11568.1904, Testing loss 10923.3422\n",
      "Epoch 920, Training loss 10281.3447, Testing loss 9694.4404\n",
      "Epoch 930, Training loss 8994.4990, Testing loss 8465.5375\n",
      "Epoch 940, Training loss 7707.6538, Testing loss 7236.6348\n",
      "Epoch 950, Training loss 6420.8076, Testing loss 6007.7317\n",
      "Epoch 960, Training loss 5133.9619, Testing loss 4778.8292\n",
      "Epoch 970, Training loss 3847.1165, Testing loss 3549.9262\n",
      "Epoch 980, Training loss 2560.2700, Testing loss 2321.0233\n",
      "Epoch 990, Training loss 1273.4241, Testing loss 1092.1206\n",
      "Epoch 1000, Training loss 13.4410, Testing loss 112.5462\n",
      "Epoch 1010, Training loss 237.5555, Testing loss 182.2982\n",
      "Epoch 1020, Training loss 119.8089, Testing loss 87.1515\n",
      "Epoch 1030, Training loss 26.0681, Testing loss 24.2003\n",
      "Epoch 1040, Training loss 27.7778, Testing loss 12.9332\n",
      "Epoch 1050, Training loss 13.3820, Testing loss 13.4943\n",
      "Epoch 1060, Training loss 10.5944, Testing loss 9.9544\n",
      "Epoch 1070, Training loss 7.5211, Testing loss 8.7577\n",
      "Epoch 1080, Training loss 6.3241, Testing loss 8.0807\n",
      "Epoch 1090, Training loss 5.5041, Testing loss 7.8156\n",
      "Epoch 1100, Training loss 5.1049, Testing loss 7.7772\n",
      "Epoch 1110, Training loss 5.1050, Testing loss 8.0211\n",
      "Epoch 1120, Training loss 5.6295, Testing loss 8.3645\n",
      "Epoch 1130, Training loss 6.2667, Testing loss 8.9670\n",
      "Epoch 1140, Training loss 6.9352, Testing loss 9.3132\n",
      "Epoch 1150, Training loss 7.3163, Testing loss 9.6078\n",
      "Epoch 1160, Training loss 7.6905, Testing loss 9.7420\n",
      "Epoch 1170, Training loss 7.9362, Testing loss 10.0912\n",
      "Epoch 1180, Training loss 8.4956, Testing loss 10.3456\n",
      "Epoch 1190, Training loss 8.6722, Testing loss 10.5945\n",
      "Epoch 1200, Training loss 8.9973, Testing loss 10.7868\n",
      "Epoch 1210, Training loss 9.1243, Testing loss 10.9015\n",
      "Epoch 1220, Training loss 9.3992, Testing loss 11.0315\n",
      "Epoch 1230, Training loss 9.4723, Testing loss 11.2991\n",
      "Epoch 1240, Training loss 9.8560, Testing loss 11.3411\n",
      "Epoch 1250, Training loss 9.7270, Testing loss 11.4063\n",
      "Epoch 1260, Training loss 9.9812, Testing loss 11.5960\n",
      "Epoch 1270, Training loss 9.9743, Testing loss 11.5001\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1280, Training loss 10.0857, Testing loss 11.6399\n",
      "Epoch 1290, Training loss 10.2695, Testing loss 11.9686\n",
      "Epoch 1300, Training loss 10.3951, Testing loss 11.9780\n",
      "Epoch 1310, Training loss 10.5427, Testing loss 11.9272\n",
      "Epoch 1320, Training loss 10.6879, Testing loss 12.1079\n",
      "Epoch 1330, Training loss 10.7959, Testing loss 12.1938\n",
      "Epoch 1340, Training loss 10.9407, Testing loss 12.3117\n",
      "Epoch 1350, Training loss 10.9035, Testing loss 12.2909\n",
      "Epoch 1360, Training loss 11.0612, Testing loss 12.2939\n",
      "Epoch 1370, Training loss 11.0371, Testing loss 12.3946\n",
      "Epoch 1380, Training loss 11.2386, Testing loss 12.5346\n",
      "Epoch 1390, Training loss 11.1920, Testing loss 12.4535\n",
      "Epoch 1400, Training loss 11.2378, Testing loss 12.5532\n",
      "Epoch 1410, Training loss 11.2763, Testing loss 12.6291\n",
      "Epoch 1420, Training loss 11.3757, Testing loss 12.6587\n",
      "Epoch 1430, Training loss 11.4273, Testing loss 12.6531\n",
      "Epoch 1440, Training loss 11.5753, Testing loss 12.7948\n",
      "Epoch 1450, Training loss 11.5640, Testing loss 12.6833\n",
      "Epoch 1460, Training loss 11.5677, Testing loss 12.8001\n",
      "Epoch 1470, Training loss 11.6999, Testing loss 12.8304\n",
      "Epoch 1480, Training loss 11.6565, Testing loss 12.8097\n",
      "Epoch 1490, Training loss 11.6353, Testing loss 12.8329\n",
      "Epoch 1500, Training loss 11.7979, Testing loss 12.9157\n",
      "Epoch 1510, Training loss 11.8046, Testing loss 12.7920\n",
      "Epoch 1520, Training loss 11.6323, Testing loss 12.7778\n",
      "Epoch 1530, Training loss 11.7722, Testing loss 12.9034\n",
      "Epoch 1540, Training loss 11.7502, Testing loss 12.8572\n",
      "Epoch 1550, Training loss 11.7871, Testing loss 12.8725\n",
      "Epoch 1560, Training loss 11.7965, Testing loss 12.9958\n",
      "Epoch 1570, Training loss 11.8386, Testing loss 12.9214\n",
      "Epoch 1580, Training loss 11.7839, Testing loss 12.9958\n",
      "Epoch 1590, Training loss 11.9517, Testing loss 12.9523\n",
      "Epoch 1600, Training loss 11.9440, Testing loss 12.9170\n",
      "Epoch 1610, Training loss 11.8753, Testing loss 12.9562\n",
      "Epoch 1620, Training loss 11.9443, Testing loss 13.0735\n",
      "Epoch 1630, Training loss 11.9693, Testing loss 12.9869\n",
      "Epoch 1640, Training loss 11.9378, Testing loss 12.9959\n",
      "Epoch 1650, Training loss 12.0144, Testing loss 13.0325\n",
      "Epoch 1660, Training loss 12.0069, Testing loss 12.9909\n",
      "Epoch 1670, Training loss 11.9232, Testing loss 12.9631\n",
      "Epoch 1680, Training loss 12.1087, Testing loss 13.0454\n",
      "Epoch 1690, Training loss 12.0105, Testing loss 13.0297\n",
      "Epoch 1700, Training loss 11.9752, Testing loss 13.0003\n",
      "Epoch 1710, Training loss 12.0556, Testing loss 13.0871\n",
      "Epoch 1720, Training loss 12.0567, Testing loss 12.9832\n",
      "Epoch 1730, Training loss 11.8919, Testing loss 13.0066\n",
      "Epoch 1740, Training loss 12.0794, Testing loss 13.0865\n",
      "Epoch 1750, Training loss 12.0500, Testing loss 13.0337\n",
      "Epoch 1760, Training loss 11.9664, Testing loss 12.9824\n",
      "Epoch 1770, Training loss 12.0291, Testing loss 13.1186\n",
      "Epoch 1780, Training loss 12.0669, Testing loss 13.0035\n",
      "Epoch 1790, Training loss 11.9582, Testing loss 12.9853\n",
      "Epoch 1800, Training loss 12.0819, Testing loss 13.0822\n",
      "Epoch 1810, Training loss 12.0708, Testing loss 13.0378\n",
      "Epoch 1820, Training loss 11.9860, Testing loss 13.0174\n",
      "Epoch 1830, Training loss 12.1005, Testing loss 13.1345\n",
      "Epoch 1840, Training loss 12.1264, Testing loss 13.0283\n",
      "Epoch 1850, Training loss 11.9580, Testing loss 13.0537\n",
      "Epoch 1860, Training loss 12.0823, Testing loss 13.0975\n",
      "Epoch 1870, Training loss 12.0560, Testing loss 13.0248\n",
      "Epoch 1880, Training loss 12.0538, Testing loss 13.0364\n",
      "Epoch 1890, Training loss 12.1000, Testing loss 13.0999\n",
      "Epoch 1900, Training loss 12.0442, Testing loss 13.0367\n",
      "Epoch 1910, Training loss 12.0063, Testing loss 13.0329\n",
      "Epoch 1920, Training loss 12.1449, Testing loss 13.0910\n",
      "Epoch 1930, Training loss 12.0775, Testing loss 13.0474\n",
      "Epoch 1940, Training loss 11.9722, Testing loss 13.0083\n",
      "Epoch 1950, Training loss 12.0934, Testing loss 13.0706\n",
      "Epoch 1960, Training loss 12.1301, Testing loss 13.0543\n",
      "Epoch 1970, Training loss 12.0013, Testing loss 13.0690\n",
      "Epoch 1980, Training loss 12.1148, Testing loss 13.1021\n",
      "Epoch 1990, Training loss 12.0969, Testing loss 13.0237\n",
      "Epoch 1991, Training loss 12.0003, Testing loss 13.2033\n",
      "Epoch 1992, Training loss 12.1563, Testing loss 13.1331\n",
      "Epoch 1993, Training loss 12.0830, Testing loss 13.0159\n",
      "Epoch 1994, Training loss 12.0193, Testing loss 13.0751\n",
      "Epoch 1995, Training loss 12.1601, Testing loss 13.0353\n",
      "Epoch 1996, Training loss 12.0590, Testing loss 13.0277\n",
      "Epoch 1997, Training loss 12.0512, Testing loss 13.1254\n",
      "Epoch 1998, Training loss 12.1090, Testing loss 13.1032\n",
      "Epoch 1999, Training loss 12.0148, Testing loss 13.0832\n",
      "Epoch 2000, Training loss 12.0523, Testing loss 13.0268\n"
     ]
    }
   ],
   "source": [
    "features = torch.from_numpy(x)\n",
    "targets = torch.from_numpy(y)\n",
    "x_test = torch.from_numpy(xt)\n",
    "y_test = torch.from_numpy(yt)\n",
    "\n",
    "# beta0 = torch.randn(6816 , requires_grad = True)\n",
    "# beta1 = torch.randn([8520 , 6816], requires_grad = True)\n",
    "\n",
    "beta0 = torch.ones(6816 , requires_grad = True)\n",
    "beta1 = torch.ones([8520,6816], requires_grad = True)\n",
    "\n",
    "rate = 1e-3\n",
    "# optimizer = optim.LBFGS([beta0 , beta1] , lr = rate)\n",
    "optimizer = optim.Adam([beta0 , beta1], lr=rate)\n",
    "# optimizer = optim.SGD([beta0 , beta1], lr=rate)\n",
    "\n",
    "epo = 2001\n",
    "loss = nn.L1Loss()\n",
    "train_error = np.zeros(epo)\n",
    "test_error = np.zeros(epo)\n",
    "\n",
    "\n",
    "for epoch in range (epo):\n",
    "    yhats_train = model(features.float() , beta0 , beta1)\n",
    "    train_loss = loss(targets.float() , yhats_train)\n",
    "    train_error[epoch] = train_loss\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    train_loss.backward() \n",
    "    optimizer.step()    \n",
    "\n",
    "    yhats_test = model(x_test.float(), beta0, beta1) \n",
    "#     for i in range (25):\n",
    "#         for j in range (6816):\n",
    "#             if y_test[i][j] == 0:\n",
    "#                 yhats_test[i][j] = 0\n",
    "    r = torch.abs(yhats_test - y_test)\n",
    "    test_loss = torch.nanmean(r)\n",
    "    # test_loss = loss(y_test , yhats_test)\n",
    "    test_error[epoch] = test_loss\n",
    "\n",
    "    if epoch <= 10 or epoch % 10 == 0:\n",
    "        print(f\"Epoch {epoch}, Training loss {train_loss.item():.4f},\"\n",
    "                    f\" Testing loss {test_loss.item():.4f}\")\n",
    "        # print('\\tBeta_0 : ' , beta0)\n",
    "        # print('\\tBeta_1 : ' , beta1)\n",
    "    else :\n",
    "        if epoch >= epo-10 :\n",
    "            print(f\"Epoch {epoch}, Training loss {train_loss.item():.4f},\"\n",
    "                        f\" Testing loss {test_loss.item():.4f}\")\n",
    "            # print('\\tBeta_0 : ' , beta0)\n",
    "            # print('\\tBeta_1 : ' , beta1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "1dac5e00",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYkAAAEICAYAAACqMQjAAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/YYfK9AAAACXBIWXMAAAsTAAALEwEAmpwYAAAxHElEQVR4nO3dd3hUdfbH8ffJpBECBEJASECCIgqIAbKAHWyADWwI4squFcWKDctv1XVddV1FWQUWxRUEKWIBFyyIumJBDEWqSChCkBJaKCGQ8v39MRccY0JInZTP63nmyZ1zy5zcJHNy7/feM+acQ0REpCAhwU5AREQqLxUJEREplIqEiIgUSkVCREQKpSIhIiKFUpEQEZFCqUiIiEihVCRESsjM1pnZQTNrmC++0MycmbUIiD3uxbrkW/ZPZpZrZnvzPZpW0LchckQqEiKlsxbof+iJmZ0MRAUuYGYGXAfs8L7m961zLjrf45fyTFrkaKlIiJTOm/z2jX8gMC7fMmcCTYA7gX5mFl5BuYmUmoqESOnMBeqa2Ulm5gP6AePzLTMQ+ACY4j2/pALzEykVFQmR0jt0NHE+sALYeGiGmUUBVwFvOeeygan8/pRTVzPbFfBYXUF5ixQpNNgJiFQDbwJfAon8/lTTZUAOMNN7PgH41MzinHPpXmyuc+6MCslUpJh0JCFSSs65n/EPYF8IvJtv9kAgGlhvZpuBt4Ew4JoKTVKkhHQkIVI2bgDqO+f2mdmhv6t44FygF7A4YNm78Z9yeqlCMxQpARUJkTLgnCtoHOFMYJFz7pPAoJkNB+41s3Ze6FQz25tv3e7Oue/LIVWRYjF96JCIiBRGYxIiIlIoFQkRESmUioSIiBRKRUJERApV7a5uatiwoWvRokWw0xARqVLmz5+/zTkXlz9e7YpEixYtSElJCXYaIiJVipn9XFBcp5tERKRQKhIiIlIoFQkRESlUtRuTKEh2djZpaWlkZWUFO5VKLzIykoSEBMLCwoKdiohUAjWiSKSlpVGnTh1atGiB/5MkpSDOObZv305aWhqJiYnBTkdEKoEacbopKyuL2NhYFYgimBmxsbE64hKRw2pEkQBUII6S9pOIBKoxRaIomQdy2LpH/0GLiARSkfDs2p/N5owsMvZnl/m2t2/fTlJSEklJSRxzzDHEx8cffn7w4MEjrpuSksKdd95Z5GucdtppZZWuiMhhNWLg+mgcUy+SzIO5pO3IJLJRNBFhvjLbdmxsLIsWLQLg8ccfJzo6mvvuu+/w/JycHEJDC/5RJCcnk5ycXORrfPPNN2WSq4hIIB1JeELMaN6gFhj8vCOTvLzy/TCmP/3pTwwaNIguXbrwwAMPMG/ePE499VQ6dOjAaaedxsqVKwH44osvuPjiiwF/gbn++uvp1q0bLVu2ZPjw4Ye3Fx0dfXj5bt26ceWVV3LiiScyYMAADn2w1MyZMznxxBPp1KkTd9555+HtiogUpsYdSTzxwTKW/7K70Pm5eY6s7FxCfSFEhB5dDW3TtC6PXdK22LmkpaXxzTff4PP52L17N3PmzCE0NJRPP/2Uhx9+mHfeeed36/z44498/vnn7Nmzh9atW3Prrbf+7p6GhQsXsmzZMpo2bcrpp5/O119/TXJyMrfccgtffvkliYmJ9O/fv9j5ikjNU+OKRFF8IUZYaAjZOXn4QiA0pPwOtq666ip8Pv9prYyMDAYOHMiqVaswM7KzCx4bueiii4iIiCAiIoJGjRqxZcsWEhISfrNM586dD8eSkpJYt24d0dHRtGzZ8vD9D/3792f06NHl9r2JSPVQ44rE0fzH75xj7bZ97DuYy/FxtakVXj67qXbt2oen/+///o/u3bvz3nvvsW7dOrp161bgOhEREYenfT4fOTk5JVpGRORoaEyiAGZG8wZRhIYYP+/IJCc3r9xfMyMjg/j4eADeeOONMt9+69atWbNmDevWrQNg8uTJZf4aIlL9qEgUItQXQvMGUWTnOtJ27j88+FteHnjgAR566CE6dOhQLv/516pVixEjRtCzZ086depEnTp1qFevXpm/johUL1beb34VLTk52eX/0KEVK1Zw0kknHXnF7CzI3gdRsb8Jb9t7gF927eeYupE0qhtZ1ulWqL179xIdHY1zjsGDB9OqVSvuueee3y13VPtLRKoVM5vvnPvd9fY6kjhk7xbYtR6yfnvlU2ztcGJqhbNldxZ7s8r+RruK9Oqrr5KUlETbtm3JyMjglltuCXZKIlLJFVkkzOx1M9tqZksDYs+Z2Y9mttjM3jOzmIB5D5lZqpmtNLMeAfGeXizVzIYGxBPN7DsvPtnMwr14hPc81Zvfoqy+6QLVS4DQSNi5DnJ+bc9hZsTXr0V4qI/1O/aTnVP+4xPl5Z577mHRokUsX76cCRMmEBUVFeyURKSSO5ojiTeAnvlis4B2zrn2wE/AQwBm1gboB7T11hlhZj4z8wGvAL2ANkB/b1mAZ4FhzrnjgZ3ADV78BmCnFx/mLVd+QnzQoKV/esdayMs9PMsXYhwbG0Wec6zfkUleNTtFJyJSmCKLhHPuS2BHvtgnzrlDo6tzgUMX6vcGJjnnDjjn1gKpQGfvkeqcW+OcOwhMAnqbv+XoOcBUb/2xQJ+AbY31pqcC51p5tygNjYAGif4jiV0/Q0AxiAzzkVC/FvsO5rA5Q40ARaRmKIsxieuBD73peGBDwLw0L1ZYPBbYFVBwDsV/sy1vfoa3fPmKqAN14yErwz9OESAmKpzY6Ai27T1ARuaRG/OJiFQHpSoSZvYIkANMKJt0SpzHzWaWYmYp6enppd9g7Tio1QD2bIL9Gb+Z1aReJFHhoaTt3M+B7NxCNiAiUj2UuEiY2Z+Ai4EB7tfraDcCzQIWS/BihcW3AzFmFpov/pttefPrecv/jnNutHMu2TmXHBcXV9Jv6VdmUK8ZhNWCXev8l8d6Qrwb7awYjQBL0yoc/E37Aru8jho1inHjxpXoWxMRKY4S9Zsws57AA8DZzrnMgFnTgbfM7AWgKdAKmAcY0MrMEvG/+fcDrnHOOTP7HLgS/zjFQGBawLYGAt968z9zFXlTR0gI1G8J21bCjjUQdwKE+HdXeGgIzRpEsXbbPjbu2k9C/VpH/ES3olqFF+WLL74gOjr68GdGDBo0qOTfl4hIMRzNJbAT8b9RtzazNDO7AXgZqAPMMrNFZjYKwDm3DJgCLAc+AgY753K9MYXbgY+BFcAUb1mAB4EhZpaKf8xhjBcfA8R68SHA4ctmK0xoONRPhNyDsPO3A9l1IsNoXDeSnZkH2bGv+OMT8+fP5+yzz6ZTp0706NGDTZs2ATB8+HDatGlD+/bt6devH+vWrWPUqFEMGzaMpKQk5syZw+OPP84///lPALp168aDDz5I586dOeGEE5gzZw4AmZmZ9O3blzZt2nDZZZfRpUsX8t9kKCJSlCKPJJxzBfWUHlNA7NDyTwFPFRCfCcwsIL4G/9VP+eNZwFVF5VdsHw6FzUuKt07eQcg5AL5w8P3aPK8RjrrZeexrcBKZlz5H1FE2AnTOcccddzBt2jTi4uKYPHkyjzzyCK+//jrPPPMMa9euJSIigl27dhETE8OgQYN+c/Qxe/bs32wvJyeHefPmMXPmTJ544gk+/fRTRowYQf369Vm+fDlLly4lKSmpeN+ziAg1sAtsiYSEQUie/4jCQvzPAcOIDAthP8b67Zkc3yiaUF/RwzwHDhxg6dKlnH/++QDk5ubSpEkTANq3b8+AAQPo06cPffr0Oar0Lr/8cgA6dep0uIHfV199xV133QVAu3btaN++fXG+YxERoCYWiV7PlGw9lwfbUiFnPzQ8wT+ojX+wJfJgDtnp+9iwcz8tYqOOOD4B/iOJtm3b8u233/5u3owZM/jyyy/54IMPeOqpp1iypOijnkOtwdUWXETKmno3HS0L8d9oZz7/QHbur2/GUeGhNK0XyZ6sbNL3HChyUxEREaSnpx8uEtnZ2Sxbtoy8vDw2bNhA9+7defbZZ8nIyGDv3r3UqVOHPXv2FCvd008/nSlTpgCwfPnyoyo2IiL5qUgUhy/MXyhys/09ngIGshvUDicm6ugaAYaEhDB16lQefPBBTjnlFJKSkvjmm2/Izc3l2muv5eSTT6ZDhw7ceeedxMTEcMkll/Dee+8dHrg+Grfddhvp6em0adOGRx99lLZt26o1uIgUm1qFl0Tmdn/H2NqNoF784XBunmP11r3k5DlaNYom7Cg/I7s85Obmkp2dTWRkJKtXr+a8885j5cqVhIeHF7muWoWL1DyFtQqveWMSZSEqFrIzYd9W/9hEVAPA3wiweWwUqVv38vOOTFrG1SaknNtNFSYzM5Pu3buTnZ2Nc44RI0YcVYEQEQmkIlFSdeP9d2LvWu9vMR7ub7t9qBHg+h2ZbM7IomlMraCkV6dOHd0XISKlVmPGJMr8tJqFQP0W/nGKHWv84xSemKhwGnqNAHdVsUaA1e30o4iUTo0oEpGRkWzfvr3s3wB9Yf47svNyYeda/2WynmMCGgFmVZFGgM45tm/fTmRk1f6YVhEpOzXidFNCQgJpaWmUSYfYghzMhszNsH4b1Kp/OJyb59i6O4ttaUZcnYigjU8UR2RkJAkJCUUvKCI1Qo0oEmFhYSQmJpbvi3zyKHz8L7hkOHQaeDj81apt/PH17+h9SlOGXZ1U5I12IiKVSY043VQhznsCjjsHZtwLG+YdDp/RqiFDzjuB9xf9wvjv1gcxQRGR4lORKCshPrhijP++icnXwu5Nh2cN7n483VvH8eQHy/lhw67g5SgiUkwqEmUpqgH0mwgH9voLRY6/RUdIiDHs6iTi6kRw24QF7CxBa3ERkWBQkShrjdvAZaNgYwrMGHK4dUdMVDgjr+1I+p4D3DNl0VF9op2ISLCpSJSHNpfCWffDwvHw/WuHw+0TYvjLJW34YmU6L3+eGsQERUSOjopEeen2MJzQEz4aCuu+Ohwe0KU5l3WIZ9inPzFnVTldkisiUkZUJMpLSAhcPtp/s92UgbBrAwBmxlOXtaNVo2junLiQX3btD3KiIiKFU5EoT5H1oP9E/yfaTboGDmYC/s+fGHltJ7JzHbdNWMDBnLwiNiQiEhwqEuWtYSu44jX/52p/cOfhgezj4qL5x5XtWbRhF3+fuSLISYqIFExFoiKc0APOeQSWvA3fvnw4fOHJTbjhjETe+GYd03/4JYgJiogUTEWiopx5H7TpDbP+Aqs/Oxwe2utEko+tz9B3FpO6tXgfUSoiUt6KLBJm9rqZbTWzpQGxBmY2y8xWeV/re3Ezs+Fmlmpmi82sY8A6A73lV5nZwIB4JzNb4q0z3LzmRoW9RpVlBr1HQNxJ8Paf/e3FgTBfCC9f05GocB+Dxi9g34GcIjYkIlJxjuZI4g2gZ77YUGC2c64VMNt7DtALaOU9bgZGgv8NH3gM6AJ0Bh4LeNMfCdwUsF7PIl6j6oqIhn4T/NOTBvjvzMbfVnx4vw6sSd/LQ+8u0Wc6iEilUWSRcM59CezIF+4NjPWmxwJ9AuLjnN9cIMbMmgA9gFnOuR3OuZ3ALKCnN6+uc26u878zjsu3rYJeo2prkAhX/QfSf4T3bz08kH3a8Q2594LWTP/hF96c+3OQkxQR8SvpmERj59yhDnabgcbedDywIWC5NC92pHhaAfEjvcbvmNnNZpZiZinl9pkRZem4c+D8v8KK6TDnn4fDt559HOee2Ign/7uchet3BjFBERG/Ug9ce0cA5Xp+pKjXcM6Nds4lO+eS4+LiyjOVsnPq7XByX/jsKVj5EeBvBPhC3yQa141k8IQF7FAjQBEJspIWiS3eqSK8r1u9+EagWcByCV7sSPGEAuJHeo3qwQwuHQ5N2sO7N0H6TwDUiwpj5IBObNt7kLsmLSRXjQBFJIhKWiSmA4euUBoITAuIX+dd5dQVyPBOGX0MXGBm9b0B6wuAj715u82sq3dV03X5tlXQa1QfYbXg6gngC/ffkZ2VAcDJCfV4/NK2zFm1jeGzVwU5SRGpyY7mEtiJwLdAazNLM7MbgGeA881sFXCe9xxgJrAGSAVeBW4DcM7tAJ4Evvcef/VieMu85q2zGvjQixf2GtVLTDPoOw52roV3b4Y8f4uO/p2bcXnHeIZ/toovVlavgygRqTqsul1umZyc7FJSUoKdRvHNexVm3udvMX7OowDsP5jLZSO+ZvPuLGbceSbxMbWCnKSIVFdmNt85l5w/rjuuK4s/3Agd/ghfPgfL/WfWaoX7GHltJ3K9RoAHcnKDnKSI1DQqEpWFGVz0PCT8Ad67FbYsAyCxYW2eu6o9P2zYxVMz1AhQRCqWikRlEhoBfd+EiDr+gexM/7BNz3ZNuOnMRMZ9+zPTFm0sYiMiImVHRaKyqdsErh4Pu3+BqddDrr+X0wM9T+QPLeoz9J0lrNqiRoAiUjFUJCqjZn/wn3pa8znMfhz4tRFg7YhQBo2fz141AhSRCqAiUVl1vA7+cBN88y9Y/DYAjetG8q/+HVi7bR9D31msRoAiUu5UJCqznk/DsafD9Nvhl0UAnHpcLPf1aM1/F2/ijW/WBTU9Ean+VCQqM18YXDUWohr6W4vv9TcvHHTWcZx3UiOemrGC+T+rEaCIlB8VicouOg76jYfMbfD2QMjNJiTEeP6qJJrERHL7WwvYvvdAsLMUkWpKRaIqaNoBLv0X/Pw1fPww8GsjwO37DnLXpEVqBCgi5UJFoqpo39ffXnzeaFjwJgDt4uvxZO+2fJW6jZc+/SnICYpIdaQiUZWc9wS07AYzhsCG7wG4+g/NuapTAsM/S+VzNQIUkTKmIlGV+ELhyv9AnSYw+VrYsxmAJ/u046Qmdbln8iLSdmYGOUkRqU5UJKqaqAbQfyIc2O0vFDkHiAzzMXJARzUCFJEypyJRFTVuC31GQtr3MONecI4WDWvzz76nsDgtgyf/uzzYGYpINaEiUVW17QNn3gcL34SUMQD0aHsMt5zVkvFz1/PewrTg5ici1YKKRFXW/WFo1QM+fBDWfQ3A/T1a0zmxAQ+9u4SVm9UIUERKR0WiKgvxwRWvQv0WMOU6yEgj1BfCy/07EB0Rxq3j57MnKzvYWYpIFaYiUdVF1oN+EyHngP8zKLL306huJC9f04Gfd2TyoBoBikgpqEhUB3En+I8oNi2GD+4C5+jaMpb7e7Rm5pLNvP71umBnKCJVlIpEddG6F3R/BBZPhrkjALjlrJZc0KYxT89cQcq6HUFOUESqIhWJ6uTMe+GkS+CTR2H155gZz111CvH1azH4rQVsUyNAESmmUhUJM7vHzJaZ2VIzm2hmkWaWaGbfmVmqmU02s3Bv2Qjveao3v0XAdh7y4ivNrEdAvKcXSzWzoaXJtUYICfHfP9GwNUz9M+xYS71a/kaAuzKzuWvSQjUCFJFiKXGRMLN44E4g2TnXDvAB/YBngWHOueOBncAN3io3ADu9+DBvOcysjbdeW6AnMMLMfGbmA14BegFtgP7esnIkEXWg/1vg8vyfQXFwH22a1uXJPu34OnU7w2apEaCIHL3Snm4KBWqZWSgQBWwCzgGmevPHAn286d7ec7z555qZefFJzrkDzrm1QCrQ2XukOufWOOcOApO8ZaUoDVr6ezylr4D3bwPn6JvcjKuTm/Hy56l89uOWYGcoIlVEiYuEc24j8E9gPf7ikAHMB3Y553K8xdKAeG86HtjgrZvjLR8bGM+3TmHx3zGzm80sxcxS0tPTS/otVS/Hn+vvGrv8ffjqBQCe6N2WNk3qcs/kH9iwQ40ARaRopTndVB//f/aJQFOgNv7TRRXOOTfaOZfsnEuOi4sLRgqV02l3QLsrYfaT8NPHRIb5GHVtJ/Kc49YJ88nKViNAETmy0pxuOg9Y65xLd85lA+8CpwMx3ukngARgoze9EWgG4M2vB2wPjOdbp7C4HC0z/yfaHXMyvHMjbFtF89goXuibxNKNu3niAzUCFJEjK02RWA90NbMob2zhXGA58DlwpbfMQGCaNz3de443/zPnvxV4OtDPu/opEWgFzAO+B1p5V0uF4x/cnl6KfGum8CjoNwF8Yf47srN2c36bxgw6+zgmzlvPO/PVCFBECleaMYnv8A9ALwCWeNsaDTwIDDGzVPxjDmO8VcYAsV58CDDU284yYAr+AvMRMNg5l+uNW9wOfAysAKZ4y0pxxTSHq8bC9tXw7s2Ql8d9F5xA15YNeOT9Jfy4eXewMxSRSsqqW1+f5ORkl5KSEuw0KqfvRsOH98PZD0L3h9m6J4uLh39F7YhQpt1+OnUjw4KdoYgEiZnNd84l54/rjuuapPNNkHQt/O9ZWD6dRnUiefmajqzfkckDb6sRoIj8nopETWIGFz0P8Z3gvUGwZTmdExswtOeJfLRsM2O+WhvsDEWkklGRqGnCIuHq8RAR7R/I3r+TG89MpGfbY3j6wx/5Xo0ARSSAikRNVLcp9H0TMtJg6vWYy+MfV7WnWf1aDJ6wgPQ9agQoIn4qEjVV8y7+U0+rP4PZT1A3MoyR13Zid1Y2d05cSE5uXrAzFJFKQEWiJus0EJJvgK9fgiVTOalJXf7W52S+XbOd59UIUERQkZCez0DzU2Ha7bDpB67slED/zs0Y+cVqZi1XI0CRmk5FoqYLDYe+4yCqgb+1+L5tPHZJW9rF12XIlEWs365GgCI1mYqEQHQj/xVPe7fC238iMiSPkQM6YaBGgCI1nIqE+MV3hEuHw7o58MmjNGsQxbCrk1j2y24en65uKCI1lYqE/OqUftB1MHw3ChaO59yTGjO4+3FM+n4Db6dsKHp9Eal2VCTkt87/KySeDf+9B9JSGHJ+a047LpZH31/K8l/UCFCkplGRkN/yhcJVb0CdY2Dytfj2bWV4/w7ERIVx24T57M7KDnaGIlKBVCTk96IaQL+3ICsDpvyRhpHwyjUdSdu5n/um/KBGgCI1iIqEFOyYk6H3K7DhO/jwAZJbNGBorxP5ZPkWXp2zJtjZiUgFCS16Eamx2l0Om5fAVy/AMe254YzrWbB+J89+tJJTEmLo0jI22BmKSDnTkYQc2TmPwvHnw4cPYOu/5dkr2nNsgyhun7iQrbuzgp2diJQzFQk5shAfXPEaxBwLU66jzoGtjLi2I3uysrldjQBFqj0VCSlarRjoPxGys2DyAE6MDePvl53MvLU7eO6TlcHOTkTKkYqEHJ241nD5v+GXhfDfe7i8QzzXdGnOv/+3hk+WbQ52diJSTlQk5OideBF0exh+mAhzR/KXi9twcnw97n37B37evi/Y2YlIOVCRkOI563448WL45FEiN8xhxICOhJgxaPwCNQIUqYZKVSTMLMbMpprZj2a2wsxONbMGZjbLzFZ5X+t7y5qZDTezVDNbbGYdA7Yz0Ft+lZkNDIh3MrMl3jrDzcxKk6+UgZAQuGwUNGwFb/+ZZraVF69OYsWm3fxl2tJgZyciZay0RxIvAR85504ETgFWAEOB2c65VsBs7zlAL6CV97gZGAlgZg2Ax4AuQGfgsUOFxVvmpoD1epYyXykLEXX8d2S7XJg0gO4ta3PHOcczJSWNKd+rEaBIdVLiImFm9YCzgDEAzrmDzrldQG9grLfYWKCPN90bGOf85gIxZtYE6AHMcs7tcM7tBGYBPb15dZ1zc52/D8S4gG1JsMUeB1e8DluXw7TB3H1uK844viH/N20pSzdmBDs7ESkjpTmSSATSgf+Y2UIze83MagONnXObvGU2A4296Xgg8N/MNC92pHhaAfHfMbObzSzFzFLS09NL8S1JsbQ6D859DJa9h++bF3mpXxL1o8K5bcICMvarEaBIdVCaIhEKdARGOuc6APv49dQSAN4RQLl3g3POjXbOJTvnkuPi4sr75STQ6XdBuytg9l+J3fQlrwzoyC+79nPvlB/Iy1MjQJGqrjRFIg1Ic8595z2fir9obPFOFeF93erN3wg0C1g/wYsdKZ5QQFwqEzO49GU4ph1MvYFO0Tt4+MKT+HTFFv79pRoBilR1JS4SzrnNwAYza+2FzgWWA9OBQ1coDQSmedPTgeu8q5y6AhneaamPgQvMrL43YH0B8LE3b7eZdfWuarouYFtSmYRHwdUT/C08Jvbnz8kNuKh9E577+Ee+Xb092NmJSCmU9uqmO4AJZrYYSAL+DjwDnG9mq4DzvOcAM4E1QCrwKnAbgHNuB/Ak8L33+KsXw1vmNW+d1cCHpcxXykv9Y6HvWNieir13K89e3o4WDWtzhxoBilRpVt0+QCY5OdmlpKQEO42aa+4o+OhB6PYQP500mN4vf83J8fWYcFMXwny6d1OksjKz+c655Pxx/dVK2epyC5xyDXzxNCfs+B9PX34y89bt4LmP1QhQpCpSkZCyZQYXD4OmHeG9W+gTv4c/dj2W0V+u4aOlagQoUtWoSEjZC4uEq8dDWBRM6s+j5zbhlGYx3P/2D6zdpkaAIlWJioSUj3rxcPWbsGsDEdNu5pV+7fH5jFvHz2f/QTUCFKkqVCSk/DTvChc+B6mfkrDweV68OomVW/bw6PtLqW4XTIhUVyoSUr6S/wyd/gxfDaNb9hzuOKcV7yxIY5IaAYpUCSoSUv56/QOadYX3B3NX2yzObNWQx6YvUyNAkSpARULKX2g49B0HterjmzyA4Zc2J7Z2OIPGzycjU40ARSozFQmpGHUaQ7/xsHcL9WfcxCv927NldxZDpixSI0CRSkxFQipOfCe45CVYN4eOP77AIxeexOwftzLyf6uDnZmIFEJFQipWUn/ocivMHcHA2t9yySlNef6TlXyzeluwMxORAqhISMW74G+QeBb2wd38o2s2LeOiuXPiQjZnqBGgSGWjIiEVzxcKV74B0Y2p9e5AXr08gcyDudz+1gKyc/OCnZ2IBFCRkOCoHQv9JsD+nSTOvo1nLzuRlJ938uyHPwY7MxEJoCIhwdOkPfR5BTbM5ZKNLzHw1GN57au1zFyyqeh1RaRCqEhIcLW7Ak6/G1Je5/+azCOpWQwPTF3MmvS9wc5MRFCRkMrg3L/A8ecR+tEDvNo9hzCfcev4BWQezAl2ZiI1noqEBF+ID654DWKaETfjRkZeegw/bd3Do++pEaBIsKlISOVQqz70ewuyM+k67y6GdG/Ouws38ta89cHOTKRGU5GQyqPRSXDZv+GXBdy+7xXOatWQJ6YvZ3HarmBnJlJjqUhI5XLSxXD2UOyHiYxs9T0No8O5dfwCdmUeDHZmIjVSqYuEmfnMbKGZ/dd7nmhm35lZqplNNrNwLx7hPU/15rcI2MZDXnylmfUIiPf0YqlmNrS0uUoVcfaD0Poian/+F8adc4Cte7K4Z7IaAYoEQ1kcSdwFrAh4/iwwzDl3PLATuMGL3wDs9OLDvOUwszZAP6At0BMY4RUeH/AK0AtoA/T3lpXqLiQELhsFscdz/BeDefbceny+Mp0RX6QGOzORGqdURcLMEoCLgNe85wacA0z1FhkL9PGme3vP8eaf6y3fG5jknDvgnFsLpAKdvUeqc26Nc+4gMMlbVmqCyLr+gey8XC5b+QBXtm/AC7N+4qtVagQoUpFKeyTxIvAAcKjhTiywyzl36AL3NCDem44HNgB48zO85Q/H861TWPx3zOxmM0sxs5T09PRSfktSaTQ8Hq4cg21ZxjOhozmuYW3unLSQTRn7g52ZSI1R4iJhZhcDW51z88swnxJxzo12ziU755Lj4uKCnY6UpVbnw7l/IXT5u0xsN48D2bkMnrCAgzlqBChSEUpzJHE6cKmZrcN/Kugc4CUgxsxCvWUSgI3e9EagGYA3vx6wPTCeb53C4lLTnHEPtL2Mht/+ndfPyGDB+l08/eGKotcTkVIrcZFwzj3knEtwzrXAP/D8mXNuAPA5cKW32EBgmjc93XuON/8z57+ddjrQz7v6KRFoBcwDvgdaeVdLhXuvMb2k+UoVZga9X4HGbemy4H6GdAzlP1+v47+Lfwl2ZiLVXnncJ/EgMMTMUvGPOYzx4mOAWC8+BBgK4JxbBkwBlgMfAYOdc7neuMXtwMf4r56a4i0rNVF4bX9rcfNx+9a/cFpCOA9OXUzqVjUCFClPVt164yQnJ7uUlJRgpyHlZc0X8OblZLW8gNPX/pnYOpG8P/h0osJDi1xVRApnZvOdc8n547rjWqqWlt3ggr8RufpD3m33Nau27uXhd5eoEaBIOVGRkKqn663Qvh/HLn6J4R028f6iXxj/nRoBipQHFQmpeszgkhehaQcuTn2c/i0zefKD5fywYVewMxOpdlQkpGoKqwVXj8fCIvnb/r+TGJ3DbRMWsHOfGgGKlCUVCam66iVA3zfxZazn7bjX2L5nP3erEaBImVKRkKrt2FOh1z+om/YFU1vP5n8/pfOvz9QIUKSsqEhI1Zd8PXQcSLs1Y3jiuJW8OPsnvvxJPbxEyoKKhFR9ZnDhc9CsC9dtfY4LYtO5a9JCftmlRoAipaUiIdVDaAT0HYdF1uPlkOeJyt3NbWoEKFJqKhJSfdQ5Bq6eQNi+zUxr9BpLNmzn7zPVCFCkNFQkpHpJ6AQXD6Nh+rdMaD6DN75Zx/Qf1AhQpKTU8Eaqnw7XwqbFdJ33b4Y0SmDoOz7aNKnD8Y3qBDszkSpHRxJSPfV4ClqcyR37/kXH0LUMGr+AfQdyil5PRH5DRUKqJ18YXPUGFt2YMZEvsjt9Iw+pEaBIsalISPVVuyH0G0/EwQzej/s3H/6wnnHf/hzsrESqFBUJqd6anAK9X6bp7kWMjnubv81YzoL1O4OdlUiVoSIh1d/JV8Jpd9J9zwfcGPUlt09YwA41AhQ5KioSUjOc9zgcdw73575Gs31LuGvSQnLVCFCkSCoSUjOE+ODK1wmpl8Abtf/FT6t+YvjsVcHOSqTSU5GQmqNWfeg/kci8/UyJeYVRny3ji5Vbg52VSKWmIiE1S6OTsMtGcWzWCobXGc/dkxayUY0ARQqlIiE1T5tL4awH6HHwU67K+4jbJizgQE5usLMSqZRKXCTMrJmZfW5my81smZnd5cUbmNksM1vlfa3vxc3MhptZqpktNrOOAdsa6C2/yswGBsQ7mdkSb53hZmal+WZFDuv2EJzQi4dCxlJr4zc8NUONAEUKUpojiRzgXudcG6ArMNjM2gBDgdnOuVbAbO85QC+glfe4GRgJ/qICPAZ0AToDjx0qLN4yNwWs17MU+Yr8KiQELh9NSOxxjIl6mdnfpjBt0cZgZyVS6ZS4SDjnNjnnFnjTe4AVQDzQGxjrLTYW6ONN9wbGOb+5QIyZNQF6ALOcczucczuBWUBPb15d59xc5++lMC5gWyKlF1kX+r1FlC+XN6OH8/g7Kfy0ZU+wsxKpVMpkTMLMWgAdgO+Axs65Td6szUBjbzoe2BCwWpoXO1I8rYB4Qa9/s5mlmFlKero+tlKKoWEr7IoxJOas4enQ0Qx6M4W9agQoclipi4SZRQPvAHc753YHzvOOAMr9jiXn3GjnXLJzLjkuLq68X06qmxN6YOc8Sk/3FeftmsKD7yxWI0ART6mKhJmF4S8QE5xz73rhLd6pIryvhy5E3wg0C1g9wYsdKZ5QQFyk7J15L7TpzdDQSexe+jFvfLMu2BmJVAqlubrJgDHACufcCwGzpgOHrlAaCEwLiF/nXeXUFcjwTkt9DFxgZvW9AesLgI+9ebvNrKv3WtcFbEukbJlB7xFYoxMZFfkKb874gvk/qxGgSGmOJE4H/gicY2aLvMeFwDPA+Wa2CjjPew4wE1gDpAKvArcBOOd2AE8C33uPv3oxvGVe89ZZDXxYinxFjiwiGuv3FrXCQ3kt4nnun/A12/ceCHZWIkFl1e3ca3JysktJSQl2GlKVrf4cN/5yPslNZnyzv/LGDV3xhegWHanezGy+cy45f1x3XIvkd1x37Pwn6REyj1PWjeGlT38KdkYiQaMiIVKQUwfj2vdlSNhUln0xmc9/VCNAqZlUJEQKYoZdMhyOac+/wkfw4qQZbNiRGeysRCqcioRIYcJqEdJvAhG1onjRPcv94+eoEaDUOCoSIkcS0wzf1W9ybEg6N6U/zZPTlwQ7I5EKpSIhUpQWpxPS6xnO9S2k8YJhvLcwreh1RKoJFQmRo/GHG8nrcB13hL7PF+++ysrNagQoNYOKhMjRMCPkon+S3aQTT/tG8Y+x77AnKzvYWYmUOxUJkaMVGkFY/wmE1qrLY5lP8cSUr9QIUKo9FQmR4qjbhPBr3iI+ZCeXrvo//jMnNdgZiZQrFQmR4mr2B0IueYGzfEvIm/UYKet2FL2OSBWlIiFSAtbxOg50vJEbfTOY/uaLbFMjQKmmVCRESijiomfY16QLD+eMYNjYKeTmaXxCqh8VCZGS8oVRe8AEcmvFctvWxxg189tgZyRS5lQkREojOo7a102mUcgekufdw2fLdKOdVC8qEiKl1TQJd+nLdAn5kS1vD1EjQKlWVCREykB4h6vZ3eFW+vMx7455mqxsNQKU6kFFQqSM1L34b2xvfDqD9o7g9UlTgp2OSJlQkRApK75QYgeOJzOyMVekDmXG1wuCnZFIqalIiJSlqAbUGTiZeiFZxH9yMys3pgc7I5FSUZEQKWOhTU/mwMWvkGSrSP3PIHbvPxjslERKTEVCpBzU63QlG9vfzkU5nzJjzN/UCFCqrEpfJMysp5mtNLNUMxsa7HxEjlZ8nyf5OfZMrkx/mf9OnxrsdERKpFIXCTPzAa8AvYA2QH8zaxPcrESOUkgIzW8cz/bwppy2YAhffTiJRV/NZM/eveTl5gU7O5GjEhrsBIrQGUh1zq0BMLNJQG9geVCzEjlKViuG6D9NIfzVMznju1v8wU9hj6tFrvnYTy1yzf9n6DAcgFnZvLZOcdU4ey94gZNO7VWm26zsRSIe2BDwPA3okn8hM7sZuBmgefPmFZOZyFGKjm9DWu9JrPt+BvUSO7F/7VwsO5M8B77sfeBy8ZcFB85/hOEcUCa1omwKjlQNMdExZb7Nyl4kjopzbjQwGiA5OVn/Pkmlk9DhfBI6nO89+2NQcxEpjko9JgFsBJoFPE/wYiIiUgEqe5H4HmhlZolmFg70A6YHOScRkRqjUp9ucs7lmNntwMeAD3jdObcsyGmJiNQYlbpIADjnZgIzg52HiEhNVNlPN4mISBCpSIiISKFUJEREpFAqEiIiUiirbt0pzSwd+LmEqzcEtpVhOmVFeRWP8ioe5VU81TWvY51zcfmD1a5IlIaZpTjnkoOdR37Kq3iUV/Eor+KpaXnpdJOIiBRKRUJERAqlIvFbo4OdQCGUV/Eor+JRXsVTo/LSmISIiBRKRxIiIlIoFQkRESmUioTHzHqa2UozSzWzoRX4us3M7HMzW25my8zsLi/+uJltNLNF3uPCgHUe8vJcaWY9yjG3dWa2xHv9FC/WwMxmmdkq72t9L25mNtzLa7GZdSynnFoH7JNFZrbbzO4O1v4ys9fNbKuZLQ2IFXsfmdlAb/lVZjawnPJ6zsx+9F77PTOL8eItzGx/wL4bFbBOJ+93INXLvVQfdVdIXsX+2ZX132sheU0OyGmdmS3y4hW5vwp7f6i43zHnXI1/4G9DvhpoCYQDPwBtKui1mwAdvek6wE9AG+Bx4L4Clm/j5RcBJHp5+8opt3VAw3yxfwBDvemhwLPe9IXAh/g/L7Mr8F0F/dw2A8cGa38BZwEdgaUl3UdAA2CN97W+N12/HPK6AAj1pp8NyKtF4HL5tjPPy9W83HuVQ17F+tmVx99rQXnlm/888Jcg7K/C3h8q7HdMRxJ+nYFU59wa59xBYBLQuyJe2Dm3yTm3wJveA6zA/9nehekNTHLOHXDOrQVS8edfUXoDY73psUCfgPg45zcXiDGzJuWcy7nAaufcke6wL9f95Zz7EthRwGsWZx/1AGY553Y453YCs4CeZZ2Xc+4T51yO93Qu/k96LJSXW13n3Fznf6cZF/C9lFleR1DYz67M/16PlJd3NNAXmHikbZTT/irs/aHCfsdUJPzigQ0Bz9M48ht1uTCzFkAH4DsvdLt3yPj6ocNJKjZXB3xiZvPN7GYv1tg5t8mb3gw0DkJeh/Tjt3+4wd5fhxR3HwUjx+vx/8d5SKKZLTSz/5nZmV4s3sulIvIqzs+uovfXmcAW59yqgFiF76987w8V9jumIlFJmFk08A5wt3NuNzASOA5IAjbhP9ytaGc45zoCvYDBZnZW4Ezvv6WgXENt/o+zvRR42wtVhv31O8HcR4Uxs0eAHGCCF9oENHfOdQCGAG+ZWd0KTKlS/uwC9Oe3/4xU+P4q4P3hsPL+HVOR8NsINAt4nuDFKoSZheH/BZjgnHsXwDm3xTmX65zLA17l11MkFZarc26j93Ur8J6Xw5ZDp5G8r1srOi9PL2CBc26Ll2PQ91eA4u6jCsvRzP4EXAwM8N5c8E7nbPem5+M/33+Cl0PgKalyyasEP7uK3F+hwOXA5IB8K3R/FfT+QAX+jqlI+H0PtDKzRO8/1H7A9Ip4Ye985xhghXPuhYB44Pn8y4BDV11MB/qZWYSZJQKt8A+WlXVetc2szqFp/IOeS73XP3RlxEBgWkBe13lXV3QFMgIOh8vDb/67C/b+yqe4++hj4AIzq++darnAi5UpM+sJPABc6pzLDIjHmZnPm26Jfx+t8XLbbWZdvd/T6wK+l7LMq7g/u4r8ez0P+NE5d/g0UkXur8LeH6jI37HSjLxXpwf+qwJ+wv9fwSMV+Lpn4D9UXAws8h4XAm8CS7z4dKBJwDqPeHmupJRXTxwhr5b4rxr5AVh2aJ8AscBsYBXwKdDAixvwipfXEiC5HPdZbWA7UC8gFpT9hb9QbQKy8Z/nvaEk+wj/GEGq9/hzOeWViv+89KHfs1Hesld4P+NFwALgkoDtJON/014NvIzXpaGM8yr2z66s/14LysuLvwEMyrdsRe6vwt4fKux3TG05RESkUDrdJCIihVKREBGRQqlIiIhIoVQkRESkUCoSIiJSKBUJEREplIqEiIgU6v8B1LKc1hU5WLMAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig1 = plt.figure(1)\n",
    "x=np.linspace(1,epo,epo)\n",
    "plt.plot(x,train_error, label = 'Training')\n",
    "plt.plot(x,test_error, label ='Testing')\n",
    "plt.legend(loc = 2)\n",
    "plt.title('MAE')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "6707e00e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.093498443585324\n"
     ]
    }
   ],
   "source": [
    "print(np.min(test_error))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62e0ece7",
   "metadata": {},
   "source": [
    "# Y = b0 + b1X1 + b2X2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "75b415e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(243, 1704)\n",
      "(243, 6816)\n"
     ]
    }
   ],
   "source": [
    "x1=np.zeros([243,1704])\n",
    "x2=np.zeros([243,6816])\n",
    "\n",
    "#x1 (0th~1703th column as x)\n",
    "for i in range (0,243):\n",
    "    for j in range (0,71):\n",
    "        a=np.array(data['obs_PMf'][6816*i+96*j:6816*i+96*j+24])\n",
    "        for k in range (0,24):\n",
    "            if a[k]=='\\\\N' :\n",
    "                a[k]=np.nan\n",
    "        for k in range (0,24):\n",
    "            x1[i][j*24+k]=a[k]\n",
    "\n",
    "#x2 (1704th~8519th column as x)\n",
    "for i in range (1,244):\n",
    "    b=np.array(data['cal_PMf'][6816*i:6816*i+6816])\n",
    "    for j in range(0,6816):\n",
    "        if b[j]=='\\\\N' :\n",
    "            b[j]=np.nan\n",
    "    for j in range(0,6816):\n",
    "        x2[i-1][j]=b[j]\n",
    "        \n",
    "print(np.shape(x1))\n",
    "print(np.shape(x2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "b74cf011",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] \"1 EOF ; RMS = 7.34673151\"\n",
      "[1] \"1 EOF ; RMS = 7.31158122\"\n",
      "[1] \"1 EOF ; RMS = 7.31372817\"\n",
      "[1] \"2 EOF ; RMS = 6.59118508\"\n",
      "[1] \"2 EOF ; RMS = 6.59653877\"\n",
      "[1] \"3 EOF ; RMS = 6.13828726\"\n",
      "[1] \"3 EOF ; RMS = 6.13567946\"\n",
      "[1] \"3 EOF ; RMS = 6.13517573\"\n",
      "[1] \"3 EOF ; RMS = 6.13494526\"\n",
      "[1] \"3 EOF ; RMS = 6.13485496\"\n",
      "[1] \"3 EOF ; RMS = 6.13482155\"\n",
      "[1] \"3 EOF ; RMS = 6.13480941\"\n",
      "[1] \"3 EOF ; RMS = 6.13480502\"\n",
      "[1] \"4 EOF ; RMS = 5.80494096\"\n",
      "[1] \"4 EOF ; RMS = 5.80329288\"\n",
      "[1] \"4 EOF ; RMS = 5.80325796\"\n",
      "[1] \"4 EOF ; RMS = 5.80320468\"\n",
      "[1] \"4 EOF ; RMS = 5.80318234\"\n",
      "[1] \"4 EOF ; RMS = 5.80317507\"\n",
      "[1] \"5 EOF ; RMS = 5.57098871\"\n",
      "[1] \"5 EOF ; RMS = 5.57602323\"\n",
      "[1] \"6 EOF ; RMS = 5.44638947\"\n",
      "[1] \"6 EOF ; RMS = 5.45025482\"\n",
      "[1] \"7 EOF ; RMS = 5.36847431\"\n",
      "[1] \"7 EOF ; RMS = 5.37645583\"\n",
      "[1] \"8 EOF ; RMS = 5.31041536\"\n",
      "[1] \"8 EOF ; RMS = 5.31257344\"\n",
      "[1] \"9 EOF ; RMS = 5.20167017\"\n",
      "[1] \"9 EOF ; RMS = 5.20167237\"\n",
      "[1] \"10 EOF ; RMS = 5.13164224\"\n",
      "[1] \"10 EOF ; RMS = 5.13971001\"\n",
      "[1] \"11 EOF ; RMS = 5.10997602\"\n",
      "[1] \"11 EOF ; RMS = 5.13229289\"\n",
      "[1] \"12 EOF ; RMS = 5.10436517\"\n",
      "[1] \"12 EOF ; RMS = 5.10518146\"\n",
      "[1] \"13 EOF ; RMS = 5.04759273\"\n",
      "[1] \"13 EOF ; RMS = 5.05504713\"\n",
      "[1] \"14 EOF ; RMS = 5.02531803\"\n",
      "[1] \"14 EOF ; RMS = 5.04253688\"\n",
      "[1] \"15 EOF ; RMS = 5.02748761\"\n",
      "[1] \"15 EOF ; RMS = 5.05036777\"\n",
      "[1] \"16 EOF ; RMS = 4.98855079\"\n",
      "[1] \"16 EOF ; RMS = 4.9873491\"\n",
      "[1] \"16 EOF ; RMS = 4.9945848\"\n",
      "[1] \"17 EOF ; RMS = 4.98774223\"\n",
      "[1] \"17 EOF ; RMS = 5.00774314\"\n",
      "[1] \"18 EOF ; RMS = 4.99740931\"\n",
      "[1] \"18 EOF ; RMS = 5.01967165\"\n",
      "[1] \"19 EOF ; RMS = 5.01771212\"\n",
      "[1] \"19 EOF ; RMS = 5.04144091\"\n",
      "[1] \"20 EOF ; RMS = 5.03602401\"\n",
      "[1] \"20 EOF ; RMS = 5.05677533\"\n",
      "[1] \"21 EOF ; RMS = 5.06357904\"\n",
      "[1] \"1 EOF ; RMS = 10.26781188\"\n",
      "[1] \"1 EOF ; RMS = 10.24818421\"\n",
      "[1] \"1 EOF ; RMS = 10.24753208\"\n",
      "[1] \"1 EOF ; RMS = 10.24748477\"\n",
      "[1] \"1 EOF ; RMS = 10.24748401\"\n",
      "[1] \"2 EOF ; RMS = 9.4505378\"\n",
      "[1] \"2 EOF ; RMS = 9.45156763\"\n",
      "[1] \"3 EOF ; RMS = 9.1276832\"\n",
      "[1] \"3 EOF ; RMS = 9.12827088\"\n",
      "[1] \"4 EOF ; RMS = 8.93202194\"\n",
      "[1] \"4 EOF ; RMS = 8.94127037\"\n",
      "[1] \"5 EOF ; RMS = 8.82038388\"\n",
      "[1] \"5 EOF ; RMS = 8.84005814\"\n",
      "[1] \"6 EOF ; RMS = 8.68808337\"\n",
      "[1] \"6 EOF ; RMS = 8.6942497\"\n",
      "[1] \"7 EOF ; RMS = 8.59578625\"\n",
      "[1] \"7 EOF ; RMS = 8.60699564\"\n",
      "[1] \"8 EOF ; RMS = 8.48845998\"\n",
      "[1] \"8 EOF ; RMS = 8.49539868\"\n",
      "[1] \"9 EOF ; RMS = 8.44370647\"\n",
      "[1] \"9 EOF ; RMS = 8.46476058\"\n",
      "[1] \"10 EOF ; RMS = 8.45218955\"\n",
      "[1] \"10 EOF ; RMS = 8.47989997\"\n",
      "[1] \"11 EOF ; RMS = 8.45537893\"\n",
      "[1] \"11 EOF ; RMS = 8.48121672\"\n",
      "[1] \"12 EOF ; RMS = 8.44310974\"\n",
      "[1] \"12 EOF ; RMS = 8.46137941\"\n",
      "[1] \"13 EOF ; RMS = 8.44851523\"\n",
      "[1] \"13 EOF ; RMS = 8.48003075\"\n",
      "[1] \"14 EOF ; RMS = 8.48247224\"\n"
     ]
    }
   ],
   "source": [
    "x1Restruct=sinkr.dineof(x1)\n",
    "x2Restruct=sinkr.dineof(x2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "79ec2c8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(243, 1704)\n",
      "(243, 6816)\n"
     ]
    }
   ],
   "source": [
    "x1Restruct_Fun=np.array(x1Restruct[0])\n",
    "x2Restruct_Fun=np.array(x2Restruct[0])\n",
    "print(np.shape(x1Restruct_Fun))\n",
    "print(np.shape(x2Restruct_Fun))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "f514ff33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(239, 6816)\n"
     ]
    }
   ],
   "source": [
    "YRestruct_Fun=np.zeros([239,6816])\n",
    "for i in range (0,239):\n",
    "    for j in range (0,1704):\n",
    "        YRestruct_Fun[i][j]=x1Restruct_Fun[i+1][j]\n",
    "    for j in range (1704,3408):\n",
    "        YRestruct_Fun[i][j]=x1Restruct_Fun[i+2][j-1704]\n",
    "    for j in range (3408,5112):\n",
    "        YRestruct_Fun[i][j]=x1Restruct_Fun[i+3][j-3408]\n",
    "    for j in range (5112,6816):\n",
    "        YRestruct_Fun[i][j]=x1Restruct_Fun[i+4][j-5112]\n",
    "print(np.shape(YRestruct_Fun))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "e2d52b41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(239, 8520)\n"
     ]
    }
   ],
   "source": [
    "XRestruct_Fun=np.zeros([239,8520])\n",
    "for i in range (0,239):\n",
    "    for j in range (0,1704):\n",
    "        XRestruct_Fun[i][j]=x1Restruct_Fun[i][j]\n",
    "    for j in range (1704,8520):\n",
    "        XRestruct_Fun[i][j]=x2Restruct_Fun[i][j-1704]\n",
    "print(np.shape(XRestruct_Fun))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "6135dd7c",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] \"1 EOF ; RMS = 9.381921\"\n",
      "[1] \"1 EOF ; RMS = 9.37341075\"\n",
      "[1] \"1 EOF ; RMS = 9.37276874\"\n",
      "[1] \"1 EOF ; RMS = 9.37265741\"\n",
      "[1] \"1 EOF ; RMS = 9.37264002\"\n",
      "[1] \"1 EOF ; RMS = 9.37263815\"\n",
      "[1] \"2 EOF ; RMS = 8.64084804\"\n",
      "[1] \"2 EOF ; RMS = 8.64152905\"\n",
      "[1] \"3 EOF ; RMS = 8.40491472\"\n",
      "[1] \"3 EOF ; RMS = 8.40853219\"\n",
      "[1] \"4 EOF ; RMS = 8.20012375\"\n",
      "[1] \"4 EOF ; RMS = 8.20526189\"\n",
      "[1] \"5 EOF ; RMS = 7.97867164\"\n",
      "[1] \"5 EOF ; RMS = 7.97752159\"\n",
      "[1] \"5 EOF ; RMS = 7.97836036\"\n",
      "[1] \"6 EOF ; RMS = 7.84713084\"\n",
      "[1] \"6 EOF ; RMS = 7.85248259\"\n",
      "[1] \"7 EOF ; RMS = 7.74196413\"\n",
      "[1] \"7 EOF ; RMS = 7.74615625\"\n",
      "[1] \"8 EOF ; RMS = 7.69004836\"\n",
      "[1] \"8 EOF ; RMS = 7.70033472\"\n",
      "[1] \"9 EOF ; RMS = 7.60995442\"\n",
      "[1] \"9 EOF ; RMS = 7.61537988\"\n",
      "[1] \"10 EOF ; RMS = 7.54608767\"\n",
      "[1] \"10 EOF ; RMS = 7.5591831\"\n",
      "[1] \"11 EOF ; RMS = 7.52163475\"\n",
      "[1] \"11 EOF ; RMS = 7.53280447\"\n",
      "[1] \"12 EOF ; RMS = 7.51762144\"\n",
      "[1] \"12 EOF ; RMS = 7.53850916\"\n",
      "[1] \"13 EOF ; RMS = 7.51406299\"\n",
      "[1] \"13 EOF ; RMS = 7.52827808\"\n",
      "[1] \"14 EOF ; RMS = 7.50425817\"\n",
      "[1] \"14 EOF ; RMS = 7.52404276\"\n",
      "[1] \"15 EOF ; RMS = 7.50189242\"\n",
      "[1] \"15 EOF ; RMS = 7.51795918\"\n",
      "[1] \"16 EOF ; RMS = 7.49753132\"\n",
      "[1] \"16 EOF ; RMS = 7.51333666\"\n",
      "[1] \"17 EOF ; RMS = 7.49715635\"\n",
      "[1] \"17 EOF ; RMS = 7.52402604\"\n",
      "[1] \"18 EOF ; RMS = 7.50449789\"\n",
      "[1] \"18 EOF ; RMS = 7.5283673\"\n",
      "[1] \"19 EOF ; RMS = 7.51917662\"\n",
      "[1] \"19 EOF ; RMS = 7.54405218\"\n",
      "[1] \"20 EOF ; RMS = 7.53393072\"\n",
      "[1] \"20 EOF ; RMS = 7.56328821\"\n",
      "[1] \"21 EOF ; RMS = 7.55227297\"\n",
      "[1] \"21 EOF ; RMS = 7.57573505\"\n",
      "[1] \"22 EOF ; RMS = 7.5530463\"\n",
      "[1] \"22 EOF ; RMS = 7.57010214\"\n",
      "[1] \"23 EOF ; RMS = 7.55687675\"\n",
      "[1] \"23 EOF ; RMS = 7.58025621\"\n",
      "[1] \"24 EOF ; RMS = 7.55851759\"\n",
      "[1] \"24 EOF ; RMS = 7.57809352\"\n",
      "[1] \"25 EOF ; RMS = 7.55884665\"\n",
      "[1] \"25 EOF ; RMS = 7.57965386\"\n",
      "[1] \"26 EOF ; RMS = 7.58375519\"\n",
      "[1] \"1 EOF ; RMS = 7.78686195\"\n",
      "[1] \"1 EOF ; RMS = 7.75050525\"\n",
      "[1] \"1 EOF ; RMS = 7.75018211\"\n",
      "[1] \"1 EOF ; RMS = 7.75023714\"\n",
      "[1] \"2 EOF ; RMS = 7.18597188\"\n",
      "[1] \"2 EOF ; RMS = 7.18733102\"\n",
      "[1] \"3 EOF ; RMS = 6.71374093\"\n",
      "[1] \"3 EOF ; RMS = 6.71259929\"\n",
      "[1] \"3 EOF ; RMS = 6.71257546\"\n",
      "[1] \"3 EOF ; RMS = 6.71257337\"\n",
      "[1] \"4 EOF ; RMS = 6.44687022\"\n",
      "[1] \"4 EOF ; RMS = 6.44929468\"\n",
      "[1] \"5 EOF ; RMS = 6.29510943\"\n",
      "[1] \"5 EOF ; RMS = 6.29742466\"\n",
      "[1] \"6 EOF ; RMS = 6.18385834\"\n",
      "[1] \"6 EOF ; RMS = 6.18715675\"\n",
      "[1] \"7 EOF ; RMS = 6.02557429\"\n",
      "[1] \"7 EOF ; RMS = 6.02185039\"\n",
      "[1] \"7 EOF ; RMS = 6.02127701\"\n",
      "[1] \"7 EOF ; RMS = 6.02112736\"\n",
      "[1] \"7 EOF ; RMS = 6.02107997\"\n",
      "[1] \"7 EOF ; RMS = 6.02106333\"\n",
      "[1] \"7 EOF ; RMS = 6.02105714\"\n",
      "[1] \"8 EOF ; RMS = 5.90170378\"\n",
      "[1] \"8 EOF ; RMS = 5.90194963\"\n",
      "[1] \"9 EOF ; RMS = 5.81611887\"\n",
      "[1] \"9 EOF ; RMS = 5.81791213\"\n",
      "[1] \"10 EOF ; RMS = 5.7305531\"\n",
      "[1] \"10 EOF ; RMS = 5.73065307\"\n",
      "[1] \"11 EOF ; RMS = 5.66200178\"\n",
      "[1] \"11 EOF ; RMS = 5.66583583\"\n",
      "[1] \"12 EOF ; RMS = 5.59558717\"\n",
      "[1] \"12 EOF ; RMS = 5.5993162\"\n",
      "[1] \"13 EOF ; RMS = 5.53675892\"\n",
      "[1] \"13 EOF ; RMS = 5.53761057\"\n",
      "[1] \"14 EOF ; RMS = 5.48769717\"\n",
      "[1] \"14 EOF ; RMS = 5.49172594\"\n",
      "[1] \"15 EOF ; RMS = 5.44417132\"\n",
      "[1] \"15 EOF ; RMS = 5.44505812\"\n",
      "[1] \"16 EOF ; RMS = 5.40355516\"\n",
      "[1] \"16 EOF ; RMS = 5.40855839\"\n",
      "[1] \"17 EOF ; RMS = 5.35695225\"\n",
      "[1] \"17 EOF ; RMS = 5.35695874\"\n",
      "[1] \"18 EOF ; RMS = 5.32961354\"\n",
      "[1] \"18 EOF ; RMS = 5.33351694\"\n",
      "[1] \"19 EOF ; RMS = 5.30514206\"\n",
      "[1] \"19 EOF ; RMS = 5.30873524\"\n",
      "[1] \"20 EOF ; RMS = 5.29718295\"\n",
      "[1] \"20 EOF ; RMS = 5.30443236\"\n",
      "[1] \"21 EOF ; RMS = 5.28926517\"\n",
      "[1] \"21 EOF ; RMS = 5.29922904\"\n",
      "[1] \"22 EOF ; RMS = 5.2756371\"\n",
      "[1] \"22 EOF ; RMS = 5.28367551\"\n",
      "[1] \"23 EOF ; RMS = 5.25689196\"\n",
      "[1] \"23 EOF ; RMS = 5.26130667\"\n",
      "[1] \"24 EOF ; RMS = 5.23779114\"\n",
      "[1] \"24 EOF ; RMS = 5.24235609\"\n",
      "[1] \"25 EOF ; RMS = 5.23007204\"\n",
      "[1] \"25 EOF ; RMS = 5.23803094\"\n",
      "[1] \"26 EOF ; RMS = 5.21487331\"\n",
      "[1] \"26 EOF ; RMS = 5.21779953\"\n",
      "[1] \"27 EOF ; RMS = 5.18892946\"\n",
      "[1] \"27 EOF ; RMS = 5.19231302\"\n",
      "[1] \"28 EOF ; RMS = 5.17897844\"\n",
      "[1] \"28 EOF ; RMS = 5.18648823\"\n",
      "[1] \"29 EOF ; RMS = 5.16839542\"\n",
      "[1] \"29 EOF ; RMS = 5.17393825\"\n",
      "[1] \"30 EOF ; RMS = 5.16242432\"\n",
      "[1] \"30 EOF ; RMS = 5.17222113\"\n",
      "[1] \"31 EOF ; RMS = 5.17051765\"\n",
      "[1] \"31 EOF ; RMS = 5.18256155\"\n",
      "[1] \"32 EOF ; RMS = 5.17524907\"\n",
      "[1] \"32 EOF ; RMS = 5.18321834\"\n",
      "[1] \"33 EOF ; RMS = 5.17396584\"\n",
      "[1] \"33 EOF ; RMS = 5.18176186\"\n",
      "[1] \"34 EOF ; RMS = 5.16741654\"\n",
      "[1] \"34 EOF ; RMS = 5.17416213\"\n",
      "[1] \"35 EOF ; RMS = 5.17351875\"\n",
      "[1] \"35 EOF ; RMS = 5.18458057\"\n",
      "[1] \"36 EOF ; RMS = 5.17737923\"\n",
      "[1] \"36 EOF ; RMS = 5.18616092\"\n",
      "[1] \"37 EOF ; RMS = 5.18866528\"\n"
     ]
    }
   ],
   "source": [
    "Xhat=XRestruct_Fun\n",
    "Yhat=YRestruct_Fun\n",
    "X1hat_train = np.zeros([239,1704])\n",
    "X2hat_train = np.zeros([239,6816])\n",
    "Yhat_train = np.zeros([239,6816])\n",
    "for i in range (0,239):\n",
    "    for j in range (0,1704):\n",
    "        X1hat_train[i][j] = Xhat[i][j]\n",
    "    for j in range (0,6816):\n",
    "        X2hat_train[i][j] = Xhat[i][j+1704]\n",
    "    for j in range (0,6816):    \n",
    "        Yhat_train[i][j] = Yhat[i][j]\n",
    "\n",
    "X1hat_test = np.zeros([30,1704])\n",
    "X2hat_test = np.zeros([30,6816])\n",
    "Yhat_test = np.zeros([30,6816])\n",
    "x=np.zeros([273,8520])\n",
    "y=np.zeros([273,6816])\n",
    "#x1 (0th~1703th column as x)\n",
    "for i in range (0,273):\n",
    "    for j in range (0,71):\n",
    "        a=np.array(data['obs_PMf'][6816*i+96*j:6816*i+96*j+24])\n",
    "        for k in range (0,24):\n",
    "            if a[k]=='\\\\N' :\n",
    "                a[k]=np.nan\n",
    "        for k in range (0,24):\n",
    "            x[i][j*24+k]=a[k]\n",
    "\n",
    "#x2 (1704th~8519th column as x)\n",
    "for i in range (1,274):\n",
    "    b=np.array(data['cal_PMf'][6816*i:6816*i+6816])\n",
    "    for j in range(0,6816):\n",
    "        if b[j]=='\\\\N' :\n",
    "            b[j]=np.nan\n",
    "    for j in range(0,6816):\n",
    "        x[i-1][j+1704]=b[j]\n",
    "\n",
    "#data_obs\n",
    "for i in range (1,274):\n",
    "    a=np.array(data['obs_PMf'][6816*i:6816*i+6816])\n",
    "    for j in range(0,6816):\n",
    "        if a[j]=='\\\\N' :\n",
    "            a[j]=np.nan\n",
    "    y[i-1]=a\n",
    "xRestruct=sinkr.dineof(x)\n",
    "yRestruct=sinkr.dineof(y)\n",
    "xRestruct_Fun=np.array(xRestruct[0])\n",
    "yRestruct_Fun=np.array(yRestruct[0])\n",
    "    \n",
    "for i in range (243,273):\n",
    "    for j in range (0,1704):\n",
    "        X1hat_test[i-243][j]=xRestruct_Fun[i][j] \n",
    "    for j in range (0,6816):\n",
    "        X2hat_test[i-243][j]=xRestruct_Fun[i][j+1704]\n",
    "    for j in range(0,6816):\n",
    "        Yhat_test[i-243][j]=yRestruct_Fun[i][j]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "bf2017e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X1hat_train\n",
      "(239, 1704)\n",
      "X2hat_train\n",
      "(239, 6816)\n",
      "Yhat_train\n",
      "(239, 6816)\n",
      "X1hat_test\n",
      "(30, 1704)\n",
      "X2hat_test\n",
      "(30, 6816)\n",
      "Yhat_test\n",
      "(30, 6816)\n"
     ]
    }
   ],
   "source": [
    "print('X1hat_train')\n",
    "#print(X1hat_train)\n",
    "print(np.shape(X1hat_train))\n",
    "print('X2hat_train')\n",
    "#print(X2hat_train)\n",
    "print(np.shape(X2hat_train))\n",
    "print('Yhat_train')\n",
    "#print(Yhat_train)\n",
    "print(np.shape(Yhat_train))\n",
    "print('X1hat_test')\n",
    "#print(X1hat_test)\n",
    "print(np.shape(X1hat_test))\n",
    "print('X2hat_test')\n",
    "#print(X2hat_test)\n",
    "print(np.shape(X2hat_test))\n",
    "print('Yhat_test')\n",
    "#print(Yhat_test)\n",
    "print(np.shape(Yhat_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51c4b3d6",
   "metadata": {},
   "source": [
    "## GD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "71d5a697",
   "metadata": {},
   "outputs": [],
   "source": [
    "x1 = X1hat_train\n",
    "x2 = X2hat_train\n",
    "y = Yhat_train\n",
    "xt1 = X1hat_test\n",
    "xt2 = X2hat_test\n",
    "yt = Yhat_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "d3a101bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model (x1,x2,b0,b1,b2):\n",
    "    y = b0 +  torch.matmul(x1,b1) + torch.matmul(x2,b2)\n",
    "    return y\n",
    "\n",
    "# def model (x1,x2,b0,b1,b2):\n",
    "#     # y = b0 +  torch.matmul(x1,b1) + torch.matmul(x2,b2)\n",
    "#     # y = torch.add(b0 +  torch.matmul(x,b1))\n",
    "#     # y = b0 +  torch.mm(x,b1)\n",
    "\n",
    "#     b = torch.mean(b1*x1)*torch.ones(6816)\n",
    "#     y = b0 + b+ b2*x2\n",
    "#     return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "2383fb4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Training loss 128660.9609, Testing loss 122755.7783\n",
      "Epoch 1, Training loss 128532.2891, Testing loss 122632.8885\n",
      "Epoch 2, Training loss 128403.6172, Testing loss 122509.9984\n",
      "Epoch 3, Training loss 128274.9453, Testing loss 122387.0997\n",
      "Epoch 4, Training loss 128146.2578, Testing loss 122264.2033\n",
      "Epoch 5, Training loss 128017.5781, Testing loss 122141.3182\n",
      "Epoch 6, Training loss 127888.9141, Testing loss 122018.4362\n",
      "Epoch 7, Training loss 127760.2578, Testing loss 121895.5299\n",
      "Epoch 8, Training loss 127631.5625, Testing loss 121772.6429\n",
      "Epoch 9, Training loss 127502.8828, Testing loss 121649.7437\n",
      "Epoch 10, Training loss 127374.2109, Testing loss 121526.8552\n",
      "Epoch 20, Training loss 126087.4844, Testing loss 120297.9705\n",
      "Epoch 30, Training loss 124800.7422, Testing loss 119068.9911\n",
      "Epoch 40, Training loss 123514.0000, Testing loss 117840.0836\n",
      "Epoch 50, Training loss 122227.2422, Testing loss 116611.1533\n",
      "Epoch 60, Training loss 120940.5078, Testing loss 115382.2349\n",
      "Epoch 70, Training loss 119653.7578, Testing loss 114153.3170\n",
      "Epoch 80, Training loss 118367.0234, Testing loss 112924.3851\n",
      "Epoch 90, Training loss 117080.2891, Testing loss 111695.4579\n",
      "Epoch 100, Training loss 115793.5391, Testing loss 110466.5312\n",
      "Epoch 110, Training loss 114506.7969, Testing loss 109237.6065\n",
      "Epoch 120, Training loss 113220.0703, Testing loss 108008.6841\n",
      "Epoch 130, Training loss 111933.3125, Testing loss 106779.7583\n",
      "Epoch 140, Training loss 110646.5703, Testing loss 105550.8386\n",
      "Epoch 150, Training loss 109359.8281, Testing loss 104321.9093\n",
      "Epoch 160, Training loss 108073.0859, Testing loss 103092.9912\n",
      "Epoch 170, Training loss 106786.3516, Testing loss 101864.0653\n",
      "Epoch 180, Training loss 105499.6016, Testing loss 100635.1448\n",
      "Epoch 190, Training loss 104212.8516, Testing loss 99406.2138\n",
      "Epoch 200, Training loss 102926.1172, Testing loss 98177.2924\n",
      "Epoch 210, Training loss 101639.3750, Testing loss 96948.3751\n",
      "Epoch 220, Training loss 100352.6328, Testing loss 95719.4530\n",
      "Epoch 230, Training loss 99065.8906, Testing loss 94490.5199\n",
      "Epoch 240, Training loss 97779.1328, Testing loss 93261.6086\n",
      "Epoch 250, Training loss 96492.3906, Testing loss 92032.6785\n",
      "Epoch 260, Training loss 95205.6719, Testing loss 90803.7412\n",
      "Epoch 270, Training loss 93918.9141, Testing loss 89574.8345\n",
      "Epoch 280, Training loss 92632.1797, Testing loss 88345.9082\n",
      "Epoch 290, Training loss 91345.4375, Testing loss 87116.9778\n",
      "Epoch 300, Training loss 90058.6953, Testing loss 85888.0569\n",
      "Epoch 310, Training loss 88771.9531, Testing loss 84659.1290\n",
      "Epoch 320, Training loss 87485.2109, Testing loss 83430.2005\n",
      "Epoch 330, Training loss 86198.4609, Testing loss 82201.2833\n",
      "Epoch 340, Training loss 84911.7188, Testing loss 80972.3589\n",
      "Epoch 350, Training loss 83624.9844, Testing loss 79743.4325\n",
      "Epoch 360, Training loss 82338.2422, Testing loss 78514.5106\n",
      "Epoch 370, Training loss 81051.4922, Testing loss 77285.5816\n",
      "Epoch 380, Training loss 79764.7578, Testing loss 76056.6623\n",
      "Epoch 390, Training loss 78478.0156, Testing loss 74827.7440\n",
      "Epoch 400, Training loss 77191.2734, Testing loss 73598.8166\n",
      "Epoch 410, Training loss 75904.5312, Testing loss 72369.8869\n",
      "Epoch 420, Training loss 74617.7812, Testing loss 71140.9632\n",
      "Epoch 430, Training loss 73331.0547, Testing loss 69912.0399\n",
      "Epoch 440, Training loss 72044.2969, Testing loss 68683.1067\n",
      "Epoch 450, Training loss 70757.5625, Testing loss 67454.1905\n",
      "Epoch 460, Training loss 69470.8203, Testing loss 66225.2698\n",
      "Epoch 470, Training loss 68184.0781, Testing loss 64996.3459\n",
      "Epoch 480, Training loss 66897.3359, Testing loss 63767.4183\n",
      "Epoch 490, Training loss 65610.5938, Testing loss 62538.4934\n",
      "Epoch 500, Training loss 64323.8477, Testing loss 61309.5695\n",
      "Epoch 510, Training loss 63037.1094, Testing loss 60080.6436\n",
      "Epoch 520, Training loss 61750.3672, Testing loss 58851.7087\n",
      "Epoch 530, Training loss 60463.6211, Testing loss 57622.7972\n",
      "Epoch 540, Training loss 59176.8828, Testing loss 56393.8737\n",
      "Epoch 550, Training loss 57890.1367, Testing loss 55164.9525\n",
      "Epoch 560, Training loss 56603.3906, Testing loss 53936.0267\n",
      "Epoch 570, Training loss 55316.6523, Testing loss 52707.1020\n",
      "Epoch 580, Training loss 54029.9102, Testing loss 51478.1756\n",
      "Epoch 590, Training loss 52743.1719, Testing loss 50249.2535\n",
      "Epoch 600, Training loss 51456.4336, Testing loss 49020.3244\n",
      "Epoch 610, Training loss 50169.6875, Testing loss 47791.4139\n",
      "Epoch 620, Training loss 48882.9453, Testing loss 46562.4851\n",
      "Epoch 630, Training loss 47596.1953, Testing loss 45333.5571\n",
      "Epoch 640, Training loss 46309.4648, Testing loss 44104.6434\n",
      "Epoch 650, Training loss 45022.7148, Testing loss 42875.7111\n",
      "Epoch 660, Training loss 43735.9727, Testing loss 41646.7849\n",
      "Epoch 670, Training loss 42449.2266, Testing loss 40417.8624\n",
      "Epoch 680, Training loss 41162.4844, Testing loss 39188.9373\n",
      "Epoch 690, Training loss 39875.7461, Testing loss 37960.0107\n",
      "Epoch 700, Training loss 38589.0000, Testing loss 36731.0888\n",
      "Epoch 710, Training loss 37302.2617, Testing loss 35502.1683\n",
      "Epoch 720, Training loss 36015.5156, Testing loss 34273.2387\n",
      "Epoch 730, Training loss 34728.7773, Testing loss 33044.3183\n",
      "Epoch 740, Training loss 33442.0391, Testing loss 31815.3929\n",
      "Epoch 750, Training loss 32155.2910, Testing loss 30586.4683\n",
      "Epoch 760, Training loss 30868.5469, Testing loss 29357.5352\n",
      "Epoch 770, Training loss 29581.7969, Testing loss 28128.6069\n",
      "Epoch 780, Training loss 28295.0469, Testing loss 26899.6783\n",
      "Epoch 790, Training loss 27008.2988, Testing loss 25670.7474\n",
      "Epoch 800, Training loss 25721.5488, Testing loss 24441.8177\n",
      "Epoch 810, Training loss 24434.8027, Testing loss 23212.8858\n",
      "Epoch 820, Training loss 23148.0527, Testing loss 21983.9578\n",
      "Epoch 830, Training loss 21861.3047, Testing loss 20755.0276\n",
      "Epoch 840, Training loss 20574.5566, Testing loss 19526.0982\n",
      "Epoch 850, Training loss 19287.8086, Testing loss 18297.1684\n",
      "Epoch 860, Training loss 18001.0605, Testing loss 17068.2382\n",
      "Epoch 870, Training loss 16714.3105, Testing loss 15839.3085\n",
      "Epoch 880, Training loss 15427.5645, Testing loss 14610.3787\n",
      "Epoch 890, Training loss 14140.8135, Testing loss 13381.4479\n",
      "Epoch 900, Training loss 12854.0654, Testing loss 12152.5180\n",
      "Epoch 910, Training loss 11567.3184, Testing loss 10923.5874\n",
      "Epoch 920, Training loss 10280.5693, Testing loss 9694.6582\n",
      "Epoch 930, Training loss 8993.8213, Testing loss 8465.7274\n",
      "Epoch 940, Training loss 7707.0718, Testing loss 7236.7973\n",
      "Epoch 950, Training loss 6420.3228, Testing loss 6007.8666\n",
      "Epoch 960, Training loss 5133.5737, Testing loss 4778.9361\n",
      "Epoch 970, Training loss 3846.8250, Testing loss 3550.0056\n",
      "Epoch 980, Training loss 2560.0762, Testing loss 2321.0751\n",
      "Epoch 990, Training loss 1273.3273, Testing loss 1092.1448\n",
      "Epoch 1000, Training loss 13.4409, Testing loss 112.5488\n",
      "Epoch 1010, Training loss 237.5361, Testing loss 182.3003\n",
      "Epoch 1020, Training loss 119.7883, Testing loss 87.1416\n",
      "Epoch 1030, Training loss 26.0692, Testing loss 24.1891\n",
      "Epoch 1040, Training loss 27.7313, Testing loss 12.8934\n",
      "Epoch 1050, Training loss 13.3002, Testing loss 13.4853\n",
      "Epoch 1060, Training loss 10.5881, Testing loss 9.9698\n",
      "Epoch 1070, Training loss 7.5747, Testing loss 8.7518\n",
      "Epoch 1080, Training loss 6.3239, Testing loss 8.1109\n",
      "Epoch 1090, Training loss 5.4680, Testing loss 7.8236\n",
      "Epoch 1100, Training loss 5.1062, Testing loss 7.7963\n",
      "Epoch 1110, Training loss 5.0777, Testing loss 8.0125\n",
      "Epoch 1120, Training loss 5.5866, Testing loss 8.3157\n",
      "Epoch 1130, Training loss 6.2041, Testing loss 8.8535\n",
      "Epoch 1140, Training loss 6.8131, Testing loss 9.1821\n",
      "Epoch 1150, Training loss 7.2181, Testing loss 9.6104\n",
      "Epoch 1160, Training loss 7.6626, Testing loss 9.7358\n",
      "Epoch 1170, Training loss 8.0141, Testing loss 10.1886\n",
      "Epoch 1180, Training loss 8.5208, Testing loss 10.4095\n",
      "Epoch 1190, Training loss 8.7184, Testing loss 10.5029\n",
      "Epoch 1200, Training loss 8.9121, Testing loss 10.7998\n",
      "Epoch 1210, Training loss 9.1464, Testing loss 10.9691\n",
      "Epoch 1220, Training loss 9.4091, Testing loss 10.9913\n",
      "Epoch 1230, Training loss 9.4094, Testing loss 11.2043\n",
      "Epoch 1240, Training loss 9.6797, Testing loss 11.3125\n",
      "Epoch 1250, Training loss 9.7508, Testing loss 11.3856\n",
      "Epoch 1260, Training loss 10.0202, Testing loss 11.5508\n",
      "Epoch 1270, Training loss 10.1651, Testing loss 11.6922\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1280, Training loss 10.1795, Testing loss 11.6645\n",
      "Epoch 1290, Training loss 10.2104, Testing loss 11.9745\n",
      "Epoch 1300, Training loss 10.4840, Testing loss 11.9189\n",
      "Epoch 1310, Training loss 10.5114, Testing loss 12.0834\n",
      "Epoch 1320, Training loss 10.5964, Testing loss 12.0566\n",
      "Epoch 1330, Training loss 10.8564, Testing loss 12.3310\n",
      "Epoch 1340, Training loss 10.8494, Testing loss 12.2237\n",
      "Epoch 1350, Training loss 10.8183, Testing loss 12.4001\n",
      "Epoch 1360, Training loss 11.1322, Testing loss 12.3732\n",
      "Epoch 1370, Training loss 11.2186, Testing loss 12.4941\n",
      "Epoch 1380, Training loss 11.1912, Testing loss 12.5210\n",
      "Epoch 1390, Training loss 11.3994, Testing loss 12.6880\n",
      "Epoch 1400, Training loss 11.4348, Testing loss 12.5605\n",
      "Epoch 1410, Training loss 11.3008, Testing loss 12.6244\n",
      "Epoch 1420, Training loss 11.4946, Testing loss 12.6046\n",
      "Epoch 1430, Training loss 11.4964, Testing loss 12.7479\n",
      "Epoch 1440, Training loss 11.5360, Testing loss 12.7669\n",
      "Epoch 1450, Training loss 11.7044, Testing loss 12.8032\n",
      "Epoch 1460, Training loss 11.5656, Testing loss 12.7519\n",
      "Epoch 1470, Training loss 11.6525, Testing loss 12.8802\n",
      "Epoch 1480, Training loss 11.8433, Testing loss 12.8952\n",
      "Epoch 1490, Training loss 11.8174, Testing loss 12.9736\n",
      "Epoch 1500, Training loss 11.7506, Testing loss 12.9899\n",
      "Epoch 1510, Training loss 11.9110, Testing loss 12.9143\n",
      "Epoch 1520, Training loss 11.9031, Testing loss 12.9546\n",
      "Epoch 1530, Training loss 11.9006, Testing loss 13.0440\n",
      "Epoch 1540, Training loss 12.0127, Testing loss 13.0088\n",
      "Epoch 1550, Training loss 11.9586, Testing loss 13.0478\n",
      "Epoch 1560, Training loss 11.9397, Testing loss 13.1286\n",
      "Epoch 1570, Training loss 12.0528, Testing loss 13.0883\n",
      "Epoch 1580, Training loss 12.0367, Testing loss 12.9857\n",
      "Epoch 1590, Training loss 11.8777, Testing loss 13.1243\n",
      "Epoch 1600, Training loss 12.0549, Testing loss 13.0547\n",
      "Epoch 1610, Training loss 12.0508, Testing loss 13.0969\n",
      "Epoch 1620, Training loss 11.9567, Testing loss 13.0741\n",
      "Epoch 1630, Training loss 12.0978, Testing loss 13.1301\n",
      "Epoch 1640, Training loss 12.1490, Testing loss 13.1054\n",
      "Epoch 1650, Training loss 12.0431, Testing loss 13.1425\n",
      "Epoch 1660, Training loss 12.1374, Testing loss 13.1000\n",
      "Epoch 1670, Training loss 12.1546, Testing loss 13.1454\n",
      "Epoch 1680, Training loss 12.0546, Testing loss 13.1215\n",
      "Epoch 1690, Training loss 12.1398, Testing loss 13.1370\n",
      "Epoch 1700, Training loss 12.1445, Testing loss 13.0715\n",
      "Epoch 1710, Training loss 12.1302, Testing loss 13.2353\n",
      "Epoch 1720, Training loss 12.2363, Testing loss 13.1753\n",
      "Epoch 1730, Training loss 12.1863, Testing loss 13.1275\n",
      "Epoch 1740, Training loss 12.0760, Testing loss 13.1645\n",
      "Epoch 1750, Training loss 12.2609, Testing loss 13.2269\n",
      "Epoch 1760, Training loss 12.1824, Testing loss 13.1161\n",
      "Epoch 1770, Training loss 12.1700, Testing loss 13.2474\n",
      "Epoch 1780, Training loss 12.2663, Testing loss 13.1423\n",
      "Epoch 1790, Training loss 12.1570, Testing loss 13.1770\n",
      "Epoch 1800, Training loss 12.1453, Testing loss 13.2183\n",
      "Epoch 1810, Training loss 12.2371, Testing loss 13.2099\n",
      "Epoch 1820, Training loss 12.1539, Testing loss 13.0899\n",
      "Epoch 1830, Training loss 12.0845, Testing loss 13.2344\n",
      "Epoch 1840, Training loss 12.2753, Testing loss 13.1333\n",
      "Epoch 1850, Training loss 12.1936, Testing loss 13.2076\n",
      "Epoch 1860, Training loss 12.1066, Testing loss 13.1365\n",
      "Epoch 1870, Training loss 12.2390, Testing loss 13.2115\n",
      "Epoch 1880, Training loss 12.2188, Testing loss 13.1517\n",
      "Epoch 1890, Training loss 12.1743, Testing loss 13.2515\n",
      "Epoch 1900, Training loss 12.2207, Testing loss 13.0829\n",
      "Epoch 1910, Training loss 12.1989, Testing loss 13.1982\n",
      "Epoch 1920, Training loss 12.1712, Testing loss 13.1833\n",
      "Epoch 1930, Training loss 12.2753, Testing loss 13.2380\n",
      "Epoch 1940, Training loss 12.2240, Testing loss 13.0749\n",
      "Epoch 1950, Training loss 12.1068, Testing loss 13.2521\n",
      "Epoch 1960, Training loss 12.3092, Testing loss 13.1921\n",
      "Epoch 1970, Training loss 12.2667, Testing loss 13.2082\n",
      "Epoch 1980, Training loss 12.1301, Testing loss 13.1543\n",
      "Epoch 1990, Training loss 12.2688, Testing loss 13.2425\n",
      "Epoch 1991, Training loss 12.2789, Testing loss 13.1995\n",
      "Epoch 1992, Training loss 12.1723, Testing loss 13.2444\n",
      "Epoch 1993, Training loss 12.2328, Testing loss 13.2271\n",
      "Epoch 1994, Training loss 12.2507, Testing loss 13.2031\n",
      "Epoch 1995, Training loss 12.2964, Testing loss 13.1761\n",
      "Epoch 1996, Training loss 12.2509, Testing loss 13.1201\n",
      "Epoch 1997, Training loss 12.1336, Testing loss 13.1523\n",
      "Epoch 1998, Training loss 12.1239, Testing loss 13.3029\n",
      "Epoch 1999, Training loss 12.3410, Testing loss 13.2320\n",
      "Epoch 2000, Training loss 12.2755, Testing loss 13.1689\n"
     ]
    }
   ],
   "source": [
    "features1 = torch.from_numpy(x1)\n",
    "features2 = torch.from_numpy(x2)\n",
    "targets = torch.from_numpy(y)\n",
    "x_test1 = torch.from_numpy(xt1)\n",
    "x_test2 = torch.from_numpy(xt2)\n",
    "y_test = torch.from_numpy(yt)\n",
    "\n",
    "# beta0 = torch.randn(6816 , requires_grad = True)\n",
    "# beta1 = torch.randn([1704,6816] , requires_grad = True)\n",
    "# beta2 = torch.randn([6816,6816] , requires_grad = True)\n",
    "\n",
    "beta0 = torch.ones(6816 , requires_grad = True)\n",
    "beta1 = torch.ones([1704,6816], requires_grad = True)\n",
    "beta2 = torch.ones([6816,6816], requires_grad = True)\n",
    "\n",
    "rate = 1e-3\n",
    "optimizer = optim.Adam([beta0 , beta1 , beta2], lr=rate)\n",
    "\n",
    "epo = 2001\n",
    "loss = nn.L1Loss()\n",
    "train_error = np.zeros(epo)\n",
    "test_error = np.zeros(epo)\n",
    "\n",
    "\n",
    "for epoch in range (epo):\n",
    "    yhats_train = model(features1.float() , features2.float(), beta0 , beta1 , beta2)\n",
    "    train_loss = loss(targets.float() , yhats_train)\n",
    "    train_error[epoch] = train_loss\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    train_loss.backward() \n",
    "    optimizer.step()    \n",
    "\n",
    "    yhats_test = model(x_test1.float(), x_test2.float() , beta0, beta1 , beta2) \n",
    "#     for i in range (25):\n",
    "#         for j in range (6816):\n",
    "#             if y_test[i][j] == 0:\n",
    "#                 yhats_test[i][j] = 0\n",
    "    r = torch.abs(yhats_test - y_test)\n",
    "    test_loss = torch.mean(r)\n",
    "    # test_loss = loss(y_test , yhats_test)\n",
    "    test_error[epoch] = test_loss\n",
    "\n",
    "    if epoch <= 10 or epoch % 10 == 0:\n",
    "        print(f\"Epoch {epoch}, Training loss {train_loss.item():.4f},\"\n",
    "                    f\" Testing loss {test_loss.item():.4f}\")\n",
    "        # print('\\tBeta_0 : ' , beta0)\n",
    "        # print('\\tBeta_1 : ' , beta1)\n",
    "        # print('\\tBeta_2 : ' , beta2)       \n",
    "    else :\n",
    "        if epoch >= epo-10 :\n",
    "            print(f\"Epoch {epoch}, Training loss {train_loss.item():.4f},\"\n",
    "                        f\" Testing loss {test_loss.item():.4f}\")\n",
    "            # print('\\tBeta_0 : ' , beta0)\n",
    "            # print('\\tBeta_1 : ' , beta1)\n",
    "            # print('\\tBeta_2 : ' , beta2)       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "09804b82",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig1 = plt.figure(1)\n",
    "x=np.linspace(1,epo,epo)\n",
    "plt.plot(x,train_error, label = 'Training')\n",
    "plt.plot(x,test_error, label ='Testing')\n",
    "plt.legend(loc = 2)\n",
    "plt.title('MAE')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "3297e16d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.089616358830082\n"
     ]
    }
   ],
   "source": [
    "print(np.min(test_error))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c0402c1",
   "metadata": {},
   "source": [
    "# LBFGS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c69bc75",
   "metadata": {},
   "source": [
    "## Y = b0 + b1X1 (96hr) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "c16a90fd",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] \"1 EOF ; RMS = 10.27211422\"\n",
      "[1] \"1 EOF ; RMS = 10.26501442\"\n",
      "[1] \"1 EOF ; RMS = 10.26492936\"\n",
      "[1] \"1 EOF ; RMS = 10.26497963\"\n",
      "[1] \"2 EOF ; RMS = 9.45481862\"\n",
      "[1] \"2 EOF ; RMS = 9.45536768\"\n",
      "[1] \"3 EOF ; RMS = 9.11077811\"\n",
      "[1] \"3 EOF ; RMS = 9.11367005\"\n",
      "[1] \"4 EOF ; RMS = 8.88785888\"\n",
      "[1] \"4 EOF ; RMS = 8.89446532\"\n",
      "[1] \"5 EOF ; RMS = 8.73260332\"\n",
      "[1] \"5 EOF ; RMS = 8.75194643\"\n",
      "[1] \"6 EOF ; RMS = 8.60766547\"\n",
      "[1] \"6 EOF ; RMS = 8.61449742\"\n",
      "[1] \"7 EOF ; RMS = 8.55258166\"\n",
      "[1] \"7 EOF ; RMS = 8.56998542\"\n",
      "[1] \"8 EOF ; RMS = 8.4745616\"\n",
      "[1] \"8 EOF ; RMS = 8.48084076\"\n",
      "[1] \"9 EOF ; RMS = 8.40640192\"\n",
      "[1] \"9 EOF ; RMS = 8.42013927\"\n",
      "[1] \"10 EOF ; RMS = 8.40354041\"\n",
      "[1] \"10 EOF ; RMS = 8.43096272\"\n",
      "[1] \"11 EOF ; RMS = 8.39670075\"\n",
      "[1] \"11 EOF ; RMS = 8.42219993\"\n",
      "[1] \"12 EOF ; RMS = 8.39549966\"\n",
      "[1] \"12 EOF ; RMS = 8.42182266\"\n",
      "[1] \"13 EOF ; RMS = 8.36028745\"\n",
      "[1] \"13 EOF ; RMS = 8.37477931\"\n",
      "[1] \"14 EOF ; RMS = 8.38182397\"\n",
      "[1] \"1 EOF ; RMS = 8.08186998\"\n",
      "[1] \"1 EOF ; RMS = 8.037959\"\n",
      "[1] \"1 EOF ; RMS = 8.03683876\"\n",
      "[1] \"1 EOF ; RMS = 8.03677003\"\n",
      "[1] \"1 EOF ; RMS = 8.03676371\"\n",
      "[1] \"2 EOF ; RMS = 7.42202122\"\n",
      "[1] \"2 EOF ; RMS = 7.42105799\"\n",
      "[1] \"2 EOF ; RMS = 7.42107887\"\n",
      "[1] \"3 EOF ; RMS = 6.96501193\"\n",
      "[1] \"3 EOF ; RMS = 6.96456216\"\n",
      "[1] \"3 EOF ; RMS = 6.96460633\"\n",
      "[1] \"4 EOF ; RMS = 6.7017535\"\n",
      "[1] \"4 EOF ; RMS = 6.70227814\"\n",
      "[1] \"5 EOF ; RMS = 6.53605418\"\n",
      "[1] \"5 EOF ; RMS = 6.53801172\"\n",
      "[1] \"6 EOF ; RMS = 6.41603976\"\n",
      "[1] \"6 EOF ; RMS = 6.42154965\"\n",
      "[1] \"7 EOF ; RMS = 6.3157182\"\n",
      "[1] \"7 EOF ; RMS = 6.32042346\"\n",
      "[1] \"8 EOF ; RMS = 6.23281102\"\n",
      "[1] \"8 EOF ; RMS = 6.23638602\"\n",
      "[1] \"9 EOF ; RMS = 6.12312228\"\n",
      "[1] \"9 EOF ; RMS = 6.12435599\"\n",
      "[1] \"10 EOF ; RMS = 6.0381368\"\n",
      "[1] \"10 EOF ; RMS = 6.04052906\"\n",
      "[1] \"11 EOF ; RMS = 5.95837003\"\n",
      "[1] \"11 EOF ; RMS = 5.96276625\"\n",
      "[1] \"12 EOF ; RMS = 5.87521709\"\n",
      "[1] \"12 EOF ; RMS = 5.87547879\"\n",
      "[1] \"13 EOF ; RMS = 5.81938647\"\n",
      "[1] \"13 EOF ; RMS = 5.82402735\"\n",
      "[1] \"14 EOF ; RMS = 5.75935115\"\n",
      "[1] \"14 EOF ; RMS = 5.76041611\"\n",
      "[1] \"15 EOF ; RMS = 5.72729377\"\n",
      "[1] \"15 EOF ; RMS = 5.73029604\"\n",
      "[1] \"16 EOF ; RMS = 5.71322465\"\n",
      "[1] \"16 EOF ; RMS = 5.7208336\"\n",
      "[1] \"17 EOF ; RMS = 5.68041479\"\n",
      "[1] \"17 EOF ; RMS = 5.682057\"\n",
      "[1] \"18 EOF ; RMS = 5.64622255\"\n",
      "[1] \"18 EOF ; RMS = 5.65034722\"\n",
      "[1] \"19 EOF ; RMS = 5.62816047\"\n",
      "[1] \"19 EOF ; RMS = 5.63593125\"\n",
      "[1] \"20 EOF ; RMS = 5.62106307\"\n",
      "[1] \"20 EOF ; RMS = 5.63273556\"\n",
      "[1] \"21 EOF ; RMS = 5.61041787\"\n",
      "[1] \"21 EOF ; RMS = 5.61892732\"\n",
      "[1] \"22 EOF ; RMS = 5.59607061\"\n",
      "[1] \"22 EOF ; RMS = 5.60225936\"\n",
      "[1] \"23 EOF ; RMS = 5.57844668\"\n",
      "[1] \"23 EOF ; RMS = 5.58361828\"\n",
      "[1] \"24 EOF ; RMS = 5.55952639\"\n",
      "[1] \"24 EOF ; RMS = 5.56513732\"\n",
      "[1] \"25 EOF ; RMS = 5.5546586\"\n",
      "[1] \"25 EOF ; RMS = 5.56408173\"\n",
      "[1] \"26 EOF ; RMS = 5.54524709\"\n",
      "[1] \"26 EOF ; RMS = 5.55170842\"\n",
      "[1] \"27 EOF ; RMS = 5.55091065\"\n",
      "[1] \"27 EOF ; RMS = 5.56187604\"\n",
      "[1] \"28 EOF ; RMS = 5.54680223\"\n",
      "[1] \"28 EOF ; RMS = 5.55515983\"\n",
      "[1] \"29 EOF ; RMS = 5.55069933\"\n",
      "[1] \"29 EOF ; RMS = 5.56155862\"\n",
      "[1] \"30 EOF ; RMS = 5.54961684\"\n",
      "[1] \"30 EOF ; RMS = 5.55899011\"\n",
      "[1] \"31 EOF ; RMS = 5.55266631\"\n",
      "[1] \"31 EOF ; RMS = 5.56207369\"\n",
      "[1] \"32 EOF ; RMS = 5.55717722\"\n",
      "[1] \"32 EOF ; RMS = 5.57040704\"\n",
      "[1] \"33 EOF ; RMS = 5.56803565\"\n",
      "[1] \"33 EOF ; RMS = 5.58148836\"\n",
      "[1] \"34 EOF ; RMS = 5.58423977\"\n"
     ]
    }
   ],
   "source": [
    "###\n",
    "#cal_PMf  \n",
    "###\n",
    "u=np.zeros([244,6816])\n",
    "for i in range (0,244):\n",
    "    a=np.array(data['cal_PMf'][6816*i:6816*i+6816])\n",
    "    for j in range(0,6816):\n",
    "        if a[j]=='\\\\N' :\n",
    "            a[j]=np.nan\n",
    "    u[i]=a.T\n",
    "###\n",
    "#obs_PMf\n",
    "###\n",
    "v=np.zeros([244,6816])\n",
    "for i in range (0,244):\n",
    "    a=np.array(data['obs_PMf'][6816*i:6816*i+6816])\n",
    "    for j in range(0,6816):\n",
    "        if a[j]=='\\\\N' :\n",
    "            a[j]=np.nan\n",
    "    v[i]=a.T\n",
    "    \n",
    "XRestruct=sinkr.dineof(u)\n",
    "YRestruct=sinkr.dineof(v)\n",
    "XRestruct_Fun=np.array(XRestruct[0])\n",
    "YRestruct_Fun=np.array(YRestruct[0])\n",
    "\n",
    "Xhat=XRestruct_Fun\n",
    "Yhat=YRestruct_Fun\n",
    "Xhat_train = np.zeros([244,6816])\n",
    "Yhat_train = np.zeros([244,6816])\n",
    "for i in range (0,244):\n",
    "    for j in range (0,6816):\n",
    "        Xhat_train[i][j] = Xhat[i][j]\n",
    "        Yhat_train[i][j] = Yhat[i][j]\n",
    "\n",
    "Xhat_test = np.zeros([30,6816])\n",
    "Yhat_test = np.zeros([30,6816])\n",
    "#data_cal\n",
    "for i in range (244,274):\n",
    "    a=np.array(data['cal_PMf'][6816*i:6816*i+6816])\n",
    "    for j in range(0,6816):\n",
    "        if a[j]=='\\\\N' :\n",
    "            a[j]=np.nan\n",
    "    Xhat_test[i-244]=a\n",
    "#data_obs\n",
    "for i in range (244,274):\n",
    "    a=np.array(data['obs_PMf'][6816*i:6816*i+6816])\n",
    "    for j in range(0,6816):\n",
    "        if a[j]=='\\\\N' :\n",
    "            a[j]=np.nan\n",
    "    Yhat_test[i-244]=a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "93cc781c",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = Xhat_train\n",
    "y = Yhat_train\n",
    "xt = Xhat_test\n",
    "yt = Yhat_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "afb125e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model (x,b0,b1):\n",
    "    # y = b0 +  torch.matmul(x,b1)\n",
    "    y = b0 + b1*x\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "ecce8d1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Training loss 144.8100, Testing loss 7.2946\n",
      "Epoch 1, Training loss 132.1356, Testing loss 7.1739\n",
      "Epoch 2, Training loss 123.6289, Testing loss 7.1072\n",
      "Epoch 3, Training loss 117.7867, Testing loss 7.0728\n",
      "Epoch 4, Training loss 113.6954, Testing loss 7.0577\n",
      "Epoch 5, Training loss 110.7545, Testing loss 7.0538\n",
      "Epoch 6, Training loss 108.5668, Testing loss 7.0549\n",
      "Epoch 7, Training loss 106.8258, Testing loss 7.0555\n",
      "Epoch 8, Training loss 105.1671, Testing loss 7.0472\n",
      "Epoch 9, Training loss 103.1031, Testing loss 7.0117\n",
      "Epoch 10, Training loss 100.0359, Testing loss 6.9113\n",
      "Epoch 20, Training loss 83.8789, Testing loss 6.0090\n",
      "Epoch 30, Training loss 83.1840, Testing loss 5.9521\n",
      "Epoch 40, Training loss 83.1389, Testing loss 5.9490\n",
      "Epoch 50, Training loss 83.1365, Testing loss 5.9494\n",
      "Epoch 60, Training loss 83.1363, Testing loss 5.9495\n",
      "Epoch 70, Training loss 83.1362, Testing loss 5.9496\n",
      "Epoch 80, Training loss 83.1362, Testing loss 5.9496\n",
      "Epoch 90, Training loss 83.1361, Testing loss 5.9496\n",
      "Epoch 100, Training loss 83.1361, Testing loss 5.9497\n",
      "Epoch 110, Training loss 83.1361, Testing loss 5.9497\n",
      "Epoch 120, Training loss 83.1361, Testing loss 5.9497\n",
      "Epoch 130, Training loss 83.1361, Testing loss 5.9497\n",
      "Epoch 140, Training loss 83.1361, Testing loss 5.9497\n",
      "Epoch 150, Training loss 83.1361, Testing loss 5.9497\n",
      "Epoch 160, Training loss 83.1361, Testing loss 5.9497\n",
      "Epoch 170, Training loss 83.1361, Testing loss 5.9498\n",
      "Epoch 180, Training loss 83.1361, Testing loss 5.9498\n",
      "Epoch 190, Training loss 83.1360, Testing loss 5.9498\n",
      "Epoch 191, Training loss 83.1361, Testing loss 5.9498\n",
      "Epoch 192, Training loss 83.1361, Testing loss 5.9498\n",
      "Epoch 193, Training loss 83.1361, Testing loss 5.9498\n",
      "Epoch 194, Training loss 83.1360, Testing loss 5.9498\n",
      "Epoch 195, Training loss 83.1360, Testing loss 5.9498\n",
      "Epoch 196, Training loss 83.1361, Testing loss 5.9498\n",
      "Epoch 197, Training loss 83.1360, Testing loss 5.9498\n",
      "Epoch 198, Training loss 83.1361, Testing loss 5.9498\n",
      "Epoch 199, Training loss 83.1361, Testing loss 5.9498\n",
      "Epoch 200, Training loss 83.1360, Testing loss 5.9498\n"
     ]
    }
   ],
   "source": [
    "features = torch.from_numpy(x)\n",
    "targets = torch.from_numpy(y)\n",
    "x_test = torch.from_numpy(xt)\n",
    "y_test = torch.from_numpy(yt)\n",
    "\n",
    "beta0 = torch.ones(6816 , requires_grad = True)\n",
    "# beta0 = torch.randn([244,6816] , requires_grad = True)\n",
    "beta1 = torch.ones(6816 , requires_grad = True)\n",
    "\n",
    "rate = 1e-2\n",
    "optimizer = optim.LBFGS([beta0 , beta1] , lr = rate)\n",
    "\n",
    "epo = 201\n",
    "loss = nn.MSELoss()\n",
    "train_error = np.zeros(epo)\n",
    "test_error = np.zeros(epo)\n",
    "\n",
    "\n",
    "for epoch in range (epo):\n",
    "    # yhats_train = model(features.float() , beta0 , beta1)\n",
    "    # train_loss = loss(targets.float() , yhats_train)\n",
    "    # train_error[epoch] = train_loss\n",
    "\n",
    "    # optimizer.zero_grad()\n",
    "    # if epoch == 0 :\n",
    "    #         train_loss.backward(retain_graph=True) \n",
    "    # else :\n",
    "    #     train_loss.backward()\n",
    "    # # train_loss.backward() \n",
    "    # optimizer.step()    \n",
    "\n",
    "    def closure():\n",
    "        yhats_train = model(features.float() , beta0 , beta1)\n",
    "        train_loss = loss(targets.float() , yhats_train)\n",
    "        train_error[epoch] = train_loss\n",
    "        optimizer.zero_grad()\n",
    "        # if epoch == 0 :\n",
    "        #     train_loss.backward(retain_graph=True) \n",
    "        # else :\n",
    "        #     train_loss.backward()\n",
    "        train_loss.backward(retain_graph=True) \n",
    "        return train_loss\n",
    "    optimizer.step(closure)    \n",
    "\n",
    "    yhats_test = model(x_test.float(), beta0, beta1) \n",
    "#     for i in range (30):\n",
    "#         for j in range (6816):\n",
    "#             if y_test[i][j] == 0:\n",
    "#                 yhats_test[i][j] = 0\n",
    "    r = torch.abs(yhats_test - y_test)\n",
    "    test_loss = torch.nanmean(r)\n",
    "    test_error[epoch] = test_loss\n",
    "\n",
    "    if epoch <= 10 or epoch % 10 == 0:\n",
    "        print(f\"Epoch {epoch}, Training loss {train_error[epoch]:.4f},\"\n",
    "                    f\" Testing loss {test_error[epoch]:.4f}\")\n",
    "        # print(f\"Epoch {epoch}, Training loss {train_loss.item():.4f},\"\n",
    "        #             f\" Testing loss {test_loss:.4f}\")\n",
    "        # print('\\tBeta_0 : ' , beta0.grad)\n",
    "        # print('\\tBeta_1 : ' , beta1.grad)\n",
    "    else :\n",
    "        if epoch >= epo-10 :\n",
    "            print(f\"Epoch {epoch}, Training loss {train_error[epoch]:.4f},\"\n",
    "                    f\" Testing loss {test_error[epoch]:.4f}\")\n",
    "            # print(f\"Epoch {epoch}, Training loss {train_loss.item():.4f},\"\n",
    "            #             f\" Testing loss {test_loss:.4f}\")\n",
    "            # print('\\tBeta_0 : ' , beta0)\n",
    "            # print('\\tBeta_1 : ' , beta1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "77d5ec24",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig1 = plt.figure(1)\n",
    "x=np.linspace(1,epo,epo)\n",
    "plt.plot(x,train_error, label = 'Training')\n",
    "plt.plot(x,test_error, label ='Testing')\n",
    "plt.legend(loc = 2)\n",
    "plt.title('MAE')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "df9058c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.948969441248646\n"
     ]
    }
   ],
   "source": [
    "print(np.min(test_error))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e777240c",
   "metadata": {},
   "source": [
    "## Y = b0 + b1X12 (120hr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "61cd0aaf",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] \"1 EOF ; RMS = 7.59471921\"\n",
      "[1] \"1 EOF ; RMS = 7.55029624\"\n",
      "[1] \"1 EOF ; RMS = 7.54926743\"\n",
      "[1] \"1 EOF ; RMS = 7.54914693\"\n",
      "[1] \"1 EOF ; RMS = 7.54912286\"\n",
      "[1] \"1 EOF ; RMS = 7.54911756\"\n",
      "[1] \"2 EOF ; RMS = 6.65979528\"\n",
      "[1] \"2 EOF ; RMS = 6.65246361\"\n",
      "[1] \"2 EOF ; RMS = 6.65174798\"\n",
      "[1] \"2 EOF ; RMS = 6.65160043\"\n",
      "[1] \"2 EOF ; RMS = 6.65156239\"\n",
      "[1] \"2 EOF ; RMS = 6.651552\"\n",
      "[1] \"2 EOF ; RMS = 6.65154913\"\n",
      "[1] \"3 EOF ; RMS = 6.21193435\"\n",
      "[1] \"3 EOF ; RMS = 6.21394993\"\n",
      "[1] \"4 EOF ; RMS = 5.88874064\"\n",
      "[1] \"4 EOF ; RMS = 5.9006884\"\n",
      "[1] \"5 EOF ; RMS = 5.6404382\"\n",
      "[1] \"5 EOF ; RMS = 5.64009827\"\n",
      "[1] \"5 EOF ; RMS = 5.64048726\"\n",
      "[1] \"6 EOF ; RMS = 5.50322584\"\n",
      "[1] \"6 EOF ; RMS = 5.50282326\"\n",
      "[1] \"6 EOF ; RMS = 5.50264793\"\n",
      "[1] \"6 EOF ; RMS = 5.5025459\"\n",
      "[1] \"6 EOF ; RMS = 5.50250614\"\n",
      "[1] \"6 EOF ; RMS = 5.50249248\"\n",
      "[1] \"6 EOF ; RMS = 5.50248806\"\n",
      "[1] \"7 EOF ; RMS = 5.4359113\"\n",
      "[1] \"7 EOF ; RMS = 5.44968466\"\n",
      "[1] \"8 EOF ; RMS = 5.38396247\"\n",
      "[1] \"8 EOF ; RMS = 5.39245792\"\n",
      "[1] \"9 EOF ; RMS = 5.35671467\"\n",
      "[1] \"9 EOF ; RMS = 5.3645738\"\n",
      "[1] \"10 EOF ; RMS = 5.34398409\"\n",
      "[1] \"10 EOF ; RMS = 5.36116817\"\n",
      "[1] \"11 EOF ; RMS = 5.30820096\"\n",
      "[1] \"11 EOF ; RMS = 5.31621899\"\n",
      "[1] \"12 EOF ; RMS = 5.30703902\"\n",
      "[1] \"12 EOF ; RMS = 5.32487725\"\n",
      "[1] \"13 EOF ; RMS = 5.29863281\"\n",
      "[1] \"13 EOF ; RMS = 5.3278881\"\n",
      "[1] \"14 EOF ; RMS = 5.32042825\"\n",
      "[1] \"14 EOF ; RMS = 5.34793563\"\n",
      "[1] \"15 EOF ; RMS = 5.35688409\"\n",
      "[1] \"1 EOF ; RMS = 10.31254371\"\n",
      "[1] \"1 EOF ; RMS = 10.29851693\"\n",
      "[1] \"1 EOF ; RMS = 10.29802932\"\n",
      "[1] \"1 EOF ; RMS = 10.29801665\"\n",
      "[1] \"1 EOF ; RMS = 10.29803224\"\n",
      "[1] \"2 EOF ; RMS = 9.51917635\"\n",
      "[1] \"2 EOF ; RMS = 9.52089772\"\n",
      "[1] \"3 EOF ; RMS = 9.243099\"\n",
      "[1] \"3 EOF ; RMS = 9.24827915\"\n",
      "[1] \"4 EOF ; RMS = 8.99893459\"\n",
      "[1] \"4 EOF ; RMS = 9.00315639\"\n",
      "[1] \"5 EOF ; RMS = 8.76573805\"\n",
      "[1] \"5 EOF ; RMS = 8.76405758\"\n",
      "[1] \"5 EOF ; RMS = 8.764979\"\n",
      "[1] \"6 EOF ; RMS = 8.63173787\"\n",
      "[1] \"6 EOF ; RMS = 8.64559122\"\n",
      "[1] \"7 EOF ; RMS = 8.54189416\"\n",
      "[1] \"7 EOF ; RMS = 8.55350967\"\n",
      "[1] \"8 EOF ; RMS = 8.5071732\"\n",
      "[1] \"8 EOF ; RMS = 8.52360873\"\n",
      "[1] \"9 EOF ; RMS = 8.46579587\"\n",
      "[1] \"9 EOF ; RMS = 8.48454002\"\n",
      "[1] \"10 EOF ; RMS = 8.48251511\"\n",
      "[1] \"10 EOF ; RMS = 8.50367132\"\n",
      "[1] \"11 EOF ; RMS = 8.5112069\"\n",
      "[1] \"1 EOF ; RMS = 9.39837307\"\n",
      "[1] \"1 EOF ; RMS = 9.38497175\"\n",
      "[1] \"1 EOF ; RMS = 9.38490726\"\n",
      "[1] \"1 EOF ; RMS = 9.38496257\"\n",
      "[1] \"2 EOF ; RMS = 8.6481999\"\n",
      "[1] \"2 EOF ; RMS = 8.64785388\"\n",
      "[1] \"2 EOF ; RMS = 8.64778005\"\n",
      "[1] \"2 EOF ; RMS = 8.64774138\"\n",
      "[1] \"2 EOF ; RMS = 8.64772567\"\n",
      "[1] \"2 EOF ; RMS = 8.64771941\"\n",
      "[1] \"3 EOF ; RMS = 8.42827998\"\n",
      "[1] \"3 EOF ; RMS = 8.43161704\"\n",
      "[1] \"4 EOF ; RMS = 8.21888507\"\n",
      "[1] \"4 EOF ; RMS = 8.22318076\"\n",
      "[1] \"5 EOF ; RMS = 8.02431587\"\n",
      "[1] \"5 EOF ; RMS = 8.02838588\"\n",
      "[1] \"6 EOF ; RMS = 7.9121486\"\n",
      "[1] \"6 EOF ; RMS = 7.91819867\"\n",
      "[1] \"7 EOF ; RMS = 7.82951138\"\n",
      "[1] \"7 EOF ; RMS = 7.83863035\"\n",
      "[1] \"8 EOF ; RMS = 7.78158327\"\n",
      "[1] \"8 EOF ; RMS = 7.79534542\"\n",
      "[1] \"9 EOF ; RMS = 7.74171352\"\n",
      "[1] \"9 EOF ; RMS = 7.74696354\"\n",
      "[1] \"10 EOF ; RMS = 7.7143963\"\n",
      "[1] \"10 EOF ; RMS = 7.73034312\"\n",
      "[1] \"11 EOF ; RMS = 7.69488179\"\n",
      "[1] \"11 EOF ; RMS = 7.70970968\"\n",
      "[1] \"12 EOF ; RMS = 7.67204377\"\n",
      "[1] \"12 EOF ; RMS = 7.68912414\"\n",
      "[1] \"13 EOF ; RMS = 7.65625132\"\n",
      "[1] \"13 EOF ; RMS = 7.66499336\"\n",
      "[1] \"14 EOF ; RMS = 7.61621204\"\n",
      "[1] \"14 EOF ; RMS = 7.62253252\"\n",
      "[1] \"15 EOF ; RMS = 7.56949099\"\n",
      "[1] \"15 EOF ; RMS = 7.57998364\"\n",
      "[1] \"16 EOF ; RMS = 7.5477638\"\n",
      "[1] \"16 EOF ; RMS = 7.55752416\"\n",
      "[1] \"17 EOF ; RMS = 7.55373788\"\n",
      "[1] \"17 EOF ; RMS = 7.57869734\"\n",
      "[1] \"18 EOF ; RMS = 7.54468891\"\n",
      "[1] \"18 EOF ; RMS = 7.56177362\"\n",
      "[1] \"19 EOF ; RMS = 7.54234725\"\n",
      "[1] \"19 EOF ; RMS = 7.5606224\"\n",
      "[1] \"20 EOF ; RMS = 7.55148894\"\n",
      "[1] \"20 EOF ; RMS = 7.57433678\"\n",
      "[1] \"21 EOF ; RMS = 7.57485414\"\n",
      "[1] \"1 EOF ; RMS = 7.79762666\"\n",
      "[1] \"1 EOF ; RMS = 7.76715314\"\n",
      "[1] \"1 EOF ; RMS = 7.76739757\"\n",
      "[1] \"2 EOF ; RMS = 7.21255022\"\n",
      "[1] \"2 EOF ; RMS = 7.21530387\"\n",
      "[1] \"3 EOF ; RMS = 6.77644125\"\n",
      "[1] \"3 EOF ; RMS = 6.77732631\"\n",
      "[1] \"4 EOF ; RMS = 6.51017725\"\n",
      "[1] \"4 EOF ; RMS = 6.51198998\"\n",
      "[1] \"5 EOF ; RMS = 6.35439742\"\n",
      "[1] \"5 EOF ; RMS = 6.35515939\"\n",
      "[1] \"6 EOF ; RMS = 6.23637766\"\n",
      "[1] \"6 EOF ; RMS = 6.23970421\"\n",
      "[1] \"7 EOF ; RMS = 6.09971506\"\n",
      "[1] \"7 EOF ; RMS = 6.1002081\"\n",
      "[1] \"8 EOF ; RMS = 5.98588456\"\n",
      "[1] \"8 EOF ; RMS = 5.98674283\"\n",
      "[1] \"9 EOF ; RMS = 5.9115073\"\n",
      "[1] \"9 EOF ; RMS = 5.91622781\"\n",
      "[1] \"10 EOF ; RMS = 5.84529779\"\n",
      "[1] \"10 EOF ; RMS = 5.84795712\"\n",
      "[1] \"11 EOF ; RMS = 5.77063662\"\n",
      "[1] \"11 EOF ; RMS = 5.773146\"\n",
      "[1] \"12 EOF ; RMS = 5.70692322\"\n",
      "[1] \"12 EOF ; RMS = 5.71151244\"\n",
      "[1] \"13 EOF ; RMS = 5.65593709\"\n",
      "[1] \"13 EOF ; RMS = 5.6579133\"\n",
      "[1] \"14 EOF ; RMS = 5.586827\"\n",
      "[1] \"14 EOF ; RMS = 5.58687634\"\n",
      "[1] \"15 EOF ; RMS = 5.5361448\"\n",
      "[1] \"15 EOF ; RMS = 5.5375973\"\n",
      "[1] \"16 EOF ; RMS = 5.48937869\"\n",
      "[1] \"16 EOF ; RMS = 5.49126954\"\n",
      "[1] \"17 EOF ; RMS = 5.46169022\"\n",
      "[1] \"17 EOF ; RMS = 5.46629682\"\n",
      "[1] \"18 EOF ; RMS = 5.43623747\"\n",
      "[1] \"18 EOF ; RMS = 5.44068251\"\n",
      "[1] \"19 EOF ; RMS = 5.40617791\"\n",
      "[1] \"19 EOF ; RMS = 5.40898541\"\n",
      "[1] \"20 EOF ; RMS = 5.384352\"\n",
      "[1] \"20 EOF ; RMS = 5.38768709\"\n",
      "[1] \"21 EOF ; RMS = 5.35892351\"\n",
      "[1] \"21 EOF ; RMS = 5.36379565\"\n",
      "[1] \"22 EOF ; RMS = 5.34763443\"\n",
      "[1] \"22 EOF ; RMS = 5.35420179\"\n",
      "[1] \"23 EOF ; RMS = 5.31562262\"\n",
      "[1] \"23 EOF ; RMS = 5.31878986\"\n",
      "[1] \"24 EOF ; RMS = 5.29095829\"\n",
      "[1] \"24 EOF ; RMS = 5.29532343\"\n",
      "[1] \"25 EOF ; RMS = 5.28516302\"\n",
      "[1] \"25 EOF ; RMS = 5.29352343\"\n",
      "[1] \"26 EOF ; RMS = 5.27397073\"\n",
      "[1] \"26 EOF ; RMS = 5.2796649\"\n",
      "[1] \"27 EOF ; RMS = 5.26330344\"\n",
      "[1] \"27 EOF ; RMS = 5.26785985\"\n",
      "[1] \"28 EOF ; RMS = 5.24596986\"\n",
      "[1] \"28 EOF ; RMS = 5.25098676\"\n",
      "[1] \"29 EOF ; RMS = 5.22984461\"\n",
      "[1] \"29 EOF ; RMS = 5.23408972\"\n",
      "[1] \"30 EOF ; RMS = 5.22631747\"\n",
      "[1] \"30 EOF ; RMS = 5.23796196\"\n",
      "[1] \"31 EOF ; RMS = 5.23186686\"\n",
      "[1] \"31 EOF ; RMS = 5.24058764\"\n",
      "[1] \"32 EOF ; RMS = 5.22487676\"\n",
      "[1] \"32 EOF ; RMS = 5.2295139\"\n",
      "[1] \"33 EOF ; RMS = 5.21452386\"\n",
      "[1] \"33 EOF ; RMS = 5.22048017\"\n",
      "[1] \"34 EOF ; RMS = 5.22034679\"\n",
      "[1] \"34 EOF ; RMS = 5.23450009\"\n",
      "[1] \"35 EOF ; RMS = 5.23949231\"\n"
     ]
    }
   ],
   "source": [
    "x1=np.zeros([243,1704])\n",
    "x2=np.zeros([243,6816])\n",
    "\n",
    "#x1 (0th~1703th column as x)\n",
    "for i in range (0,243):\n",
    "    for j in range (0,71):\n",
    "        a=np.array(data['obs_PMf'][6816*i+96*j:6816*i+96*j+24])\n",
    "        for k in range (0,24):\n",
    "            if a[k]=='\\\\N' :\n",
    "                a[k]=np.nan\n",
    "        for k in range (0,24):\n",
    "            x1[i][j*24+k]=a[k]\n",
    "\n",
    "#x2 (1704th~8519th column as x)\n",
    "for i in range (1,244):\n",
    "    b=np.array(data['cal_PMf'][6816*i:6816*i+6816])\n",
    "    for j in range(0,6816):\n",
    "        if b[j]=='\\\\N' :\n",
    "            b[j]=np.nan\n",
    "    for j in range(0,6816):\n",
    "        x2[i-1][j]=b[j]\n",
    "        \n",
    "x1Restruct=sinkr.dineof(x1)\n",
    "x2Restruct=sinkr.dineof(x2)\n",
    "x1Restruct_Fun=np.array(x1Restruct[0])\n",
    "x2Restruct_Fun=np.array(x2Restruct[0])\n",
    "\n",
    "YRestruct_Fun=np.zeros([239,6816])\n",
    "for i in range (0,239):\n",
    "    for j in range (0,1704):\n",
    "        YRestruct_Fun[i][j]=x1Restruct_Fun[i+1][j]\n",
    "    for j in range (1704,3408):\n",
    "        YRestruct_Fun[i][j]=x1Restruct_Fun[i+2][j-1704]\n",
    "    for j in range (3408,5112):\n",
    "        YRestruct_Fun[i][j]=x1Restruct_Fun[i+3][j-3408]\n",
    "    for j in range (5112,6816):\n",
    "        YRestruct_Fun[i][j]=x1Restruct_Fun[i+4][j-5112]\n",
    "        \n",
    "XRestruct_Fun=np.zeros([239,8520])\n",
    "for i in range (0,239):\n",
    "    for j in range (0,1704):\n",
    "        XRestruct_Fun[i][j]=x1Restruct_Fun[i][j]\n",
    "    for j in range (1704,8520):\n",
    "        XRestruct_Fun[i][j]=x2Restruct_Fun[i][j-1704]\n",
    "        \n",
    "Xhat=XRestruct_Fun\n",
    "Yhat=YRestruct_Fun\n",
    "Xhat_train = np.zeros([239,8520])\n",
    "Yhat_train = np.zeros([239,6816])\n",
    "for i in range (0,239):\n",
    "    for j in range (0,8520):\n",
    "        Xhat_train[i][j] = Xhat[i][j]\n",
    "    for j in range (0,6816):    \n",
    "        Yhat_train[i][j] = Yhat[i][j]\n",
    "\n",
    "Xhat_test = np.zeros([30,8520])\n",
    "Yhat_test = np.zeros([30,6816])\n",
    "x=np.zeros([273,8520])\n",
    "y=np.zeros([273,6816])\n",
    "#x1 (0th~1703th column as x)\n",
    "for i in range (0,273):\n",
    "    for j in range (0,71):\n",
    "        a=np.array(data['obs_PMf'][6816*i+96*j:6816*i+96*j+24])\n",
    "        for k in range (0,24):\n",
    "            if a[k]=='\\\\N' :\n",
    "                a[k]=np.nan\n",
    "        for k in range (0,24):\n",
    "            x[i][j*24+k]=a[k]\n",
    "\n",
    "#x2 (1704th~8519th column as x)\n",
    "for i in range (1,274):\n",
    "    b=np.array(data['cal_PMf'][6816*i:6816*i+6816])\n",
    "    for j in range(0,6816):\n",
    "        if b[j]=='\\\\N' :\n",
    "            b[j]=np.nan\n",
    "    for j in range(0,6816):\n",
    "        x[i-1][j+1704]=b[j]\n",
    "\n",
    "#data_obs\n",
    "for i in range (1,274):\n",
    "    a=np.array(data['obs_PMf'][6816*i:6816*i+6816])\n",
    "    for j in range(0,6816):\n",
    "        if a[j]=='\\\\N' :\n",
    "            a[j]=np.nan\n",
    "    y[i-1]=a\n",
    "xRestruct=sinkr.dineof(x)\n",
    "yRestruct=sinkr.dineof(y)\n",
    "xRestruct_Fun=np.array(xRestruct[0])\n",
    "yRestruct_Fun=np.array(yRestruct[0])\n",
    "    \n",
    "for i in range (243,273):\n",
    "    for j in range (0,8520):\n",
    "        Xhat_test[i-243][j]=xRestruct_Fun[i][j]\n",
    "    for j in range(0,6816):\n",
    "        Yhat_test[i-243][j]=yRestruct_Fun[i][j]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1ae15bd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = Xhat_train\n",
    "y = Yhat_train\n",
    "xt = Xhat_test\n",
    "yt = Yhat_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "991411bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model (x,b0,b1):\n",
    "    y = b0 +  torch.mm(x , b1)\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c516fa7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Training loss 21391738880.0000, Testing loss 122887.4376\n",
      "Epoch 1, Training loss 715327012864.0000, Testing loss 697812.4868\n",
      "Epoch 2, Training loss 478533451776.0000, Testing loss 570589.2935\n"
     ]
    }
   ],
   "source": [
    "features = torch.from_numpy(x)\n",
    "targets = torch.from_numpy(y)\n",
    "x_test = torch.from_numpy(xt)\n",
    "y_test = torch.from_numpy(yt)\n",
    "\n",
    "beta0 = torch.ones(6816 , requires_grad = True)\n",
    "beta1 = torch.ones([8520,6816] , requires_grad = True)\n",
    "\n",
    "rate = 1e-2\n",
    "optimizer = optim.LBFGS([beta0 , beta1] , lr = rate)\n",
    "\n",
    "epo = 201\n",
    "loss = nn.MSELoss()\n",
    "train_error = np.zeros(epo)\n",
    "test_error = np.zeros(epo)\n",
    "\n",
    "\n",
    "for epoch in range (epo):\n",
    "    # yhats_train = model(features.float() , beta0 , beta1)\n",
    "    # train_loss = loss(targets.float() , yhats_train)\n",
    "    # train_error[epoch] = train_loss\n",
    "\n",
    "    # optimizer.zero_grad()\n",
    "    # if epoch == 0 :\n",
    "    #         train_loss.backward(retain_graph=True) \n",
    "    # else :\n",
    "    #     train_loss.backward()\n",
    "    # # train_loss.backward() \n",
    "    # optimizer.step()    \n",
    "\n",
    "    def closure():\n",
    "        yhats_train = model(features.float() , beta0 , beta1)\n",
    "        train_loss = loss(targets.float() , yhats_train)\n",
    "        train_error[epoch] = train_loss\n",
    "        optimizer.zero_grad()\n",
    "        # if epoch == 0 :\n",
    "        #     train_loss.backward(retain_graph=True) \n",
    "        # else :\n",
    "        #     train_loss.backward()\n",
    "        train_loss.backward(retain_graph=True) \n",
    "        return train_loss\n",
    "    optimizer.step(closure)    \n",
    "\n",
    "    yhats_test = model(x_test.float(), beta0, beta1) \n",
    "#     for i in range (25):\n",
    "#         for j in range (6816):\n",
    "#             if y_test[i][j] == 0:\n",
    "#                 yhats_test[i][j] = 0\n",
    "    r = torch.abs(yhats_test - y_test)\n",
    "    test_loss = torch.nanmean(r)\n",
    "    test_error[epoch] = test_loss\n",
    "\n",
    "    if epoch <= 10 or epoch % 10 == 0:\n",
    "        print(f\"Epoch {epoch}, Training loss {train_error[epoch]:.4f},\"\n",
    "                    f\" Testing loss {test_error[epoch]:.4f}\")\n",
    "        # print(f\"Epoch {epoch}, Training loss {train_loss.item():.4f},\"\n",
    "        #             f\" Testing loss {test_loss:.4f}\")\n",
    "        # print('\\tBeta_0 : ' , beta0.grad)\n",
    "        # print('\\tBeta_1 : ' , beta1.grad)\n",
    "    else :\n",
    "        if epoch >= epo-10 :\n",
    "            print(f\"Epoch {epoch}, Training loss {train_error[epoch]:.4f},\"\n",
    "                        f\" Testing loss {test_error[epoch]:.4f}\")\n",
    "            # print(f\"Epoch {epoch}, Training loss {train_loss.item():.4f},\"\n",
    "            #             f\" Testing loss {test_loss:.4f}\")\n",
    "            # print('\\tBeta_0 : ' , beta0)\n",
    "            # print('\\tBeta_1 : ' , beta1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b71353ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig1 = plt.figure(1)\n",
    "x=np.linspace(1,epo,epo)\n",
    "plt.plot(x,train_error, label = 'Training')\n",
    "plt.plot(x,test_error, label ='Testing')\n",
    "plt.legend(loc = 2)\n",
    "plt.title('MAE')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccc64b7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.min(test_error))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe5afc09",
   "metadata": {},
   "source": [
    "## Y = b0 + b1X1 + b2X2 (24+96hr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcb2ebd8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94cd87e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "features1 = torch.from_numpy(x1)\n",
    "features2 = torch.from_numpy(x2)\n",
    "targets = torch.from_numpy(y)\n",
    "x_test1 = torch.from_numpy(xt1)\n",
    "x_test2 = torch.from_numpy(xt2)\n",
    "y_test = torch.from_numpy(yt)\n",
    "\n",
    "beta0 = torch.ones(6816 , requires_grad = True)\n",
    "beta1 = torch.ones([1704,6816], requires_grad = True)\n",
    "beta2 = torch.ones([6816,6816] , requires_grad = True)\n",
    "\n",
    "# rate = 1\n",
    "rate = 1e-2\n",
    "optimizer = optim.LBFGS([beta0 , beta1 , beta2] , lr=rate)\n",
    "# optimizer = optim.Adam([beta0 , beta1 , beta2], lr=rate)\n",
    "# optimizer = optim.SGD([beta0 , beta1 , beta2], lr=rate)\n",
    "\n",
    "epo = 201\n",
    "loss = nn.MSELoss()\n",
    "train_error = np.zeros(epo)\n",
    "test_error = np.zeros(epo)\n",
    "\n",
    "for epoch in range (epo):\n",
    "    # yhats_train = model(features.float() , beta0 , beta1)\n",
    "    # train_loss = loss(targets.float() , yhats_train)\n",
    "    # train_error[epoch] = train_loss\n",
    "\n",
    "    # optimizer.zero_grad()\n",
    "    # if epoch == 0 :\n",
    "    #         train_loss.backward(retain_graph=True) \n",
    "    # else :\n",
    "    #     train_loss.backward()\n",
    "    # # train_loss.backward() \n",
    "    # optimizer.step()    \n",
    "\n",
    "    def closure():\n",
    "        yhats_train = model(features1.float() , features2.float(), beta0 , beta1 , beta2)\n",
    "        train_loss = loss(targets.float() , yhats_train)\n",
    "        train_error[epoch] = train_loss\n",
    "        optimizer.zero_grad()\n",
    "        # if epoch == epo-1 :\n",
    "        #     train_loss.backward() \n",
    "        # else :\n",
    "        #     train_loss.backward(retain_graph=True)\n",
    "        train_loss.backward(retain_graph=True)\n",
    "        return train_loss\n",
    "    optimizer.step(closure)    \n",
    "\n",
    "    yhats_test = model(x_test1.float(), x_test2.float() , beta0, beta1 , beta2) \n",
    "#     for i in range (25):\n",
    "#         for j in range (6816):\n",
    "#             if y_test[i][j] == 0:\n",
    "#                 yhats_test[i][j] = 0\n",
    "    r = torch.abs(yhats_test - y_test)\n",
    "    test_loss = torch.mean(r)\n",
    "    test_error[epoch] = test_loss\n",
    "\n",
    "    if epoch <= 10 or epoch % 10 == 0:\n",
    "        print(f\"Epoch {epoch}, Training loss {train_error[epoch]:.4f},\"\n",
    "                    f\" Testing loss {test_error[epoch]:.4f}\")\n",
    "        # print(f\"Epoch {epoch}, Training loss {train_loss.item():.4f},\"\n",
    "        #             f\" Testing loss {test_loss.item():.4f}\")\n",
    "\n",
    "        # print('\\tBeta_0 : ' , beta0)\n",
    "        # print('\\tBeta_1 : ' , beta1)\n",
    "        # print('\\tBeta_2 : ' , beta2)       \n",
    "    else :\n",
    "        if epoch >= epo-10 :\n",
    "            print(f\"Epoch {epoch}, Training loss {train_error[epoch]:.4f},\"\n",
    "                        f\" Testing loss {test_error[epoch]:.4f}\")\n",
    "            # print(f\"Epoch {epoch}, Training loss {train_loss.item():.4f},\"\n",
    "            #             f\" Testing loss {test_loss.item():.4f}\")\n",
    "            # print('\\tBeta_0 : ' , beta0)\n",
    "            # print('\\tBeta_1 : ' , beta1)\n",
    "            # print('\\tBeta_2 : ' , beta2) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff5165e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig1 = plt.figure(1)\n",
    "x=np.linspace(1,epo,epo)\n",
    "plt.plot(x,train_error, label = 'Training')\n",
    "plt.plot(x,test_error, label ='Testing')\n",
    "plt.legend(loc = 2)\n",
    "plt.title('MAE')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41d82dfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.min(test_error))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chou",
   "language": "python",
   "name": "chou"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
